\SweaveOpts{fig=T,echo=F}
\section{PLUE: uma proposta de perfilhamento de verossimilhança}

Nesta seção, vamos descrever uma metodologia inédita para a realização de análises de incerteza
baseada no perfilhamento da verossimilhança dos parâmetros. Argumentamos que essa metodologia é
intuitivamente atraente dentro de um paradigma de verossimilhança.
Vamos nos referir ao nosso
procedimento como PLUE - Profiled Likelihood Uncertainty Estimation.

%Vamos também retomar neste momento a descrição do GLUE - Generalized Likelihood Uncertainty Estimation,
%proposto por Beven \& Binley \citep{Beven92} e discutido brevemente no capítulo \ref{cap:pse}, juntamente
%com críticas feitas a esta metodologia. Nosso objetivo é argumentar que o procedimento de 
%perfilhamento de verossimilhança tem convergências com o GLUE. Por este motivo, vamos nos referir ao nosso
%procedimento como PLUE - Profiled Likelihood Uncertainty Estimation.

\subsection{Intuição}

Suponha que temos em mãos um modelo de crescimento populacional estruturado como aqueles discutidos
nas seções \ref{Leslie} e \ref{Tribolium}, e um conjunto de dados a partir dos quais podemos calcular
as taxas de sobrevivência, crescimento e fertilidade para a espécie. 

De posse desses dados, gostaríamos de fazer as seguintes perguntas:

{\em ``Qual o suporte que os dados dão para a afirmação de que a população está estável {\em versus} 
em declínio? Qual o suporte que os dados dão para a afirmação de que a população vai se extinguir em menos 
de 10 anos {\em versus} em mais de dez anos?''}

Essas perguntas não podem ser respondidas de um ponto de vista verossimilhantista, por corresponderem
a hipóteses compostas sobre os parâmetros. No entanto, podemos tomar o ponto de máxima verossimilhança
como um ponto privilegiado, e restringir nossa pergunta à forma:

{\em ``Qual o suporte que os dados dão para o ponto de máxima verossimilhança {\em versus} qualquer ponto
cuja taxa de crescimento populacional seja negativa?''}

Esquematicamente, podemos ver na figura \ref{fig:esquema} que estamos comparando a verossimilhança
do ponto de máximo global ($x_{max}$) com o máximo obtido em uma região distinta ($x_{lim}$). Esta pergunta
compara uma hipótese simples, a que corresponde à máxima verossimilhança, com uma hipótese composta.
Esse tipo de comparação evita os problemas e peculiaridades que surgem em propostas de leis gerais de 
verossimilhança devido à sobreposição entre a verossimilhança de hipóteses diferentes. Em termos dos axiomas
de Zhang\footnote{vide seção \ref{sec:GLL}}, estamos aceitando uma forma fraca do primeiro, mas não o segundo.

Esse raciocínio pode ser expandido para perguntarmos:

{\em ``Quais são os pontos do espaço de parâmetros para os quais o suporte é maior do que uma certa distância
$\delta$ em relação ao ponto de máxima verossimilhança?''}

Ao fazer essa pergunta para uma série de valores de $\delta$, estamos efetivamente perfilhando a 
verossimilhançao de cada ponto do espaço de parâmetros.

\begin{figure}[htb]
<<>>=
L <- function(x) {
	x = x + 3
	((x)*(x-2)*(x-6)*(x-9)+100)/20
}
x = seq(-3,5, length.out=200)
plot(x, L(x), type="l", xaxs = "i", yaxs = "i", ylim=c(0, 10))
# regioes shaded
x = seq(-3,0, length.out=200)
polygon(c(-3,x,0), c(0,L(x),0), col="cadetblue1")
x = seq(0,5, length.out=200)
polygon(c(0,x,5), c(0,L(x),0), col="darkolivegreen2")
# pontos de "interesse"
lines(c(0,0), c(7.7,8.3), lty=2)
text(0, 8.5, expression(x[lim]))
lines(c(1.08,1.08), c(9,9.5), lty=2)
text(1.08, 9.7, expression(x[max]))
arrows(1.08, 7.7, 1.08, 9, angle=90, code=3, length=0.15)
text(1.4,(7.7+9)/2,(expression(Delta[L])), cex=1.3)
@
	\caption{Representação esquemática de uma função de verossimilhança, mostrando dois pontos de interesse:
	o máximo global $x_max$ e o máximo de uma região distinta $x_lim$. Veja detalhes no texto.} 
	\label{fig:esquema}
\end{figure}

\subsection{Método}

Seja $y = F(\bu{x})$ o resultado escalar de um modelo $F$.
%%PI: completar o método


%# Use PLUE to perform a Profile Likelihood Uncertainty Analysis
%PLUE <- function(model=NULL, factors, N, LL, start, res.names=NULL,
%				 method=c("internal", "mcmc"), opts = list(), nboot=0, 
%				 repetitions=1, cl=NULL) {
%	# Input validation for common errors and "default" value handling:
%	my.opts = list(Q=NULL, blen=1, nspac=1, scale=1)
%
%	if (method=="internal") {
%		data = MCMC.internal(LL, start, N, my.opts)
%	# Apply the model to the input data
%	colnames(data\$L) <- factors
%	res <- internal.run(cl, model, data\$L, repetitions)
%	prcc <- internal.prcc(data\$L, res, nboot)
%	if (is.null(res.names) && ! is.na(res)) res.names <- paste("O", 1:dim(res)[2], sep="")
%
%	# Now we profile the results to find out 
%	# "what are the lower/upper limits to the 0.1-Delta nLL region?"
%	# "what are the lower/upper limits to the 0.2-Delta nLL region?"
%	# and so on and so on
%	
%	mmin <- min(data\$nLL[valid(data\$nLL) ])
%	mmax <- max(data\$nLL[valid(data\$nLL)])
%	myN <-round(N/100)
%	prof <- seq(mmin, mmax, length.out=myN)
%	lower = c(); upper = c();
%	for (i in 1:myN)
%	{
%		search <- res[data\$nLL <= prof[i]]
%		search <- search[valid(search)] #throw away NA, NaN, Inf, etc
%		lower[i] <- min(search); upper[i] <- max(search)
%	}
%	# finally, we subtract the minimum LL to normalize the profile to 0
%	prof = prof - mmin
%	profile <- rbind(data.frame(limit = lower[myN:1], ll = prof[myN:1]), data.frame(limit=upper, ll = prof))
%}
