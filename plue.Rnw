\SweaveOpts{fig=T,echo=F}
\section{PLUE: uma proposta de perfilhamento de verossimilhança}\label{sec:plue}

Nesta seção, vamos descrever uma metodologia inédita para a realização de análises de incerteza
baseada no perfilhamento da verossimilhança dos parâmetros. Argumentamos que essa metodologia é
intuitivamente atraente dentro de um paradigma de verossimilhança.
Vamos nos referir ao nosso
procedimento como PLUE - Profiled Likelihood Uncertainty Estimation.

%Vamos também retomar neste momento a descrição do GLUE - Generalized Likelihood Uncertainty Estimation,
%proposto por Beven \& Binley \citep{Beven92} e discutido brevemente no capítulo \ref{cap:pse}, juntamente
%com críticas feitas a esta metodologia. Nosso objetivo é argumentar que o procedimento de 
%perfilhamento de verossimilhança tem convergências com o GLUE. Por este motivo, vamos nos referir ao nosso
%procedimento como PLUE - Profiled Likelihood Uncertainty Estimation.

\subsection{Intuição}

Suponha que temos em mãos um modelo de crescimento populacional estruturado como aqueles discutidos
nas seções \ref{Leslie} e \ref{Tribolium}, e um conjunto de dados a partir dos quais podemos calcular
as taxas de sobrevivência, crescimento e fertilidade para a espécie. 

De posse desses dados, gostaríamos de fazer as seguintes perguntas:

{\em ``Qual o suporte que os dados dão para a afirmação de que a população está estável {\em versus} 
em declínio? Qual o suporte que os dados dão para a afirmação de que a população vai se extinguir em menos 
de 10 anos {\em versus} em mais de dez anos?''}

Essas perguntas não podem ser respondidas de um ponto de vista verossimilhantista, por corresponderem
a hipóteses compostas sobre os parâmetros. No entanto, podemos tomar o ponto de máxima verossimilhança
como um ponto privilegiado, e restringir nossa pergunta à forma:

{\em ``Qual o suporte que os dados dão para o ponto de máxima verossimilhança {\em versus} qualquer ponto
cuja taxa de crescimento populacional seja negativa?''}

Esquematicamente, podemos ver na figura \ref{fig:esquema} que estamos comparando a verossimilhança
do ponto de máximo global ($x_{max}$) com o máximo obtido em uma região distinta ($x_{lim}$). Esta pergunta
compara uma hipótese simples, a que corresponde à máxima verossimilhança, com uma hipótese composta.
Esse tipo de comparação evita os problemas e peculiaridades que surgem em propostas de leis gerais de 
verossimilhança devido à sobreposição entre a verossimilhança de hipóteses diferentes. Em termos dos axiomas
de Zhang\footnote{vide seção \ref{sec:GLL}}, estamos aceitando uma forma fraca do primeiro, mas não o segundo.

Esse raciocínio pode ser expandido para perguntarmos:

{\em ``Quais são os pontos do espaço de parâmetros para os quais o suporte é maior do que uma certa distância
$\delta$ em relação ao ponto de máxima verossimilhança?''}

Ao fazer essa pergunta para uma série de valores de $\delta$, estamos efetivamente perfilhando a 
verossimilhançao de cada ponto do espaço de parâmetros.

\begin{figure}[htb]
<<>>=
L <- function(x) {
	x = x + 3
	((x)*(x-2)*(x-6)*(x-9)+100)/20
}
x = seq(-3,5, length.out=200)
plot(x, L(x), type="l", xaxs = "i", yaxs = "i", ylim=c(0, 10))
# regioes shaded
x = seq(-3,0, length.out=200)
polygon(c(-3,x,0), c(0,L(x),0), col="cadetblue1")
x = seq(0,5, length.out=200)
polygon(c(0,x,5), c(0,L(x),0), col="darkolivegreen2")
# pontos de "interesse"
lines(c(0,0), c(7.7,8.3), lty=2)
text(0, 8.5, expression(x[lim]))
lines(c(1.08,1.08), c(9,9.5), lty=2)
text(1.08, 9.7, expression(x[max]))
arrows(1.08, 7.7, 1.08, 9, angle=90, code=3, length=0.15)
text(1.4,(7.7+9)/2,(expression(Delta[L])), cex=1.3)
@
	\caption{Representação esquemática de uma função de verossimilhança, mostrando dois pontos de interesse:
	o máximo global $x_max$ e o máximo de uma região distinta $x_lim$. Veja detalhes no texto.} 
	\label{fig:esquema}
\end{figure}

\subsection{Método}
% \section{Proposta de aplicação}
% PI: A proposta faz sentido para mim, mas depois de ler as seções anteriores fiquei em dúvida sobre a interpretação. Creio que isso acontece porque ainda não há uma ligação clara entre a teoria das seções anteriores a aplicação e exemplos que começma aqui, confere?
% C: Verdade. Fiquei mto em dúvida sobre colocar ou não essa seção aqui; estou retirando ela para a quali, depois volto a inseri-la qndo tiver a teoria mais clara. Vou tb deixar mais claro que as aplicações nas proximas sessoes NÃO foram feitas dentro desse  paradigma ainda.

%Uma metodologia de análise de incerteza baseada em uma função de suporte, após a escolha de uma determinada forma para
%a equação de suporte $\Psi(\gamma|\bu{x})$ deve ser dada por:

%\begin{enumerate}
%	\item Formule diferentes modelos para explicar seus dados (ex, fertilidade constante ou agrupada, taxa de crescimento 
%		constante ou descrescente com a classe de tamanho, modelo com 4 ou com 5 classes de tamanho, etc). Escreva a 
%		função de verossimilhança $L_i(\bu{\theta}|\bu{x})$ para cada modelo $i \in 1,...,n$.
%	\item Encontre o conjunto de parâmetros que melhor ajusta seus dados para cada modelo. 
%	\item Determine o valor de AIC para cada modelo. 
%	\item Caso haja um modelo cujo desempenho é inequivocamente superior, use o método de Metropolis (forma de Monte Carlo) 
%		para gerar amostras a partir da função de verossimilhança do modelo, e combine
%		esses valores para gerar uma função de suporte de acordo com a equação \ref{eqn:Psi}. 
%	\item Caso haja um empate entre modelos, gere amostras a partir de todos os modelos vencedores e as combine usando 
%		os pesos designados pela diferença de informação.
%	\item Realize as análises de incerteza e sensibilidade a partir dos resultados gerados.
%\end{enumerate}

% PI: Tive dificuldades de identificar estes passos nos exemplos. No
% modelo mínimo me parece que há um só modelo, portanto os passos 1, 3
% e 5 não se aplicam? Ainda assim, me parece ter sentido após o ajuste
% fazer a análise de incerteza e sensisbilidade amostrando-se a
% superfície de verossimilhança como você fez. No caso do Tribolium vc
% não usou este procedimento, apenas o hipercubo para investigar numa
% vizinhança definida por uma distribuiçãod e probabilidades, correto?
% Novamente faz sentido, mas falta conectar teoria com exemplos de
% alguma maneira. Um caminho é separar a proposta em duas partes:
% exploração estocástica (contra análise local por aproximação linear,
% no caso de modelos matriciais); e exploração estocástica baseada na
% inceerteza das estimativas, expressa pela função de
% verossmilhança. Me lembre de discutirmos isso.

% C: Fato, os passos 1 3 e 5 desaparecem caso só haja um modelo para a geração dos dados.
% O Tribolium estah feito como descrito no cap. 2, ainda nao consegui montar a pse verossim. pra ele
Seja $y = F(\bu{x})$ o resultado escalar de um modelo $F$.
%%PI: completar o método


%# Use PLUE to perform a Profile Likelihood Uncertainty Analysis
%PLUE <- function(model=NULL, factors, N, LL, start, res.names=NULL,
%				 method=c("internal", "mcmc"), opts = list(), nboot=0, 
%				 repetitions=1, cl=NULL) {
%	# Input validation for common errors and "default" value handling:
%	my.opts = list(Q=NULL, blen=1, nspac=1, scale=1)
%
%	if (method=="internal") {
%		data = MCMC.internal(LL, start, N, my.opts)
%	# Apply the model to the input data
%	colnames(data\$L) <- factors
%	res <- internal.run(cl, model, data\$L, repetitions)
%	prcc <- internal.prcc(data\$L, res, nboot)
%	if (is.null(res.names) && ! is.na(res)) res.names <- paste("O", 1:dim(res)[2], sep="")
%
%	# Now we profile the results to find out 
%	# "what are the lower/upper limits to the 0.1-Delta nLL region?"
%	# "what are the lower/upper limits to the 0.2-Delta nLL region?"
%	# and so on and so on
%	
%	mmin <- min(data\$nLL[valid(data\$nLL) ])
%	mmax <- max(data\$nLL[valid(data\$nLL)])
%	myN <-round(N/100)
%	prof <- seq(mmin, mmax, length.out=myN)
%	lower = c(); upper = c();
%	for (i in 1:myN)
%	{
%		search <- res[data\$nLL <= prof[i]]
%		search <- search[valid(search)] #throw away NA, NaN, Inf, etc
%		lower[i] <- min(search); upper[i] <- max(search)
%	}
%	# finally, we subtract the minimum LL to normalize the profile to 0
%	prof = prof - mmin
%	profile <- rbind(data.frame(limit = lower[myN:1], ll = prof[myN:1]), data.frame(limit=upper, ll = prof))
%}
