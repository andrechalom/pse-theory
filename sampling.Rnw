\SweaveOpts{fig=T,echo=F}
\section{Sampling Techniques}\label{Sampling}
There are several strategies that can be used to choose the samples from the
parameter space that will be used as input to our model of interest. Here,
we will present some of them, along with their limitations, to justify our 
choice for the Latin Hypercube Sampling, which we will describe in section
\ref{LHS}.

One way of exploring the parameter space is by discretizing every distribution
and running the model for every
possible combination of values for all parameters. 
This is called full parameter space exploration, as done 
in \citep{Turchin97}, and altough it 
possesses many advantages, it may become very costly in terms of computer time. 
In addition, the number of possible combinations increases exponentially with 
the number of parameter dimensions considered.

To circumvent the exponential increase in the number of samples, it is usual
to explore the parameter space in the following fashion: holding all but one
parameter constant, we analyse how the output of a model is affected by one
parameter dimension at a time (as done in \citep{Yang08}). This analysis is
refered to as individual parameter perturbance.
This kind of analysis
is, however, limited by the fact that the combinations of changed parameters
may give rise to complex and unexpected behaviours. 
Often, algorithms based on individual parameter perturbance are used as a first 
step in order to discriminate between parameters
that may have a substantial impact on the output, and parameters that are less
relevant (but see \citep{Morris91} for an alternative). 

Another viable option would be to
chose $N$ random samples from the entire space, in order to analyse both
the effect of each parameter and the combined effect of changing any
combined number of parameters. This sampling scheme is called random 
sampling, or Monte Carlo sampling,
and has been applied to many biological models \citep{Letcher96}.
One important feature of the Monte Carlo sampling is that its accuracy
does not depend on the number of dimensions of the problem,
as shown in \citep{MacKay03}.

Stratified sampling strategies, which are a special case of Monte 
Carlo sampling, consist in strategies for chosing these random samples while,
at the same, making sure that each of the subdivisions (or {\em strata}) of
the distribution are well represented.
As shown on \citep{McKay79}, the estimatives of statistical properties (such
as the mean or the variance) of the model output are better represented by 
stratified random sampling than by simple random sampling
(see figure \ref{fig:SamplingMethods} for examples). As we shall see in the
next session, the Latin Hypercube sampling is a practical and easy to understand
stratified sampling strategy.

Another class of Monte Carlo methods that should be mentioned here is the Markov
Chain Monte Carlo (MCMC), which is also used on similar analyses \citep{MacKay03}. This
methods consists in generating a sequence of points $\{\bu{x}^{(t)}\}$ from
the parameter space whose distribution {\em converges} to the joint
probability distribution $\bu{D}(\bu{x})$, and in which each sample $\bu{x}^{(t)}$
is chosen based on the previous $\bu{x}^{(t-1)}$. MCMC methods perform better
than LHS methods for estimating the distribution of the model responses,
however, they require a number of model runs which is orders of magnitude higher
than LHS requirements.

\begin{figure}[htbp]
	\begin{center}
<<SamplingMethods>>=
par(mfrow=c(2,2), pch=4, lty=3)
x<-rep(seq(0,1,0.2),6)
y<-rep(seq(0,1,0.2),each=6)
plot(x,y,xlab='',ylab='',main='Full PS exploration')
x<-c(seq(0,1,0.2),rep(0.4,6))
y<-c(rep(0.4,6),seq(0,1,0.2))
plot(x,y,xlab='',ylab='',main='Individual parameter perturbation')
x<-runif(10,0,1)
y<-runif(10,0,1)
plot(x,y,xlab='',ylab='',main='Simple random sampling')
x<-sample(qunif((1:10)/10-1/10/2,0,1))
y<-sample(qunif((1:10)/10-1/10/2,0,1))
plot(x,y,xlab='',ylab='',main='Latin Hypercube sampling')
for (i in (seq(0,1,0.1))) {abline(h=i); abline(v=i) }
@
	\end{center}
	\caption{Illustration of four sampling methods. While the
	full parameter space exploration is clearly representative
	of the whole space, it requires a very large number of samples.
	The individual parameter perturbation chooses samples by
	holding one parameter constant and varying the other, and
	clearly cannot take into account interactions between 
	parameters. The random sampling uses information about the
	whole parameter space with a small number of samples, but
	can oversample some regions while undersampling others. The
	Latin Hypercube (section \ref {LHS}) samples all the 
	intervals with equal intensity.}
	\label{fig:SamplingMethods}
\end{figure}

\subsection{Latin Hypercube: Definition and use}\label{LHS}
In this section, we describe the Latin Hypercube Sampling, and show how it can
be used to efficiently solve the questions posed in section \ref{Introduction}.
We also
discuss what are the available methods for obtaining the LHS. 

Firstly, let us define, in the context of statistical sampling, what is a 
Latin Square:
\begin{definition}
If we divide each side in a square in $N$ intervals, and then take samples
from the square, the resulting square will be called
Latin if and only if there is exactly one sample in each row and each column.
\end{definition}

A Latin Hypercube is simply the generalization of the Latin
Square to an arbitrary number of dimensions $m$. 
It should be noted, then, that the number of samples $N$ is fixed {\em a priori},
and does not depend on the number of parameters considered.

We will now construct the Latin Hypercube. Let's fix our attention in one parameter
dimension $i$ of the parameter space.
The first step we should take is to divide the range of $x_i$
in $N$ equally probable intervals. In order to do so, we will turn our attention
to the probability distribution of $x_i$, defined on section \ref{ps} as $D_i$.
Recall that this probability distribution must be chosen in a way that represents our
current understanding of the biology of the given system. This function might
be estimated by an expert in the field, it might represent a data set from
field or laboratory work, or in some cases it may be simply the broadest
possible set of parameters, in some cases where the actual values are unknown
or experiments are unfeasible (see fig. \ref{fig:Di}). 

In possession of the distribution function $D_i$, we must sample one point
from each equally probable interval. There are two approaches used here:
it is possible to choose a random value from within the interval (as proposed
by \citep{McKay79}), or instead, we can use the midpoint from each interval 
\citep{Huntington98}. As the statistical properties of the generated samples
are very similar, we will use the second approach here.

The integral of the distribution function is called the cummulative distribution
function $F_i(x)$. This function relates the values $x$ that the parameter may assume
with the probability $p$ that the parameter is less than or equal to $x$.
We will refer to the inverse of the cummulative distribution function, $F_i^{-1}$,
as the quantile function of the parameter $x_i$, as it associates every probability
value $p$ in the range $(0,1)$ to the value $x$ such that $P(x_i \leq x) = p$.
We divide the range $(0,1)$ in $N$ intervals of size $1/N$, and use this quantile
function to determine the $x$ values as the midpoints of each interval. 
Summarizing, we take the $N$ points, represented as $x_{i,k}$, $k \in [1,N]$, 
from the inverse cummulative distribution $F_i^{-1}(x)$ as\footnote{This formula is
given for simplicity; see \citep{Huntington98} for an alternative with better numerical
properties}:

\begin{equation}
	x_{i,k} = F_i^{-1}\left( \frac{k-0.5}{N}\right)
	\label{inverseCDF}
\end{equation}

\begin{figure}[htpb]
	\begin{center}
<<Samples>>=
par(cex=1.7)
x<-seq(-3,3,0.01)
plot(x, dnorm(x), type='l', xlab='Probability distribution of x', ylab='')
segments(qnorm(0.2), 0, qnorm(0.2), dnorm(qnorm(0.2)))
segments(qnorm(0.4), 0, qnorm(0.4), dnorm(qnorm(0.4)))
segments(qnorm(0.6), 0, qnorm(0.6), dnorm(qnorm(0.6)))
segments(qnorm(0.8), 0, qnorm(0.8), dnorm(qnorm(0.8)))
text(qnorm(0.1), 0.1, expression(x[5]))
text(qnorm(0.3), 0.1, expression(x[1]))
text(qnorm(0.5), 0.1, expression(x[4]))
text(qnorm(0.7), 0.1, expression(x[2]))
text(qnorm(0.9), 0.1, expression(x[3]))
@
	\end{center}
	\caption{Sample normal probability distribution, with 5 samples 
	collected from regions with same probability and shuffled. 
	Note that the first sample correspond to the 
	second interval, the second sample correspond to the fourth interval,
	and so on.}
	\label{fig:Samples}
\end{figure}

The samples from each dimension are subsequently shuffled, to randomize
the order in which each value will be used
(see example on figure \ref{fig:Samples}).
As the samples come from the distributions $D_i$, and are only reordered, their
(marginal) distribution 
will remain that of $D_i$. However, the joint distribution
of the parameters is still not well defined. In particular, this simple shuffling
may result in some of the parameters to be positively or negatively correlated with
each others, which might be undesirable. Some techniques have been developed to
eliminate these correlation terms or to impose different correlations between the
variables, and will be presented on section \ref{ext}.

It should be noted that, in the mathematical literature, it is usual to refer 
to a somewhat different object as a Latin Square: this would be a square whose 
sides are divided in $N$ intervals, and is filled with $N$ different symbols, 
such that for each row and column there is exactly one occurrence of each 
symbol, as represented in figure \ref{fig:glassLS}.

\begin{figure}[htpb]
	\begin{center}
		\includegraphics[width=200 pt]{glassLS.png}
	\end{center}
	\caption{A stained glass window at the Caius College, Cambridge, 
	showing a full Latin Square. Notice how there is only one occurence 
	of each color in each row and in each column.}
	\label{fig:glassLS}
\end{figure}

\subsection{Algorithms and extensions}\label{ext}
As described above, the LH sampling generates an uniform distribution of samples 
in each parametric dimension. However, there is no guarantee that the correlation 
between two or more parameters will be zero, and the classical algorithm
from McKay \citep{McKay79}, described in the previous section,
usually produces correlations as high as 0.3 between
pairs of factors, which can difficult or even compromise further analyses. In this
section, we will present one algorithm designed to take into
account the correlation between the parameter variables \citep{Huntington98}, using
a single-switch-optimized sample reordering scheme. 
We will present the general case of prescribing a correlation matrix, and will 
also present results for the trivial case of zero correlation terms. Other
methods have been proposed to address this problem \citep{Ye98, Steinberg06, Florian92}, 
including methods that deal with higher-order correlation terms \citep{Tang98}
using orthogonal designs and methods that resort to stochastic optimisation based on 
simulated annealing \citep{Vovrechovsky09}. These methods, however, either impose severe 
restrictions on the number of samples that must be chosen or are too
computationally intensive.

In order to obtain the samples with prescribed correlation terms, we define 
as $C_{i,j}$ the 
desired $m \times m$ correlation matrix between the variables $x_i$ and $x_j$,
and denote by $C_{i,j}^*$ the current correlation between $x_i$ and $x_j$.

The next step is done iteratively for each parameter dimension, starting with 
the second one. Suppose that the method has already been applied to $i=1,2,....,l-1$, 
and we will apply it to $i=l$. The square sum of the errors in the correlations 
between $x_l$ and the anterior parameters is given by

\begin{equation}
	E = \sum_{k=1}^{l-1} \left( C_{l, k} - C_{l,k}^* \right) ^2
	\label{Error}
\end{equation}

Afterwards, we calculate, for each pair of values sampled from the parameter 
dimension $l$, what would be the error in the correlation if they were switched.
The pair that corresponds to the greater error reduction is then switched, and
the procedure is repeated iteratively until the error is acceptably small.

\subsubsection{Note on the existence of solutions}
The problem of generating a sample with specified marginal distributions and correlation
terms is tighly connected to the problem of generating samples from a multivariate
distribution. This class of problems is very complex, and has received limited
attention from the probabilist community, with the recent paper describing the
exact construction of all feasible bivariate exponential distributions being considered
a significant theoretical advance \citep{Bladt10}.

One surprising result by \cite{Hoeffding1940} ({\em apud} \citep{Dukic13}) is that
the specified distribution function need not exist. For any pair of marginal distributions
$D_1$ and $D_2$, there exist a maximum and a minimum correlation coefficients,
$\rho^{+}$ and $\rho^{-}$ such that there exists a joint probability distribution $\bu{D}$
with specified marginal distributions and correlation. For some marginal distributions,
such as the Gaussian, these values are $\rho^+=1$ and $\rho^-=-1$, so that any correlation
term results in a valid distribution. On the other hand, if both $D_1$ and $D_2$ are
exponential distributions with parameter $\lambda=1$, the values change to $\rho^+=1$ and
$\rho^-=1-\pi^2/6 \approx -0.65$. 

In short, finding a Latin Hypercube with exponential marginal distributions and
at least one correlation term of, for example, $-0.9$ is impossible, no matter which 
algorithm is employed. 
However important this result is for the theory of probability,
it may not have strong consequences in practical applications. If the marginal distributions
and correlation terms are chosen after theoretical considerations or real world data, the
impossible probability distributions will not be attempted.

For recent developments in this area, see the work of \cite{Dukic13} and \cite{Huber14}.

\subsection{Stochastic models} %TODO menos handwaving
When dealing with stochastic models, like several relevant individual based 
models (IBM), the questions presented become complicated by the fact that 
running the same model with exactly the same parameters might wield largely 
different results, both quantitative and qualitatively. In this scenario,
we must be able to diferentiate the variation in responses due to the variation
of the parameters with the variation in response due to stochastic effects.

We will refer to the variation due to the input parameters as
the epistemic uncertainty. This uncertainty arises from the fact that we do
now know what are the correct values for a given parameter in a given natural
system, and is related to the probability distributions $D_i$, presented 
in section \ref{PSE}. The variation in the behaviour of the model which is
caused by stochastic effects, for a fixed set of parameters,
is called stochastic uncertainty, and is inherent
to the model. 

It is important to note that the two uncertainty components are impossible
to disentangle in general stochastic models. This has prevented the general
analysis of such models until recently. In recent years, studies have shown
that the important parameters and their effects can be correctly identified
by running such models repeatedly for the same input variables and then
averaging the output \citep{Segovia04}, given that
the following conditions are respected:

\begin{itemize}
	\item Sample sizes should be large, relative to the aleatory
		uncertainty.
	\item The output values should be unimodal, that is, the output values
		for a given parameter choice should be clustered around a 
		central value.
	\item The correct analysis tools should be used (as will be
		discussed on session \ref{QuantAnal}).
\end{itemize}

\subsection{Measuring the concordance with increasing sample size}
\label{SBMA}
We will now turn our attention to the problem of determining the optimal
number of model runs we should apply in order to provide a good estimate
of which are the relevant parameters for a given model.
One way of proceeding is by sistematically increasing the number $N$ of
model runs and applying any of the sensitivity analysis techniques, which
will be discussed on the following section. If the analyses indicate 
similar results for consecutive runs, we can presume that 
increasing the sample size will not yield major changes to the results.

All of the sensitivity analyses present us with a list of the parameters 
that have most influence in the model output. By comparing the resulting 
lists from two experiments, we can decide to stop increasing $N$ when
the lists are sufficiently similar. Our problem then is to determine
how similar are two vectors of ranks. In principle, we could
apply any distance function to those vectors. However, consider that
3 analyses indicated that the order of the most influential parameters is:
\begin{center}
\begin{tabular}[h!]{c c c c c c c c}
		\hline
		H1 &=& 1& 2& 3& 4& 5& 6\\
		H2 &=& 1& 2& 3& 6& 4& 5\\
		H3 &=& 2& 3& 1& 4& 5& 6\\
		\hline
\end{tabular}
\end{center}
By using standard distances (like Spearman's rho or Kendall's tau),
we will see the same difference between H1 and H2 and between H1 and H3.
On the other hand, in the context of determining the most influential
parameters, we would be inclined to see H1 and H2 as more similar than
any of them to H3, as the first two preserve the ordering of the three
first parameters.

Iman and Conover proposed a correlation coefficient for this problem
called Top-Down Correlation Coefficient \citep{ImanConover87}, which is
based on Savage Scores. This coefficient, know as TDCC, 
was extensively used for sensitivity analyses \citep{Marino08}. 
Another measure of concordance proposed more recently is the Symmetrized
Blest Measure of Association (SBMA) \citep{Genest03}.
Recent research suggests that estimates for SBMA 
produce a smaller standard error than TDCC without the assumption that 
there is no correlation between the variables \citep{Maturi10}.
Thus, we propose using SBMA as a measure of concordance between analyses
from different sample sizes. Defining the ranks from the first sample as
$R_i$ and the ranks for the second sample as $S_i$, the estimator for the
SBMA is:
\begin{equation}
		\xi_n = - \frac{4n+5}{n-1}+\frac{6}{n^3-n} %
		\sum _{i=1}^n R_iS_i \left( 4- \frac{R_i+S_i}{n+1} \right)
		\label{eqSBMA}
\end{equation}

For the techniques that may output negative values, for instance negative
correlations, the SBMA must be applied on the absolute values. Otherwise,
the parameters which present strong negative effects will be ranked very low,
and will not be taken into account by the SBMA.

We will apply SBMA for the PRCC technique (discussed on section \ref{QuantAnal})
on the example on section \ref{Leslie}. 
