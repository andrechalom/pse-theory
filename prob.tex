\section{Conceitos históricos sobre a natureza das probabilidades}

John Venn, no prefácio do seu estudo de 1866, escreve:

``The science of Probability occupies at present a somewhat anomalous position. It is impossible, I think, not to observe
in it some of the marks and consequent disadvantages of a {\em sectional} study. By a small body of ardent students it
has been cultivated with great assiduity, and the results they have obtained will always be reckoned among the most 
extraordinary products of mathematical genius. But by the general body of thinking men its principles seem to be regarded
with indifference or suspicion. Such persons may admire the ingenuity displayed, and be struck with the profoundity
of many of the calculatins, but there seems to them, if I may so express it, an {\em unreality} about the whole treatment
of the subject. To many persons the mention of Probability suggests little else than the notion of a set of rules, very
ingenious and profound rules no doubt, with which mathematicians amuse themselves by setting and solving puzzles.''
\citep{Venn1866}

No século e meio que se passou desde esta publicação, a aplicação da estatística se tornou uma obrigatoriedade no meio científico.
Em qualquer periódico renomado da área, é virtualmente impossível publicar um artigo experimental que não 
mencione alguma propriedade
estatística a respeito das amostras coletadas. Por muitos anos, a estatística vigente nas análises biológicas teve uma
forte inspiração frequentista, com testes de hipóteses e valores {\em p} sendo requeridos para publicações. 
No entanto, existe uma ampla crítica à forma como essa estatística é
utilizada nas ciências biológicas e na área de medicina \citep{Ioannidis05}.

Muito da crítica decorre da interpretação
simplista assumida por muitos cientistas a respeito do valor {\em p} reportado sobre um problema, mas as suas raízes devem
ser traçadas mais profundamente em uma falta de consciência dos cientistas sobre as interpretações a respeito da natureza
das probabilidades que são pressupostas por uma escola de pensamento, mas raramente examinadas profundamente. 
De outro lado, análises recentes sugerem que o tamanho dos efeitos em várias áreas da ecologia são tão pequenos que estes
dificilmente serão corretamente detectados pelos procedimentos tradicionais \citep{Jennions03}. Em áreas como a ecologia do
comportamento, o poder dos testes estatísticos é muito pequeno, e esse problema é acentuado quando são utilizados métodos
(como o método de Bonferroni) para corrigir o valor {\em p} reportado. A solução para esses problemas pode não estar ligada
a desenvolver novos métodos ou procedimentos estatísticos, mas na análise crítica e racional dos pressupostos por trás
dos testes frequentemente utilizados.

Novamente, citamos John Venn:

``Students of Philosophy in general have thence conceived a prejudice against Probability, which has for the most part
deterred them from examining it. As soon as a subject comes to be considered `mathematical' its claims seem generally,
by the mass of readers, to be either on the one hand scouted or at least courteously rejected, or on the other hand
to be blindly accepted with all their assumed consequences. Of impartial and liberal criticism it obtains little or nothing.''
\citep{Venn1866}

Embora o debate sobre a natureza da probabilidade tenha sido fervorosamente promovido por matemáticos e estatísticos,
este não alcança, muitas vezes, o cientista que faz uso do ferramental da probabilidade para resolver um problema particular.
Para poder discorrer sobre a estimação de incertezas e a atribuição de probabilidades a diferentes resultados de um modelo,
é necessário examinar com atenção o que entendemos sobre probabilidade. O conceito por trás da palavra probabilidade vai
embasar não apenas o que entendemos pela palavra probabilidade, mas também traz um significado para os conceitos de
incerteza, inferência e simulação, que serão de importância para a discussão a seguir. 

Embora o debate surja como uma questão sobre a natureza das probabilidades, vamos encontrar as divergências práticas
em torno de questões de inferência. Um problema simplesmente de atribuição de probabilidades sobre um experimento mental,
que seja bem formulado, poderá ter a mesma solução, não importando a afiliação filosófica de quem o resolve. Esse problema 
pode ser, por exemplo, qual a probabilidade de uma moeda justa cair com a cara para cima em pelo menos dois lançamentos de 
três; qual a chance de pelo menos dois alunos em uma classe de trinta fazerem aniversário no mesmo dia; qual a probabilidade 
de uma única observação de uma distribuição normal cair no intervalo de $x$ até $x+dx$. As escolas vão divergir principalmente
no procedimento adotado para conectar essas experiências mentais com problemas vindos de observações reais. 

Esse texto é resultado de uma investigação breve sobre a história dos conceitos de probabilidade, inferência, e incerteza,
e sua relação mais geral com visões alternativas de ciência. Ele foi motivado por uma percepção de que a vasta maioria dos
textos introdutórios em estatística se foca em uma escola de pensamento, alinhada com as visões do autor, sem uma análise crítica
aprofundada das suas relações complexas com as demais. A separação da estatística entre as escolas frequentista e bayesiana,
muito repetida em textos introdutórios, esconde uma riqueza de detalhes, envolvendo contradições importantes dentro de cada
escola e convergências nos pensamentos entre escolas diferentes. Essa separação também apaga a importância de definições que
não se encaixam em nenhuma das alternativas, como a lógica probabilística de Carnap e o apelo à verossimilhança enquanto 
única base de inferência. A presente exposição é, por força, limitada e fragmentária; muitos pontos mais finos da argumentação
(envolvendo, por exemplo, a regra de sucessão de Laplace) foram deixados de fora para que o texto não se tornasse proibitivamente
longo. 

\section{Contra uma teoria axiomática}\label{sec:axiomatica}
O ensino moderno da probabilidade é, em larga escala, baseado na axiomatização proposta por Andrey Kolmogorov, matemático
soviético muito prolífico em diversas áreas da ciência. Os axiomas da probabilidade podem ser escritos como \citep{Morettin09}:

\begin{description}
	\item[Não-negatividade] A probabilidade de um evento é um número real não-negativo: $P(E) > 0$.
	\item[Unitariedade] A probabilidade de um evento certo é 1: $P(\Omega) = 1$.
	\item[Aditividade] A probabilidade de eventos mutuamente exclusivos é aditiva: $P(U \cup V) = P(U)+P(V)$.
\end{description}

Uma abordagem puramente axiomática para a probabilidade, embora seja útil para resolver problemas matemáticos, resultantes do
cálculo da probabilidade associada a um experimento mental, é insuficiente do ponto de vista filosófico por não 
levar a uma conclusão sobre no que {\em consiste} uma probabilidade. A proposição desses axiomas não leva a nenhuma conclusão 
sobre problemas reais: não há como conectar uma proposição teórica derivada dos axiomas da probabilidade a um problema
prático sem assumir, em algum momento, um significado para a palavra probabilidade que não está contido nos axiomas. Uma posição
que pode ser tentadora é presumir que probabilidade assume {\em qualquer} significado coerente com os axiomas: essa posição,
no entanto, também falha por não restringir o tipo de proposição ao qual probabilidades podem ser associadas (veja a sessão
\ref{sec:classref}). 

Outro problema da visão axiomática é que, se os axiomas se baseiam em conveniência matemática e não na coerência com o uso 
real da probabilidade, alguns problemas decorrentes da axiomatização podem passar despercebidos. Considere a troca do axioma
de aditividade pelo seguinte axioma de aditividade completa (ou $\sigma$-aditividade):

\begin{description}
	\item[Aditividade completa] Qualquer conjunto contável de eventos mutuamente exclusivos satisfaz:

		$P(E_1 \cup E_2 \cup \cdots) = \sum_{i=1}^\infty P(E_i)$
\end{description}

Esse axioma é uma generalização natural do axioma de aditividade a conjuntos infinitos (contáveis) de eventos, e é amplamente
aceito. No entanto, tome um sorteio de um número racional aleatório entre 0 e 1, com distribuição uniforme. É fácil ver que,
já que existem infinitos números racionais em qualquer intervalo intervalo,
a probabilidade de sortear cada número racional individualmente é zero; logo a probabilidade do evento certo, que é 1, 
pelo axioma da unitariedade, deve ser também 0, pois é a soma de infinitos termos iguais a zero.

Para encontrar uma definição de probabilidade que permita dar um sentido aos axiomas, vamos repassar a opinião de alguns dos 
grandes pensadores sobre o tema nas próximas sessões.

\section{Interpretação clássica}
A teoria matemática da probabilidade remonta aos séculos XVI e XVII, nos quais Girolamo Cardano, Galileu Galilei, Blaise Pascal
e Pierre Fermat desenvolveram métodos para resolver problemas envolvendo combinações de resultados em jogos de dados e outros
jogos de azar. 

Essa origem, juntamente com a facilidade e a universalidade de problemas envolvendo jogos de azar, explicam porque tantos textos
introdutórios sobre estatística empregam exemplos envolvendo rolar dados e tirar cartas de baralhos. Embora a simplicidade
destes exemplos ajude a transmitir os conceitos de probabilidade com facilidade, isso causa muitas vezes a sensação de que
as regras da probabilidade servem apenas para os casos simples, e que estas não se aplicam, ou não se adequam totalmente aos 
problemas de natureza complexa com os quais a biologia e a ecologia têm de enfrentar. De fato, é muito difícil justificar que
questões como ``qual a chance de que uma população de jararaca-ilhoa entre em extinção nos próximos dez anos?'' sejam embasadas
nas mesmas idéias que ``qual a chance de uma moeda jogada para cima dar coroa?''. É mais natural tratar a primeira pergunta como
um questionamento sobre o nosso conhecimento atual dos processos biológicos e da condição de vida dos indivíduos dessa espécie
do que um questionamento sobre um processo físico simples que pode ser repetido um grande número de vezes. 

Porém, mesmo textos escritos por estatísticos empregam frequentemente definições conflitantes sobre probabilidades: às vezes,
elas são usadas para representar algo pessoal e subjetivo, como um nível de confiança; outras vezes são apresentadas como a
razão entre diferentes contagens.

A primeira tentativa de definir formalmente o conceito de probabilidade vem em 1814,
com o trabalho de Pierre Simon de Laplace:

``La théorie des hasards consiste à réduire tous les évènemens du même genre, à un certain nombre de cas également
possibles, c'est-à-dire tels que nous soyons également indécis sue leur existence, et à determiner le nombre de cas
favorables à l'évènement dont on cherche la probabilité. Le rapport de ce nombre à celui de tous les cas possibles,
est la mesure de cette probabilité'' 
\citep{Laplace1814}

(``A teoria da probabilidade consiste em reduzir todos os eventos de um mesmo tipo a um certo número de casos igualmente
possíveis, isso é, tal que sejamos igualmente indecisos sobre a sua ocorrência, e a determinar o número de casos
favoráveis ao evento ao qual buscamos a probabilidade. A razão deste número para o número de todos os casos possíveis
é a medida desta probabilidade'')

Para Laplace, todos os eventos que presenciamos seriam resultados de leis físicas
imutáveis, e uma inteligência superior, dotada do conhecimento do estado do universo em um dado instante, poderia prever
todos os eventos futuros sem qualquer incerteza. No entanto, nosso conhecimento limitado, tanto do estado do universo
quanto das leis que o regem, faz com que não possamos fazer previsões para um grande número de sistemas. O estudo das 
probabilidades se coloca, então, como um apoio ao nosso poder de realizar previsões sobre o universo, 
enquanto não possuímos o conhecimento das leis e estados necessária para realizar previsões certeiras. 

Uma dificuldade com essa definição de Laplace é que a demarcação de ``eventos igualmente prováveis'' deve ser feita
com base em argumentos que não partam da idéia de probabilidade; caso contrário, podemos incorrer em raciocínios
circulares. Por exemplo, se uma moeda tem 2 lados perfeitamente simétricos, e a lançamos de forma a não privilegiar um dos lados,
podemos dizer que o número de eventos totais é dois, e o número de eventos favoráveis a tirar coroa é um. Não há qualquer motivo
para supor que um dos lados seja mais propenso a cair para cima que o outro, visto a moeda ser perfeitamente simétrica; logo,
a probabilidade de tirar coroa em um lançamento é de $\frac{1}{2}$. Aqui, o ato de atribuir iguais chances aos dois
lados da moeda vem de um argumento físico de simetria. Isso é dizer que, para Laplace,
a ciência da probabilidade deve se basear em leis físicas do mundo natural, e não no nosso estado presente de conhecimento 
sobre o mundo. A probabilidade é, desta forma, algo que existe objetivamente.
Esta interpretação faz com que as leis da probabilidade, estritamente, só possam ser aplicadas para 
sistemas simples, sobre os quais temos um alto conhecimento e controle. Laplace, no entanto, não parece ter levado essa 
definição à suas últimas consequências, já que ele discute no tratado citado acima problemas referentes a fazer inferências 
sobre testemunhos judiciais, nos 
quais a testemunha poderia mentir, com uma certa probabilidade, ou ter se equivocado, com outra probabilidade. 
Presumindo que as probabilidades estão corretas, o método usado por Laplace é conhecido como ``método das probabilidades
inversas'', e tem uma alta importância na história da estatística.  No entanto, 
nenhuma explicação convincente é dada sobre como medir essas probabilidades dentro do paradigma clássico.

Deve-se notar que o argumento de Laplace espelha o raciocínio desenvolvido pelo rev. Thomas Bayes em seu trabalho publicado
postumamente em 1763. Esse trabalho não será discutido a fundo aqui, por ser muito convoluto em seus detalhes e notação,
e não oferecer uma visão clara sobre a natureza das probabilidades.
Determinar quais os méritos e falhas de Laplace e Bayes, respectivamente, é um problema muito debatido e que será retomado na
sessão \ref{sec:bayes}.

\section{Interpretação subjetivista}
Uma visão alternativa é dada por Augustus de Morgan, conhecido principalmente por suas leis em lógica proposicional, 
no seu tratado Formal Logic, de 1847. Para de Morgan, a única certeza que podemos ter é a de nossa própria existência. 
Este conhecimento
não é passível de ser refutado. No entanto, qualquer outra proposição feita deve ser acompanhada por 
um {\em grau de conhecimento} subjetivo:

``It will be found that, frame what circunstances we may, we cannot invent a case of purely objective probability.
I put ten white balls and ten black ones into an urn, and lock the door of the room. I may feel well assured that,
when I unlock the room again, and draw a ball, I am justified in saying it is an even chance that it will be a white one.
If all the metaphysicians who ever wrote on probability were to witness the trial, they would, each in his own sense and 
manner, hold me right in my assertion. But how many things there are to be taken for granted! Do my eyes still distinguish
colours as before? Some persons never do, and eyes alter with age. Has the black paint melted, and blackened the white balls?
Has any one else possessed a key of the room, or got in at the window, and changed the balls? We may be {\em very sure},
as those words are commonly used, that none of these things have happened, and it may turn out (and I have no doubt will do so,
if the reader try the circumstances) that the ten white and ten black balls will be found, as distinguishable as ever, and
unchanged. But for all that, there is much to be assumed in reckoning upon such a result, which is not so objective (in the
sense in which I used the word) as the knowledge of what the balls were when they were put into the urn.'' \citep{deMorgan1847}

A teoria de probabilidades segundo de Morgan, portanto, lida com graus de conhecimento subjetivos. Assim, ao perguntar para
uma pessoa comum qual é a chance de que um lançamento de moeda resulte em cara, essa pessoa pode responder $\frac{1}{2}$, e 
esta é a medida de probabilidade correta, dada a informação que ele possui sobre o problema. Uma outra pessoa, que sabe
que esta moeda é viciada, ou que é capaz de jogar a moeda de forma a privilegiar um resultado, pode dar outra resposta
completamente diferente, e ainda assim estará correta. Para de Morgan, o passo essencial na construção de uma probabilidade
consiste em {\em medir} o grau de certeza que temos em uma proposição. Algumas proposições são fáceis de medir, como
``dois mais dois são quatro'', ``um lançamento de uma moeda justa vai dar coroa'', e ``dois mais dois são cinco'', aos quais
podemos atribuir facilmente as probabilidades de 1, $\frac{1}{2}$ e 0. Essas medidas, juntamente com o senso de que
algum evento é mais ou menos provável que outro, podem ser usados para construir uma escala subjetiva de probabilidades.
Como essa visão sobre probabilidades parte exclusivamente do conhecimento que temos, e não de uma realidade objetiva,
a probabilidade que queremos determinar não é algo que exista à parte no mundo; ela existe somente enquanto descrição
de um processo mental.

Um empecilho ao uso dessa interpretação de probabilidades se dá quando percebemos que, quanto mais complexo é o 
problema que queremos resolver, mais imperfeita se torna nossa sensação de conseguir ordenar a probabilidade referente
a diferentes resultados de um experimento. É fácil apreender que, se eu jogar duas moedas para cima, a chance de que ambas 
resultem em cara é menor do que a chance de que um único lançamento resulte em cara. É consideravelmente mais difícil encaixar
nessa ordenação a frase ``ao escolher uma letra aleatória da obra completa de Shakespeare, ela será E ou A''; mas esse caso
é passível de uma análise exaustiva. No entanto, questões como ``qual a probabilidade de que o signo de uma pessoa afete
suas características, como sociabilidade ou perseverança?'', embora perfeitamente válidas para a visão de de Morgan, apresentam
grande dificuldade em ser respondidas nesse paradigma.

A visão subjetiva de probabilidades permite ainda que frases cotidianas, como ``é provável que chova hoje'' ou ``existe uma grande
chance de encontrarmos o José hoje'', sejam tratadas pelo mesmo ponto de vista que os jogos de azar. Mas
embora essa interpretação seja de grande utilidade por permitir a generalização do conceito de probabilidade para problemas
mais gerais do que os sistemas físicos simples contemplados pela interpretação clássica, ela não garante que o valor encontrado
para a probabilidade de um evento seja coerente para diversas pessoas. Isso irá embasar a crítica de que uma visão subjetivista 
de probabilidade pode ser perfeitamente adequada para embasar uma decisão pessoal, mas falhar enquanto uma ferramenta de 
avanço da ciência, entendida como um corpo teórico objetivo e independente das opiniões de cada cientista individualmente.

\section{Interpretação frequentista}
A próxima crítica à visão de Laplace vem de uma série de pensadores dentre os quais se destacam George Boole e John Venn
Este último, em seu trabalho de 
1866, postula que a probabilidade de um evento se refere a frequência relativa com a qual esse evento se repetiria em uma
série infinita de experimentos:

``We may define the probability or chance (the terms are here regarded as synonymous) of the event happening in that
particular way as the numerical fraction which represents the proportion between the two different classes in the long run (...).
This assumes that the series are of indefinite extent, and of the kind which we have described as possessing a fixed type.
If this be not the case, but the series be supposed terminable, or regularly or irregularly fluctuating, (...) the series ceases
to be a subject of science. What we have to do under these circumstances, is to substitute a series of the right kind for
the inappropriate one presented by nature, choosing it, of course, with as little deflection as possible from the observed facts.
This is nothing more than has to be done, and invariably is done, whenever natural objects are made subjects of strict science.''
\citep{Venn1866}

Venn também critica a visão subjetivista de de Morgan, sob
o argumento de que uma avaliação individual está sujeita a
um enorme número de fontes de erro:

``Our conviction generally rests upon a sort of chaotic basis
composed of an infinite number of inferences and analogies
of every description, and there moreover distorted by our
state of feeling at the time, dimmed by the degree of our
recollection of them afterwards, and probably received
from time to time with varying force according to the way in
which they happen to combine in our consciousness at the 
moment.''\citep{Venn1866}

Há um problema claro com a visão dita frequentista, advogada por Venn, que é o fato de que não temos 
acesso a infinitas experiências para determinar o valor
de uma dada probabilidade. A probabilidade de um dado evento poderia ser definida como o limite da razão entre o número de eventos
favoráveis sobre o número de experimentos realizados, com o número de experimentos realizados indo para o infinito. Com 
$n_x$ sendo o número de eventos favoráveis e $n_t$ sendo o número de experimentos realizados:

\begin{equation}
	P(x) = \lim_{n_t \rightarrow \infty} \frac{n_x}{n_t} 
\end{equation}

No entanto, só temos acesso aos primeiros termos dessa série, e não podemos tirar conclusões sobre o limite de uma série apenas
observando os primeiros elementos. Observar o início de uma série sequer garante que o limite descrito acima exista.
Mesmo quando temos acesso a um grande conjunto de dados para estimar o valor de uma determinada
probabilidade, esse conjunto pode ser suficientemente heterogêneo para que nossa conclusão se torne incorreta. Como escreve Venn,

``At the present time the average duration of life in England may be, say, forty years; but a century ago it was decidedly less;
several centuries ago it was presumably very much less (...). Let us assume that the regularity is fixed and permanent. It is
making a hypothesis which may not be altogether consistent with fact, but which is forced upon us for the purpose of securing
precision of statement and definition.''\citep{Venn1866}

Assim, para a visão frequentista a probabilidade que queremos medir é algo que existiria em um mundo idealizado no qual fosse
possível repetir o experimento infinitas vezes, sem haver variações alheias ao fenômeno estudado. Dentro dessa idealização,
a probabilidade medida se torna algo tangível e objetivo. No entanto, para qualquer problema real, essa objetividade
apenas se concretiza dentro da idealização de um dado modelo, para o qual temos que presumir uma certa regularidade. 

Em sua monografia Likelihood, de 1972, A.W.F. Edwards, escreve:

``Though the notion of a random choice is not devoid of philosofical difficulties, I have a fairly clear idea of what I mean
by `drawing a card at random'. That the population may exist only in the mind, an abstraction possibibly infinite in extent,
raises no more (and no less) alarm than the infinite straight line of Euclid. I am only prepared to use probability to describe
a situation if there is an analogy between the situation and the concept of random choice from a defined population.''
\citep{Edwards72}

Não faz sentido tratar uma hipótese estatística como uma sentença escolhida aleatoriamente de uma população de sentenças, algumas
das quais são verdadeiras. Desta forma, a escolha da definição frequentista de probabilidade implica em abandonar o método
das probabilidades inversas para comparar o mérito de diferentes hipóteses. Essa percepção vai levar ao desenvolvimento
da escola de testes de significância e à proposição do conceito de verossimilhança, ambos por R. A.Fisher.

\section{A formalização da inferência}

A escola frequentista será de grande influência para a formulação da teoria de testes de hipóteses de R. A. Fisher, que se
lança no começo da década de 1920 sobre o problema de formalizar a inferência estatística, ou seja, o processo de
extrair conclusões a partir de dados obtidos por um experimento. Baseando-se nos trabalhos de Karl Pearson e William Sealy Gosset
(conhecido por seu pseudônimo Student), Fisher realiza um passo fundamental dessa formalização
diferenciando os conceitos de população e amostra:

``It is customary to apply the same name, {\em mean}, {\em standard deviation}, {\em correlation coefficient}, etc., both 
to the true value which we should like to know, but can only estimate, and to the particular value at which we happen 
to arrive by our methods of estimation.''\citep{Fisher1922}

Nesta perspectiva, precisamos diferenciar uma {\em população}, hipoteticamente infinita, na qual os objetos se dividam em duas
classes de acordo com uma característica de interesse, de uma {\em amostra} finita, que será nossa base para tirar conclusões.
Por exemplo, examinando o lançamento de um dado, a população de interesse se constitui numa sequência infinita de rolagens
de dado, na qual distinguimos uma característica, como o fato da rolagem resultar no número 5. O valor da probabilidade de que
uma rolagem resulte em 5 {\em na população} pode ser visto como um parâmetro da distribuição teórica de resultados para essa
população infinita, portanto trata-se de um número fixo, embora a princípio desconhecido; 
já a proporção de lançamentos realizados que resulta em 5 {\em em uma dada amostra} é um estimador desse parâmetro,
a partir da amostra, e portanto seu valor está sujeito à variações conforme repetimos o experimento diversas vezes.
Qualquer função calculada sobre uma dada amostra é conhecida então como uma {\em estatística} da amostra.
A população idealizada corresponde a um {\em modelo} da realidade, descrito por uma função matemática.
Assim, Fisher explicita o fato de que a inferência estatística, em sua visão, depende da construção de um modelo matemático
abstrato.

Em seu trabalho de 1922, Fisher divide a tarefa da inferência estatística em 3 classes de 
problemas: problemas de especificação, ou seja, a determinação da função
matemática que define a população; problemas de estimação, ou seja, o cálculo de estatísticas sobre a amostra que estimem 
corretamente os parâmetros populacionais; e problemas de distribuição, ou seja, estimações realizadas sobre a distribuição
das estatísticas calculadas sobre a amostra. Fisher sugere brevemente que o problema de especificação é um interesse menor,
se concentrando nos problemas de estimação e distribuição. Sobre especificação, ele escreve:

``We may know by experience what forms are likely to be suitable, and the adequacy of our choice may be
tested {\em a posteriori} [by an objective criterion of goodness of fit].
For empirical as the specification of the hypothetical population may be, this empiricism is cleared of its dangers if
we can apply a rigorous and objective test of the adequacy with which the proposed population represents the whole of
the available facts.''\citep{Fisher1922}

Presumindo uma escolha adequada de modelo, ou seja,	de função matemática que descreva as propriedades da população,
a tarefa da inferência estatística se concentra em estudar o pequeno número de parâmetros desse modelo que a descrevem
completamente. Esse estudo corresponde aos problemas de estimação e distribuição.

Fisher descreve ainda as seguintes propriedades desejáveis para um estimador. Embora as suas definições tenham sido
melhor formalizadas posteriormente, as idéias presentes ainda são de grande valia:
\begin{enumerate}
	\item Consistência: ``when applied to the whole population the derived statistic should be equal to the parameter''
	\item Eficiência: ``that statistic is to be chosen which has the least probable error''
	\item Suficiência: ``the statistic chosen should summarise the whole of the relevant information supplied by the sample''
\end{enumerate}

Para este estudo, no entanto, o ponto mais importante do trabalho de Fisher é a descrição de uma quantidade conhecida como
verossimilhança (likelihood), que será fundamental para contrastar as teorias de inferência. Fisher define essa quantidade,
cujo uso mais imediato é identificar as hipóteses sobre determinado parâmetro que encontram maior suporte das evidências,
como:

``The likelihood that any parameter (or set of parameters) should have
any assigned value (or set of values) is proportional to the probability
that if this were so, the totality of observations should be that observed.''\citep{Fisher1922}

Para melhor compreender essa grandeza, vamos usar um exemplo citado por Laplace \citep{Laplace1814}, que se ocupa da 
proporção de batismos (usado para representar o número de nascimentos) de meninos e meninas em Paris nos anos 
de 1745 a 1784. Laplace tem a hipótese, corroborada por dados coletados em diversas outras cidades, de que nascem mais meninos
do que meninas em uma proporção de $22$ para $21$. Coletando a informação de que houveram $393386$ nascimentos de 
meninos e $377555$
nascimentos de meninas no período, e presumindo o modelo binomial para os nascimentos, a verossimilhança da hipótese levantada
por Laplace é dada pela probabilidade atribuída ao valor $393386$ pelo modelo binomial com tamanho $393386 + 377555 = 770941$
e probabilidade de sucesso $22/(21+22) \sim 0.511$. Essa probabilidade é um número da ordem de $10^{-5}$. Ao calcular a 
verossimilhança da hipótese de que não há preponderância de nenhum gênero, ou seja, a probabilidade de sucesso é de $0.5$, 
encontramos o valor de $10^{-74}$, muitas ordens de grandeza inferior ao anterior. Portanto, os dados indicados por Laplace
fornecem uma evidência mais forte para a hipótese de que os nascimentos de meninos são mais prováveis na proporção de $22$
para $21$.

Laplace não formaliza esse argumento em seu livro, sem dúvida pela dificuldade de realizar as contas indicadas acima sem
a ajuda de computadores. Mas é importante notar que, enquanto Laplace poderia desenvolver esse raciocínio em termos das 
{\em probabilidades} das hipóteses conflitantes, Fisher nota que as verossimilhanças calculadas acima não se comportam como 
probabilidades, no sentido de não estarem sujeitas às leis da probabilidade (veja na sessão \ref{sec:axiomatica}).

\section{Diferentes abordagens para testes de hipóteses}

A estrutura mental desenvolvida por Fisher durante a década de 1920 vai levar também a um procedimento conhecido como teste de 
significância, e que será alvo de uma intensa controvérsia entre ele e a dupla formada por Jerzy Neyman e Egon Pearson, 
durante a década de 1930. Em um terminologia que vai influenciar muito do pensamento em inferência estatística do século XX,
Fisher usa as expressões {\em modelo estatístico} como uma função que descreve a população de interesse, sendo definida 
por um conjunto de parâmetros; e {\em hipótese} como uma afirmação a respeito do valor desses parâmetros. O modelo contém, 
assim, todas as suposições sobre o problema a respeito das quais os dados não irão discriminar, enquanto a hipótese
contém a afirmação que será julgada pela aplicação do teste. Dada uma hipótese, é possível
calcular a distribuição que uma certa estatística teria, se um mesmo experimento fosse repetido infinitas vezes.

O procedimento de teste de significância, segundo Fisher, requer a escolha de um modelo apropriado e a especificação de uma 
hipótese (conhecida como hipótese nula) para a qual a é possível calcular a distribuição da estatística de interesse. 
Após realizar um experimento, a estatística calculada sobre a amostra é comparada com a distribuição teórica sob a hipótese nula,
e é calculada a probabilidade do desvio entre a estatística calculada e a sua esperança ser {\em tão grande ou maior} do que o
encontrado. 

Em um exemplo simples, queremos saber se uma moeda é justa ou não lançando-a 20 vezes. É improvável que o número de caras seja
exatamente 10, mesmo para uma moeda justa. Como o resultado de cada lançamento é
admite duas	respostas (cara ou coroa), e a probabilidade de cada lançamento é suposta constante e independente, o modelo escolhido
deve ser uma binomial. A hipótese nula é a de que a probabilidade da binomial é de 0.5. Nesse caso, podemos calcular a 
distribuição esperada do número de caras, e veremos que em 82\% dos experimentos realizados, a diferença entre o valor esperado
de dez caras e o resultado obtido será de 1 ou mais; em 26\%, de 3 ou mais; e em 4\%, de 5 ou mais. Ao examinar o resultado 
de um experimento, podemos então {\em rejeitar} a hipótese nula, ou seja, entendemos que a moeda não é justa, ou {\em deixar
de rejeitá-la}, ou seja, não encontramos evidência suficiente de que esta hipótese está errada.

Neyman e Pearson fazem uma crítica ao método descrito por Fisher, visto por eles como incompleto, e estabelecem uma metodologia
para escolha entre diversas hipóteses concorrentes, muitas vezes sumarizadas em duas: a nula e a alternativa.
Um procedimento de teste de significância
no qual uma única hipótese sobre o valor de um parâmetro é privilegiada em detrimento de todo o espectro de outras respostas
possíveis é falho. Como o valor de parâmetros é uma variável contínua, Neyman e Pearson argumentam (numa linha muito semelhante
à usada por argumentos Bayesianos):

``If {\em x} is a continuous variable, then any value of {\em x} is a singularity of relative probability equal to zero. We are
inclined to think that as far as a particular hypothesis is concerned, no test based upon the theory of probability can by
itself provide any valuable evidence of the truth or falsehood of that hypothesis.'' \citep{Neyman1933}

Enquanto isso é verdade a respeito de probabilidades, o mesmo não se verifica com verossimilhanças. Esse é um dos motivos
pelos quais a verossimilhança é sempre instrumental em comparar hipóteses alternativas.

Neyman e Pearson propõe um procedimento formal para delimitar quando devemos aceitar e quando devemos rejeitar a hipótese
nula em favor de uma hipótese alternativa, dividindo o resultado desse processo em quatro classes: podemos aceitar uma hipótese
verdadeira ou falsa, ou podemos rejeitar uma hipótese verdadeira ou falsa. O ato de rejeitar a hipótese nula verdadeira
é conhecido como erro de tipo I, e o ato de aceitar uma hipótese nula falsa é conhecido como erro de tipo II. O conceito do 
erro de tipo I encontra paralelo na teoria de Fisher, enquanto o erro de tipo II é ausente, e será contestado por Fisher em
diversas bases. Esse procedimento consiste na construção prévia de uma região crítica, tal que, se os dados observados se
encontrarem nesta região, a hipótese nula é rejeitada em favor da alternativa; caso contrário, a hipótese nula é aceita.

A construção dessa região crítica é feita de forma a tolerar um valor para a probilidade de erro de tipo I, normalmente em
0.05. Um resultado central do trabalho de Neyman e Pearson foi a demonstração de que, entre todas as possíveis regiões críticas,
a região ótima, no sentido de minimizar a probabilidade de erro de tipo II, é aquela construída pela razão entre as 
verossimilhanças \citep{Neyman1933}. 

Convém notar que esse resultado é verdadeiro apenas para hipóteses ditas simples. Enquanto Neyman e Pearson obtém resultados
parciais para testes envolvendo hipóteses compostas, outros autores vão eliminar essas hipóteses das suas considerações.
Edwards, por exemplo, afirma que:

``The class of hypotheses we call `statistical' is not necessarily closed with respect to the logical operations of
alternation (`or') and negation (`not'). For a hypothesis resulting from either of these operations is likely to be composite,
and composite hypotheses do not have well-defined statistical consequences, because the probabilities of occurrence of the
component simple hypotheses are undefined. For example, if {\em p} is the parameter of a binomial model, about which inferences
are to be made from some particular binomial results, `$p = \frac{1}{2}$' is a statistical hypothesis, because its 
consequences are well-defined in probability terms, but its negation, `$p \neq \frac{1}{2}$', is not a statistical hypothesis,
its consequences beign ill-defined.''\citep{Edwards72}

Fisher critica duramente o procedimento de Neyman-Pearson por uma série de motivos, entre os quais o engessamento do procedimento,
que levaria a uma aceitação ou rejeição de hipóteses acriticamente:

``It is a fallacy (...) to conclude from a test of significance that the null hypothesis is thereby established (...).
In an acceptance procedure, on the other hand, acceptance is irreversible, whether the evidence for it was strong or weak.
It is the result of applying mechanically rules laid down in advance; no {\em thought} is given to the particular case,
and the tester's state of mind, or his capacity for {\em learning}, is inoperant.
By contrast, the conclusions drawn by a scientific worker from a test of significance are {\em provisional}, and involve
an intelligent attempt to {\em understand} the experimental situation.''\citep{Fisher1955}

É curioso que um autor que tenha atacado tão intensamente os conceitos de probabilidade subjetiva seja também tão crítico de 
uma abordagem que visa minimizar as decisões subjetivas a serem tomadas por meio de uma estruturação rígida do procedimento
de teste de hipóteses. 

\section{Lógica probabilística}

Tanto de Morgan como Boole, contados entre os fundadores da interpretação subjetivista e frequentista da probabilidade,
se ocuparam do estudo da probabilidade como um subconjunto da lógica formal, que constituía um interesse mais amplo para
ambos. No mesmo espírito, o trabalho de Rudolph Carnap, à partir da década de 1940, vai ligar novamente o estudo da probabilidade
às suas raízes dentro da lógica. A notação usada originalmente por Carnap favoreceu sua comunicação com filósofos e logicistas,
mas dificultou seu acesso por estatísticos, de forma que vamos adaptar alguns termos e notações seguindo textos mais
modernos (\citep{Zabell09} e \citep{Fitelson07}). 

Carnap, primeiramente, reconhece que a palavra ``probabilidade'' é usada em
dois significados diferentes, e portanto separa os conceitos de {\em probabilidade$_1$} e {\em probabilidade$_2$}, o primeiro
se referindo a uma medida de confirmação, enquanto o segundo é uma medida de frequência. Enquanto {\em probabilidade$_1$} é
um conceito útil para a formulação de um sistema de inferência, {\em probabilidade$_2$} pode ver seu uso em leis físicas, por
exemplo. Assim, quando a lei da entropia é formulada em termos probabilísticos, é claro que a visão pessoal de cada sujeito
é irrelevante para o resultado de um experimento físico, como a mistura entre dois gases. (**CONFIRMAR**)

***** Ler Carnap50 e Carnap62

Carnap, seguindo o trabalho de 
Wittgenstein, trabalha com a {\em probabilidade condicional} de uma proposição lógica $h$ dada uma proposição $e$, $c(h, e)$, 
e interpreta essa grandeza como a medida com a qual a evidência $e$ corrobora uma hipótese $h$ \citep{Zabell09}.


*** visao de Carnap sobre probabilidades ***
%http://en.wikipedia.org/wiki/Probability_interpretations
%http://stephanhartmann.org/HHL10_Zabell.pdf
\section{Probabilidade Bayesiana moderna}\label{sec:bayes}

Embora uma grande parcela da comunidade científica tenha adotado uma visão frequentista durante a primeira metade do século XX,
tomando algum dos lados do debate entre Fisher e Neyman-Pearson, muitos autores continuaram seguindo a interpretação 
subjetivista da probabilidade, mais ou menos semelhante à advogada por de Morgan. Entre eles, Frank P. Ramsey, Dennis Lindley,
Harold Jeffreys, Bruno de Finetti e Leonard J. Savage
podem ser apontados como os principais responsáveis por um interesse continuado na interpretação subjetiva, que, próximo à
década de 1960, passou a ser o principal embasamento para uma classe de métodos de inferência conhecidos como Bayesianos. 
Para Ramsey e Savage, a probabilidade surge como um sistema de preferência racional, enquanto para de Finetti, ela representa
uma chance justa em uma aposta; embora as visões difiram no grau de empiricismo advogado,
ambas levam logicamente aos axiomas padrão da probabilidade\footnote{Note-se que as discordâncias entre as posições de 
Savage e de Finetti	dependem da época do texto analisado, com seus pensamentos convergindo durante suas vidas}.

A inferência Bayesiana é devida ao rev. Thomas Bayes, que demonstrou o teorema que leva seu nome, e ao já mencionado marquês de
Laplace, que formulou a metodologia das probabilidades inversas. A contribuição relativa de Bayes e Laplace aos sucessos
e fracassos da teoria Bayesiana é um tópico controverso	entre os que estudam a história da probabilidade\citep{Zabell09}.
Fisher, por exemplo, considera o argumento com o qual Bayes estabelece uma {\em priori} uniforme como um comentário
a respeito do problema específico que Bayes estava estudando, enquanto a escola Bayesiana verá esse argumento
como aplicável a outras classes de problemas\citep{Aldrich08}. Assim, o resultado conhecido como Teorema de Bayes é aceito
por todas as escolas de interpretação, enquanto que o argumento de Laplace 
é, em essência, aceito pela escola subjetivista e completamente rejeitado pela frequentista. 

Talvez a formalização mais convincente da teoria subjetivista de probabilidades é a dada por de Finetti no seu argumento sobre
{\em Dutch books}. Um {\em Dutch book}, ou ``aposta holandesa'' 
é um conjunto de apostas que garante um ganho para um apostador, qualquer que seja o
resultado observado após o jogo. Suponha que um evento vai ser observado (seja uma corrida de cavalos, uma luta, etc), e um
agente pode apostar uma quantia em dinheiro para cada possível resultado desse evento, baseado na sua avaliação subjetiva da
probabilidade de cada resultado, e recebe um prêmio correspondente ao resultado que de fato ocorrer. A relação entre as 
probabilidades e os prêmios é dita {\em incoerente} se uma ``aposta holandesa'' puder ser feita a favor ou contra o agente
(ou seja, garantindo lucro ou perda sempre), e {\em coerente} caso contrário. De Finetti mostra que toda avaliação 
coerente de probabilidades subjetivas implica nos axiomas da probabilidade \citep{deFinetti37}.

Já a teoria de inferência Bayesiana pode ser vista, resumidamente, como uma estrutura de atualização das probabilidades subjetivas
frente a novas evidências. O teorema de Bayes conecta a probabilidade {\em a priori} com a probabilidade {\em a posteriori}
através do uso da verossimilhança dos dados. As grandes questões que precisam ser resolvidas são: como construir uma {\em priori}
a partir do conhecimento prévio (que será retomada na sessão \ref{sec:classref}), e principalmente, como construir {\em prioris}
na ausência de informação sobre o problema.
Note-se que nem todo Bayesiano é totalmente subjetivista: como veremos, é possível empregar o teorema de Bayes a partir de
{\em prioris} objetivas.

A proposição da escola Bayesiana foi acompanhada de um debate intenso, nos quais os argumentos eram não raramente personalizados.
Ronald Fisher foi um árduo crítico
de Laplace, escrevendo que ``We can know nothing of the probability of hypotheses or hypothetical quantities'' \citep{Fisher1921},
e ``The theory of inverse probability is founded upon an error, and must be wholly rejected'' \citep{Fisher1925}, ao mesmo tempo
em que propunha uma metodologia de estimação fiducial, que permitia a inferência absoluta sobre hipóteses sem {\em prioris} 
especificadas para uma classe de ``problemas bem-formulados''. O método fiducial foi fundamentalmente abandonado nos anos 
seguintes à sua morte, sendo chamado por Savage de ``a bold attempt to make the Bayesian omelet without
breaking the Bayesian eggs'' \citep{Savage60}, e é refutado por I. J. Good \citep{Good92}. No entanto, os argumentos de Fisher
são sempre extremamente lúcidos e perspicazes, e a formulação moderna da teoria Bayesiana deve muito à crítica constante
feita por ele sobre os pontos mais sutis da sua lógica.

Com a axiomatização e formulação moderna da interpretação subjetivista, cuja história é intimamente ligada à da teoria
econômica de utilidade (veja por exemplo \citep{Friedman48} e \citep{Pfanzagl67}),
a escola Bayesiana de pensamento ganhou maior aceitação nos meios científicos. Savage escreve sobre essa formulação moderna:

``Personal probability can be regarded as part of a certain theory of coherent preference in the face of uncertainty. This
preference theory is normative; its goal is to help us make better decisions by exposing to us possible incoherences in our
attitudes toward real and hypothetical alternatives.'' \citep{Savage67}

Essa exposição é reminiscente das palavras de de Morgan:

``I throw away objective probability altogether, and consider the word as meaning the state of the mind with respect to an 
assertion (...). `It is more probable than improbable' means in this chapter 'I believe that it will happen more than I 
believe that it will not happen. Or rather `I {\em ought} to believe, \&c.', for it may happen that the state of mind which {\em
is}, is not the state of mind which should be. 
D'Alembert believed that it was {\em two} to {\em one} that the first head which the throw of a halfpenny was to give would occur
before the third throw; a juster view of the mode of applying the theory would have taught him it was {\em three} to {\em one}.
But he {\em believed} it, and thought he could show reason for his belief: to him the probability {\em was} two to one. But 
I shall say, for all that, that the probability {\em is} three to one; meaning that in the universal opinion of those who
examine the subject, the state of mind to which a person {\em ought} to be able to bring himself is to look three times
as confidently upon the arrival as upon the non-arrival.''\citep{deMorgan1847}

\section{Críticas ao pensamento Bayesiano}

Uma crítica da escola frequentista pode ser resumida como: se existirem chances físicas associadas a um evento
e probabilidades subjetivas, não haveria motivo para supor que ambas serão iguais. Há duas respostas para esse problema 
entre os proponentes do subjetivismo:

Por um lado, dadas algumas condições sobre a construção da
probabilidade subjetiva de um ator racional, pode ser demonstrado que as chances físicas de um evento são iguais 
às probabilidades subjetivas às quais ele deve chegar; embora
alguns autores discordem sobre quais são as condições razoáveis para tomar como axioma. Anscombe e Aumann, por exemplo,
escrevem sobre um problema compondo processos do tipo roleta (nas quais as chances físicas de cada resultado são conhecidas) 
com processos do tipo corridas de cavalo (nas quais a chance física é desconhecida) que:

``In this case the subjective probability of any outcome is equal to the [physical] chance associated with that outcome.
Since the two are equal, it does not matter much which word or symbol we use. The
chance refers to the phenomenon, the probability refers to your attitude
towards the phenomenon, and they are in perfect agreement''\citep{Anscombe63}

Contraste-se com essa visão a posição advogada por de Finneti, para quem a probabilidade não precisa ser racionalmente
justificável:

``The subjective theory (...) does not contend that the opinions about probability are uniquely determined and
justifiable. Probability does not correspond to a self-proclaimed `rational' belief but to the effective personal
belief of anyone''\citep{deFinetti51}

Neste sentido, a posição de Anscombe, que privilegia um certo valor racional para a probabilidade de um evento,
se afastam daquela defendida por de Finetti, e se aproxima da lógica probabilística de Rudolph Carnap e do Bayesianismo
objetivo advogado por E. T. Jaynes \citep{Jaynes68}, que se utiliza do princípio da máxima entropia para determinar
{\em prioris} plenamente objetivas.

Outra crítica diz respeito ao fato de que, enquanto subjetiva, a probabilidade percebida por um sujeito pode não corresponder
à probabilidade anunciada por ele. Esse problema é abordado pela teoria de {\em Scoring Rules}, desenvolvida por
de Finetti e Savage durante a década de 1970 \citep{Lindley82}, cujo
desenvolvimento mostra métodos para garantir coerência entre ambas. Dennis Lindley expande esses resultados para mostrar que:

``Let a person express his uncertainty about an event E, conditional upon an event F, by a number x and let him be given, 
as a result, a score which depends on x and the truth or falsity of E when F is true. It is shown that if the scores are 
additive for different events and if the person chooses admissible values only, then there exists a known transform of the 
values x to values which are probabilities. In particular, it follows that values x derived by significance tests, confidence 
intervals or by the rules of fuzzy logic are inadmissible. Only probability is a sensible description of uncertainty.''
\citep{Lindley82}

A escola Bayesiana também é criticada ao assumir uma postura subjetivista, que não seria compatível com o desenvolvimento
da ciência, definida como a construção de conhecimento racional objetivo. Essa oposição, a bem da verdade, é mais reveladora
de uma visão sobre a natureza da ciência do que uma crítica propriamente ao paradigma Bayesiano. Frise-se que mesmo 
avaliações de probabilidade pessoais e subjetivas podem estar abertas ao estudo objetivo, uma posição que remonta ao trabalho
de C.S. Peirce \citep{Stigler78}.
Outro fator importante é a maior percepção moderna
de que a escola frequentista, ao ser obrigada a escolher um modelo mental de referência, é menos objetiva do que se propõe.
Savage escreve que ``the Bayesian approach is more objectivistic than the frequentist approach in that it imposes a greater
order on the subjective elements of the deciding person.'' \citep{Savage60}.

\section{Probabilidade e propensão}

Peirce on randomization (Stigler78)

	** Popper sobre probabilidades **

O interesse de Popper em estudar a probabilidade a partir de descrições da física quântica é natural por motivos. O primeiro
é que há evidências de que a natureza da física quântica é intrinsecamente probabilística \citep{Gudder88}, ao contrário 
de fenômenos
macroscópicos para os quais a descrição probabilística pode ser vista como uma aproximação dada a falta de informações completas.
O segundo é que o estudo de problemas quânticos leva naturalmente a estruturas matemáticas semelhantes às probabilidades, mas
com	propriedades distintas, como as quasiprobabilidades, úteis no estudo de ótica quântica, as quais podem não satisfazer o
axioma de aditividade de Kolmogorov e ter regiões de probabilidade negativa \citep{Mandel95}.

A visão de Popper é, no entanto, criticada por não prover nenhuma base operacional para o cálculo de probabilidades à partir
da definição de propensidades. É inconclusivo se a visão de Popper pode ser considerada uma teoria de interpretação
de probabilidades, pois uma interpretação deve possuir uma determinada estrutura, como cita Peter Milne:

``Interpretations in the literal sense (...) have sufficient intrinsic mathematical structure that one can derive
the characteristics of probability from them.''\citep{Milne93}

\section {O problema do ``Optional stopping''}

Uma crítica muito veemente aos proponentes da inferência frequentista é dada pelo problema da parada opcional, ou 
``optional stopping''. Suponha que um cientista realizou uma série de 12 ensaios de Bernoulli, ou seja, experimentos aleatórios
com dois possíveis resultados (sucesso e falha), independentes e de mesma probabilidade. Após obter 3 sucessos, o cientista
decide testar a hipótese nula de que a probabilidade de sucesso é de 0.5. Pelo paradigma frequentista, esse teste depende
da maneira como o experimento foi projetado. Se o cientista planejava fazer 12 experimentos, a probabilidade de observar 3
ou menos sucessos sob a hipótese nula é de $ \left( {0 \choose 12} + ... + {3 \choose 12}\right) (\frac{1}{2})^{12} = 7.3\%$. No entanto, se o cientista decidiu continuar realizando experimentos até
encontrar o terceiro sucesso, a probabilidade de chegar a doze tentativas é de 
$1 - \left( \frac{1}{2}^3 + {3 \choose 2} \frac{1}{2}^4 + ... + {10 \choose 2} \frac{1}{2}^{11}\right) = 3.3\%$. Ou seja,
dependendo da intenção original do cientista, os mesmos dados podem servir para refutar ou não a hipótese nula.

Para evitar esse tipo de  paradoxo, a sabedoria convencional dos estatísticos frequentistas afirma que um cientista {\em não pode}
examinar os próprios dados enquanto os coleta. Toda a análise estatística deve ser feita posteriormente à coleta. 
Desta forma, o cientista que realizou os 12 testes e encontrou um p-valor de 7.3\%, próximo do nível crítico de 5\%, é
proibido de continuar a coleta de dados. Richard Royall, citando essa prática, diz:

``Subsequent observations, no matter how consistent and convincing, can never justify a claim of statistical significance
at his target level (...). Finding that the early partial results represent evidence that is only fairly strong precludes
the possibility that the evidence in the final results might be quite strong. Does this make sense?'' \citep{Royall97}

Enquanto alguns estatísticos vêem neste problema ``um uso ingênuo de p-valores'' \citep{Good92}, outros, como Richard Royall,
consideram esta situação uma indicação clara de que o paradigma frequentista leva a contradições inerentes, e portanto 
deve ser abandonado. A discordância entre as conclusões frequentistas e a inferência julgada aceitável por seus opositores 
pode ser traçada na incoerência entre
o procedimento de teste de hipóteses e o princípio da verossimilhança. Identificado e demonstrado por Allan Birnbaum em 1962, 
este princípio afirma que toda a informação contida em uma amostra está	contida na função de verossimilhança \citep{Birnbaum62}. 
Desta forma, a inferência feita à partir da amostra deve ser dependente da função de verossimilhança, e não do desenho
experimental.
No exemplo discutido acima, a função de verossimilhança do experimento é a mesma, não importando a intenção do pesquisador:

\begin{equation}
	\mathcal{L}(p|x) \,\, \propto \,\, p^3(1-p)^9
\end{equation}

A razão de verossimilhanças, critério já utilizado por Fisher e Neyman-Pearson, entre os valores de $p=0.5$ e $p=3/12$ é de 
aproximadamente 5, oferecendo uma evidência moderada a favor do valor $p=3/12$. Dentro do paradigma Bayesiano, onde o princípio
da verossimilhança é aceito, essa razão de verossimilhanças deve ser usada para atualizar as probabilidades que cada hipótese
tem {\em a priori}. Como vamos expor na sessão \ref{sec:likelihood}, proponentes da verossimilhança como base da inferência
vão questionar o uso de {\em prioris}, se atendo apenas ao valor da razão de verossimilhanças.

\section{O problema da classe de referência}\label{sec:classref}

Em seu trabalho de 1922, Fisher escreve:

``The framing by means of a model is located at the beginning of the
statistical treatment of a problem of application. The postulate of randomness thus resolves itself into the 
question, `Of what population is this a random sample?' which must frequently be asked by every practical
statistician''\citep{Fisher1922}

Fisher volta a essa questão em 1955, ao contrastar seu método de teste de hipóteses com o procedimento de Neyman-Pearson:

``The root of the difficulty of carrying over the idea from the field of acceptance procedures to that of tests of significance
is that, where acceptance procedures are appropriate, the source of supply has an objective reality, and the population
of lots, or one or more, which could be successively chosen for examination is uniquely defined; whereas if we possess a unique
sample in Student's sense on which significance tests are to be performed, there is always, as Venn (1876) in particular has
shown, a multiplicity of populations to each of which we can legitimately regard our sample as belonging; so that the phrase
`repeated sampleing from the same population' does not enable us to determine which population is to be used to define the
probability level, for no one of them has objective reality, all being products of the statistican's imagination.''
\citep{Fisher1955}

Reference class problem
% http://mcps.umn.edu/philosophy/6_8Kyburg.pdf

\section{A escolha da verossimilhança}\label{sec:likelihood}
Fisher 22, pag 16. em diante

%Livro do Edwards
**** Discutir o livro do Edwards e verossimilhança ****

 pagina 176
* Verossimilhança por Fisher e seu papel na estat frequentista
* Papel da ver. na estat bayesiana

* Data are fixed, pars are random VERSUS pars are fixed, data is random

``The probability model, the set of statistical hypotheses, and the data, form a triplet which is the foundation of
statistical inference. Of the many outcomes, each with a specified probability given the hypothesis, which could have
occurredon the basis of the accepted model, one {\em has} occurred - the data. What can they reveal about the hypotheses?''
\citep{Edwards72}


Definições e teoremas de verossimilhança em Edwards, pp 29-3?
Sufficiency p. 35

``No special meaning attaches to any part of the area under a likelihood curve, or to the sum of the likelihoods of two or more
hypotheses.
(...)
Altough the likelihood function, and hence the curve, has the mathematical form of a [known] distribution, it does not
represent a statistical distribution in any sense.''\citep{Edwards72}

* Pernerger \& Courvoisier, 2010: likelihood ratios are a more natural way of understanding evidence

Edwards propõe que definir a natureza da probabilidade é um problema irrelevante, dado que a evidência estatística é
formulada por razões de verossimilhança, e não por probabilidades. No entanto, a definição de verossimilhança {\em é} uma
probabilidade, de forma que sem uma interpretação para a probabilidade não podemos ter uma interpretação para a verossimilhança.
A teoria verossimilhantista corre, portanto, o risco de se assentar sobre uma areia movediça filosófica.

Fitelson: crítica em termos lógicos à verossimilhança. Middle way?

\section{A abordagem por teoria da informação}

%http://books.google.com.br/books?id=DlP_h4aMhiYC&printsec=frontcover#v=onepage&q&f=false
Anderson 2008, AIC, Kulback-Liebler

\section{Conclusões}

Concluímos essa sessão com a conclusão de que a divisão rotineiramente feita entre as ditas ``estatística frequentista'' e 
``estatística Bayesiana'' escondem o fato de que há, na verdade, uma variedade de correntes filosóficas que, baseadas
em idéias distintas sobre a natureza das probabilidades, chegam a conclusões distintas sobre assuntos díspares da epistemologia 
à metodologia de inferência. A escola frequentista é fundamentalmente dividida entre a modelagem de Fisher e a rigidez de
Neyman-Pearson, enquanto a escola Bayesiana é dividida entre os subjetivistas como de Finetti e os objetivistas como Jaynes.
Essa taxonomia, no entanto, não acomoda a visão de positivistas lógicos como Carnap e Karl Pearson, que atacarão tanto
a visão subjetiva de probabilidade quanto o modelo de população infinita dos frequentistas \citep{Lenhard06, Zabell09}.
Tampouco é suficiente para enquadrar o trabalho de Peirce sobre propensões, ou a visão agnóstica de Edwards, que propõe
que a verossimilhança é a única base lógica para a inferência.

É necessário que as divergências entre os diversos autores citados sejam interpretadas não na sua práxis, que representa 
o nível metodológico, como costuma acontecer, e sim nos seus fundamentos, em cada suposição elementar feita sobre os
elementos constituintes da teoria probabilística e de inferência. Possivelmente, essas suposições devem ser mapeadas em
distintos elementos da filosofia da ciência.



** O papel de Carnap e Karl Pearson entre os positivistas lógicos; críticas a visão subjetiva E a população infinita de Fisher **
(Lenhard06 e Zabell09)

**** discutir a visão de Popper ****
``Those theories which are under a probabilistic form (thermodynamics, statistical [mechanics?], etc.) can never be refuted (...):
after all, even an event with probability 0 can well occur. According to Popper we then have to take a methodological decision
and consider a probabilistic theory {\em T} refuted if a certain event occured which, as far as {\em T} is concerned, would 
otherwise be extremely unlikely.'' \citep{deFinetti2010}

*** positivismo lógico VERSUS falsificacionismo?? Lakatos? ***

*** Kuhn e o papel do subjetivismo na ciência ****

*** Hume sobre inducao. Paper do Savage sobre induction  Mais Fitelson??  ***

*** Simberloff e testes de hipoteses ***

Ciência enquanto descrição/compreensão versus tomada de decisão científica. 'Ultimo paragrafo de Fisher55

A discussão metodológica na aplicação da estatística às ciências práticas deve estar subordinada a uma discussão
epistemológica e filosófica sobre a verdadeira base do pensamento estatístico. Nesse sentido, a apresentação de
métodos baseados na verossimilhança no presente trabalho não trata de rejeitar o uso de testes de hipótese 
e procedimentos frequentistas em
bases pragmáticas, e sim na construção de ferramentas e na aparelhagem do pesquisador com métodos que sejam
coerentes com uma visão de ciência enquanto descrição/compreensão do mundo natural.

