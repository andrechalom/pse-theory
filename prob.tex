\section{A história e a natureza da probabilidade}

John Venn, no prefácio do seu estudo de 1866, escreve:

\begin{quote}
``The science of Probability occupies at present a somewhat anomalous position. It is impossible, I think, not to observe
in it some of the marks and consequent disadvantages of a {\em sectional} study. By a small body of ardent students it
has been cultivated with great assiduity, and the results they have obtained will always be reckoned among the most 
extraordinary products of mathematical genius. But by the general body of thinking men its principles seem to be regarded
with indifference or suspicion. Such persons may admire the ingenuity displayed, and be struck with the profoundity
of many of the calculations, but there seems to them, if I may so express it, an {\em unreality} about the whole treatment
of the subject. To many persons the mention of Probability suggests little else than the notion of a set of rules, very
ingenious and profound rules no doubt, with which mathematicians amuse themselves by setting and solving puzzles.''
\citep{Venn1866}
\end{quote}

No século e meio que se passou desde esta publicação, a aplicação da estatística se tornou uma obrigatoriedade no meio científico.
Em qualquer periódico renomado da área, é virtualmente impossível publicar um artigo experimental que não 
mencione alguma propriedade
estatística a respeito das amostras coletadas ou de resultados experimentais obtidos. 
Por muitos anos, a estatística vigente nas análises biológicas teve uma
forte inspiração frequentista, com testes de hipóteses e valores {\em p} sendo requeridos para publicações. 
No entanto, existe uma ampla e duradoura crítica à forma como essa estatística é
utilizada nas ciências biológicas e na área de medicina \citep{Ioannidis05, Burnham11, Barber14, Gardner86}.

Muito da crítica decorre da interpretação
simplista assumida por muitos cientistas a respeito do valor {\em p} reportado sobre um problema, mas as suas raízes devem
ser traçadas mais profundamente em uma falta de consciência dos cientistas sobre as interpretações a respeito da natureza
das probabilidades que são pressupostas por uma escola de pensamento, mas raramente examinadas profundamente. 
De outro lado, análises recentes sugerem que o tamanho dos efeitos em várias áreas da ecologia são tão pequenos que estes
dificilmente serão corretamente detectados pelos procedimentos tradicionais \citep{Jennions03}. Em áreas como a ecologia do
comportamento, o poder dos testes estatísticos é muito pequeno, e esse problema é acentuado quando são utilizados métodos
(como o método de Bonferroni) para corrigir o valor {\em p} reportado. A solução para esses problemas pode não estar ligada
a desenvolver novos métodos ou procedimentos estatísticos, mas na análise crítica e racional dos pressupostos por trás
dos testes frequentemente utilizados.

Novamente, citamos John Venn:

\begin{quote}
``Students of Philosophy in general have thence conceived a prejudice against Probability, which has for the most part
deterred them from examining it. As soon as a subject comes to be considered `mathematical' its claims seem generally,
by the mass of readers, to be either on the one hand scouted or at least courteously rejected, or on the other hand
to be blindly accepted with all their assumed consequences. Of impartial and liberal criticism it obtains little or nothing.''
\citep{Venn1866}
\end{quote}

Embora o debate sobre a natureza da probabilidade tenha sido fervorosamente promovido por matemáticos e estatísticos,
este não alcança, muitas vezes, o cientista que faz uso do ferramental da probabilidade para resolver um problema particular.
Assim como o debate matemático era ignorado por estudantes de filosofia à época de Venn, hoje o debate filosófico e 
epistemológico é ignorado por estudantes da área de ciências e cientistas. Isso pode ser parcialmente atribuído 
ao aumento histórico na especialização de currículos - o que diminui o entusiasmo e desempenho dos estudantes em áreas não
relacionadas à sua especialidade \citep{Popham04}.

Para poder discorrer sobre a estimação de incertezas e a atribuição de probabilidades a diferentes resultados de um modelo,
é necessário examinar com atenção o que entendemos sobre probabilidade. O conceito por trás da palavra probabilidade vai
embasar não apenas o que entendemos pela palavra probabilidade em si, mas também traz um significado para os conceitos de
incerteza, inferência e simulação, que serão de importância para a discussão a seguir. 

Embora o debate surja como uma questão sobre a natureza das probabilidades, vamos encontrar as divergências práticas
em torno de questões de inferência. Um problema simplesmente de atribuição de probabilidades sobre um experimento mental,
que seja bem formulado, poderá ter a mesma solução, não importando a afiliação filosófica de quem o resolve. Esse problema 
pode ser, por exemplo, qual a probabilidade de uma moeda justa cair com a cara para cima em pelo menos dois lançamentos de 
três; qual a chance de pelo menos dois alunos em uma classe de trinta fazerem aniversário no mesmo dia; qual a probabilidade 
de uma única observação de uma distribuição normal cair no intervalo de $x$ até $x+dx$. As escolas vão divergir principalmente
no procedimento adotado para conectar essas experiências mentais com problemas vindos de observações reais. 

Esse texto é resultado de uma investigação breve sobre a história dos conceitos de probabilidade, inferência, e incerteza,
e sua relação mais geral com visões alternativas de ciência. Ele foi motivado por uma percepção de que a vasta maioria dos
textos introdutórios em estatística se foca em uma escola de pensamento, alinhada com as visões do autor, sem uma análise crítica
aprofundada das suas relações complexas com as demais. A separação da estatística entre as escolas frequentista e bayesiana,
muito repetida em textos introdutórios, esconde uma riqueza de detalhes, envolvendo contradições importantes dentro de cada
escola e convergências nos pensamentos entre escolas diferentes. Essa separação também apaga a importância de definições que
não se encaixam em nenhuma das alternativas, como a lógica probabilística de Carnap e o apelo à verossimilhança enquanto 
única base de inferência. A presente exposição é, por força, limitada e fragmentária; muitos pontos mais finos da argumentação
foram deixados de fora para que o texto não se tornasse proibitivamente longo. É um lugar comum afirmar que cada 
seção aqui poderia ser expandida para um livro; algumas delas na verdade são esforços para resumir livros já escritos.

\section{Contra uma teoria axiomática}\label{sec:axiomatica}
O ensino moderno da probabilidade é, em larga escala, baseado na axiomatização proposta por Andrey Kolmogorov, matemático
soviético muito prolífico em diversas áreas da ciência. Os axiomas da probabilidade podem ser escritos como \citep{Morettin09}:

\begin{description}
	\item[Não-negatividade] A probabilidade de um evento é um número real não-negativo: $P(E) > 0$.
	\item[Unitariedade] A probabilidade de um evento certo é 1: $P(\Omega) = 1$.
	\item[Aditividade] A probabilidade de eventos mutuamente exclusivos é aditiva: $P(U \cup V) = P(U)+P(V)$.
\end{description}

Uma abordagem puramente axiomática para a probabilidade, embora seja útil sob uma perspectiva estritamente matemática, como
para resolver problemas resultantes do cálculo da probabilidade associada a um experimento mental, 
é insuficiente ao ser aplicada ao conhecimento científico de experimentos empíricos, por não
levar a uma conclusão sobre no que {\em consiste} uma probabilidade. A proposição desses axiomas não leva a nenhuma conclusão 
sobre problemas reais: não há como conectar uma proposição teórica derivada dos axiomas da probabilidade a um problema
prático sem assumir, em algum momento, um significado para a palavra probabilidade que não está contido nos axiomas. Uma posição
que pode ser tentadora é presumir que probabilidade assume {\em qualquer} significado coerente com os axiomas: essa posição,
no entanto, também falha por não restringir o tipo de proposição ao qual probabilidades podem ser associadas (veja a seção
\ref{sec:classref}). 

Outro problema da visão estritamente axiomática é que alguns problemas decorrentes da axiomatização podem passar despercebidos. 
Considere a troca do axioma
de aditividade pelo seguinte axioma de aditividade completa (ou $\sigma$-aditividade):

\begin{description}
	\item[Aditividade completa] Qualquer conjunto contável de eventos mutuamente exclusivos satisfaz:

		$P(E_1 \cup E_2 \cup \cdots) = \sum_{i=1}^\infty P(E_i)$
\end{description}

Esse axioma é uma generalização natural do axioma de aditividade a conjuntos infinitos (contáveis) de eventos, e é amplamente
aceito. No entanto, tome um sorteio de um número racional aleatório entre 0 e 1, com distribuição uniforme. É fácil ver que,
já que existem infinitos números racionais em qualquer intervalo intervalo,
a probabilidade de sortear cada número racional individualmente é zero; logo a probabilidade do evento certo, que é 1, 
pelo axioma da unitariedade, deve ser também 0, pois é a soma de infinitos termos iguais a zero.

Para encontrar uma definição de probabilidade que permita conectar os axiomas a questões práticas do uso de probabilidades na
ciência, vamos repassar a opinião de alguns dos grandes pensadores sobre o tema nas próximas sessões.

\section{Interpretação clássica}
A teoria matemática da probabilidade remonta aos séculos XVI e XVII, nos quais Girolamo Cardano, Galileu Galilei, Blaise Pascal
e Pierre Fermat desenvolveram métodos para resolver problemas envolvendo combinações de resultados em jogos de dados e outros
jogos de azar. 

Essa origem, juntamente com a facilidade e a universalidade de problemas envolvendo jogos de azar, explicam porque tantos textos
introdutórios sobre estatística empregam exemplos envolvendo rolar dados e tirar cartas de baralhos. Embora a simplicidade
destes exemplos ajude a transmitir os conceitos de probabilidade com facilidade, isso causa muitas vezes a sensação de que
as regras da probabilidade servem apenas para os casos simples, e que estas não se aplicam, ou não se adequam totalmente aos 
problemas de natureza complexa com os quais a biologia e a ecologia têm de enfrentar. De fato, é muito difícil justificar que
questões como ``qual a chance de que uma população de jararaca-ilhoa entre em extinção nos próximos dez anos?'' sejam embasadas
nas mesmas idéias que ``qual a chance de uma moeda jogada para cima dar coroa?''. É mais natural tratar a primeira pergunta como
um questionamento sobre o nosso conhecimento atual dos processos biológicos e da condição de vida dos indivíduos dessa espécie
do que um questionamento sobre um processo físico simples que pode ser repetido um grande número de vezes. 

O tamanho desta disparidade entre os exemplos simples e os problemas mais práticos faz com que muitas vezes
textos sobre probabilidades empreguem definições conflitantes sobre probabilidades: às vezes,
elas são usadas para representar algo pessoal e subjetivo, como um nível de confiança; outras vezes são apresentadas como a
razão entre diferentes contagens.

A primeira tentativa influente de definir formalmente o conceito de probabilidade vem em 1814,
com o trabalho de Pierre Simon de Laplace\footnote{
	Há uma série de trabalhos mais antigos sobre a natureza das probabilidades, mas estes ou não propuseram uma definição 
	explícita, como o {\em Ars conjectandi} de Jakob Bernoulli (1713) ou não tiveram grande influência
	na comunidade científica, como o {\em Liber de ludo aleae} de Cardano, escrito em 1525 e publicado apenas em 1663.
}:

\begin{quote}
``La théorie des hasards consiste à réduire tous les évènemens du même genre, à un certain nombre de cas également
possibles, c'est-à-dire tels que nous soyons également indécis sue leur existence, et à determiner le nombre de cas
favorables à l'évènement dont on cherche la probabilité. Le rapport de ce nombre à celui de tous les cas possibles,
est la mesure de cette probabilité'' 
\citep{Laplace1814}
\end{quote}

(``A teoria da probabilidade consiste em reduzir todos os eventos de um mesmo tipo a um certo número de casos igualmente
possíveis, isso é, tal que sejamos igualmente indecisos sobre a sua ocorrência, e a determinar o número de casos
favoráveis ao evento ao qual buscamos a probabilidade. A razão deste número para o número de todos os casos possíveis
é a medida desta probabilidade'')

Para Laplace, todos os eventos que presenciamos seriam resultados de leis físicas
imutáveis, e uma inteligência superior, dotada do conhecimento do estado do universo em um dado instante, poderia prever
todos os eventos futuros sem qualquer incerteza. No entanto, nosso conhecimento limitado, tanto do estado do universo
quanto das leis que o regem, faz com que não possamos fazer previsões para um grande número de sistemas. O estudo das 
probabilidades se coloca, então, como um apoio ao nosso poder de realizar previsões sobre o universo, 
enquanto não possuímos o conhecimento das leis e estados necessária para realizar previsões certeiras. 

Uma dificuldade com essa definição de Laplace é que a demarcação de ``eventos igualmente prováveis'' deve ser feita
com base em argumentos que não partam da idéia de probabilidade; caso contrário, podemos incorrer em raciocínios
circulares. Por exemplo, se uma moeda tem 2 lados perfeitamente simétricos, e a lançamos de forma a não privilegiar um dos lados,
podemos dizer que o número de eventos totais é dois, e o número de eventos favoráveis a tirar coroa é um. Não há qualquer motivo
para supor que um dos lados seja mais propenso a cair para cima que o outro, visto a moeda ser perfeitamente simétrica; logo,
a probabilidade de tirar coroa em um lançamento é de $\frac{1}{2}$. Aqui, o ato de atribuir iguais chances aos dois
lados da moeda vem de um argumento físico de simetria. Isso é dizer que, para Laplace,
a ciência da probabilidade deve se basear em leis físicas do mundo natural, e não no nosso estado presente de conhecimento 
sobre o mundo. A probabilidade é, desta forma, algo que existe objetivamente.
Esta interpretação faz com que as leis da probabilidade, estritamente, só possam ser aplicadas para 
sistemas simples, para os quais leis simples e com poucas premissas possam ser formuladas,
como é o caso de baralhos bem embaralhados ou dados perfeitos.
Laplace, no entanto, não parece ter levado essa 
definição à suas últimas consequências, já que ele discute no tratado citado acima problemas referentes a fazer inferências 
sobre testemunhos judiciais, nos 
quais a testemunha poderia mentir, com uma certa probabilidade, ou ter se equivocado, com outra probabilidade. 
Presumindo que as probabilidades estão corretas, o método usado por Laplace é conhecido como ``método das probabilidades
inversas'', e tem uma alta importância na história da estatística.  No entanto, 
nenhuma explicação convincente é dada sobre como medir essas probabilidades dentro do paradigma clássico.
%% PI: Se o método de probabilidade inversa é importante para seu
%% argumento aqui é preciso explicar melhor e deixar mais clara a
%% antítese com a concepção de probabilidade apres netada no trecho anterior. <TODO>
Deve-se notar que o argumento de Laplace se assemelha ao raciocínio desenvolvido pelo rev. Thomas Bayes 
em seu trabalho publicado
postumamente em 1763. Esse trabalho não será discutido a fundo aqui, por ser muito convoluto em seus detalhes e notação,
e não oferecer uma visão clara sobre a natureza das probabilidades. No entanto, vamos considerar 
o impacto que esse trabalho
teve sobre o pensamento estatístico na seção \ref{sec:bayes}.
%% PI: Não ficou claro o propósito deste trecho finalizando essa seção. No
%% início parece que vc não vai
%% considerar o método de probabilidades inversas (logo Bayes) mas em
%% seguida diz que será tratado em outra seção. C: reformulei

\section{Interpretação subjetivista}
Uma visão alternativa é dada por Augustus de Morgan, conhecido principalmente por suas leis em lógica proposicional, 
no seu tratado Formal Logic, de 1847. Para de Morgan, a única certeza que podemos ter é a de nossa própria existência. 
Este conhecimento
não é passível de ser refutado. No entanto, qualquer outra proposição feita deve ser acompanhada por 
um {\em grau de conhecimento} subjetivo:

\begin{quote}
``It will be found that, frame what circumstances we may, we cannot invent a case of purely objective probability.
I put ten white balls and ten black ones into an urn, and lock the door of the room. I may feel well assured that,
when I unlock the room again, and draw a ball, I am justified in saying it is an even chance that it will be a white one.
If all the metaphysicians who ever wrote on probability were to witness the trial, they would, each in his own sense and 
manner, hold me right in my assertion. But how many things there are to be taken for granted! Do my eyes still distinguish
colours as before? Some persons never do, and eyes alter with age. Has the black paint melted, and blackened the white balls?
Has any one else possessed a key of the room, or got in at the window, and changed the balls? We may be {\em very sure},
as those words are commonly used, that none of these things have happened, and it may turn out (and I have no doubt will do so,
if the reader try the circumstances) that the ten white and ten black balls will be found, as distinguishable as ever, and
unchanged. But for all that, there is much to be assumed in reckoning upon such a result, which is not so objective (in the
sense in which I used the word) as the knowledge of what the balls were when they were put into the urn.'' \citep{deMorgan1847}
\end{quote}

A teoria de probabilidades segundo de Morgan, portanto, lida com graus de conhecimento subjetivos. Assim, ao perguntar para
uma pessoa comum qual é a chance de que um lançamento de moeda resulte em cara, essa pessoa pode responder $\frac{1}{2}$, e 
esta é a medida de probabilidade correta, dada a informação que ele possui sobre o problema. Uma outra pessoa, que sabe
que esta moeda é viciada, ou que é capaz de jogar a moeda de forma a privilegiar um resultado, pode dar outra resposta
completamente diferente, e ainda assim estará correta. Para de Morgan, o passo essencial na construção de uma probabilidade
consiste em {\em medir} o grau de certeza que temos em uma proposição. Algumas proposições são fáceis de medir, como
``dois mais dois são quatro'', ``um lançamento de uma moeda justa vai dar coroa'', e ``dois mais dois são cinco'', aos quais
podemos atribuir facilmente as probabilidades de 1, $\frac{1}{2}$ e 0. Essas medidas, juntamente com o senso de que
algum evento é mais ou menos provável que outro, podem ser usados para construir uma escala subjetiva de probabilidades.
Como essa visão sobre probabilidades parte exclusivamente do conhecimento que temos, e não de uma realidade objetiva,
a probabilidade que queremos determinar não é algo que exista à parte no mundo; ela existe somente enquanto descrição
de um processo mental.

Um empecilho ao uso dessa interpretação de probabilidades se dá quando percebemos que, quanto mais complexo é o 
problema que queremos resolver, mais imperfeita se torna nossa sensação de conseguir ordenar a probabilidade referente
a diferentes resultados de um experimento. É fácil apreender que, se eu jogar duas moedas para cima, a chance de que ambas 
resultem em cara é menor do que a chance de que um único lançamento resulte em cara. É consideravelmente mais difícil encaixar
nessa ordenação a frase ``ao escolher uma letra aleatória da obra completa de Shakespeare, ela será E ou A''; mas esse caso
é passível de uma análise exaustiva. No entanto, questões como ``qual
a probabilidade de que o signo zodiacal
de uma pessoa afete
suas características, como sociabilidade ou perseverança?'', embora perfeitamente válidas para a visão de de Morgan, apresentam
grande dificuldade em ser respondidas nesse paradigma.

A visão subjetiva de probabilidades permite ainda que frases cotidianas, como ``é provável que chova hoje'' ou ``existe uma grande
chance de encontrarmos o José hoje'', sejam tratadas pelo mesmo ponto de vista que os jogos de azar. Mas
embora essa interpretação seja de grande utilidade por permitir a generalização do conceito de probabilidade para problemas
mais gerais do que os sistemas físicos simples contemplados pela interpretação clássica, ela não garante que o valor encontrado
para a probabilidade de um evento seja coerente para diversas pessoas. Isso irá embasar a crítica de que uma visão subjetivista 
de probabilidade pode ser perfeitamente adequada para embasar uma
decisão pessoal, mas falhar como  
uma ferramenta de avanço da ciência, entendida como um corpo teórico objetivo e independente das opiniões de cada cientista individualmente.

\section{Interpretação frequentista}
A próxima crítica à visão de Laplace 
%% PI: Uma leitura descuidada poderia considerar a interpretação
%% frequentista como um mero refinamento da concepção mecanística
%% Laplaciana. De fato há pontos de contato. Esta seção pode avançar
%% mais nas concordâncias e diferenças entre estas duas
%% concepções. 
vem de uma série de pensadores dentre os quais se destacam George Boole e John Venn.
Este último, em seu trabalho de 
1866, postula que a probabilidade de um evento se refere à
frequência relativa com a qual esse evento se repetiria em uma
série infinita de experimentos, pelo que sua visão será conhecida como frequentista:

\begin{quote}
``We may define the probability or chance (the terms are here regarded as synonymous) of the event happening in that
particular way as the numerical fraction which represents the proportion between the two different classes in the long run (...).
This assumes that the series are of indefinite extent, and of the kind which we have described as possessing a fixed type.
If this be not the case, but the series be supposed terminable, or regularly or irregularly fluctuating, (...) the series ceases
to be a subject of science. What we have to do under these circumstances, is to substitute a series of the right kind for
the inappropriate one presented by nature, choosing it, of course, with as little deflection as possible from the observed facts.
This is nothing more than has to be done, and invariably is done, whenever natural objects are made subjects of strict science.''
\citep{Venn1866}
\end{quote}

A probabilidade de um dado evento poderia então ser definida como o limite da razão entre o número de eventos
favoráveis sobre o número de experimentos realizados, com o número de experimentos realizados indo para o infinito. Com 
$n_x$ sendo o número de eventos favoráveis e $n_t$ sendo o número de experimentos realizados:

\begin{equation}
	P(x) = \lim_{n_t \rightarrow \infty} \frac{n_x}{n_t} 
\end{equation}

Venn também critica a visão subjetivista de de Morgan, sob
o argumento de que uma avaliação individual está sujeita a
um enorme número de fontes de erro:

\begin{quote}
``Our conviction generally rests upon a sort of chaotic basis
composed of an infinite number of inferences and analogies
of every description, and there moreover distorted by our
state of feeling at the time, dimmed by the degree of our
recollection of them afterwards, and probably received
from time to time with varying force according to the way in
which they happen to combine in our consciousness at the 
moment.''\citep{Venn1866}
\end{quote}

Venn propõe um conceito de probabilidade que é independente da pessoa que vai medir a probabilidade. A probabilidade
de um experimento físico será a mesma para qualquer pesquisador que realizar os mesmos passos para medi-la. Isso
tem consequências para o uso da probabilidade na ciência e também na moral: convém lembrar que um dos casos em que Laplace e
Bernoulli empregam probabilidades é no apoio ao julgamento de réus. Uma medida de probabilidade que independa de quem
a fornece vai fornecer um peso argumentativo maior durante um julgamento do que uma probabilidade que está necessariamente
atrelada ao estado de conhecimento de um indivíduo particular.

A definição frequentista leva a um problema claro: não temos acesso a infinitas experiências para
determinar o valor de uma dada probabilidade. A série infinita postulada por Venn precisa, para
qualquer fim prático, ser identificada com uma realização parcial observável. No entanto,
essa realização parcial pode ser identificada com infinitas séries diferentes, algumas das quais convergentes,
outras divergentes. O pesquisador frequentista precisa, assim, tomar uma decisão pessoal sobre
qual série utilizar. Ao embasar esse modelo em princípios físicos (dois lados de uma moeda caem com igual chance),
ele estará se aproximando da concepção física de Laplace; ao escolher qualquer outra série, ele estará
transferindo a subjetividade da medida de probabilidade para a escolha do modelo.

Nenhum critério objetivo garante que uma destas séries potenciais é aquela com ``menor desvio possível
em relação aos fatos observados''; a Lei dos Grandes Números \citep{Loeve77} vai embasar uma determinada escolha,
que a média amostral de uma variável aleatória converge quase certamente para sua esperança. 
Embora essa lei forneça uma
justificativa para pensar na probabilidade como uma média a longo prazo, é importante frisar que ela não implica a
interpretação frequentista: a atualização de crenças Bayesiana vai levar ao mesmo resultado com amostras suficientemente
grandes.

Mesmo quando temos acesso a um grande conjunto de dados para estimar o valor de uma determinada
probabilidade, esse conjunto pode ser suficientemente heterogêneo para que nossa conclusão se torne incorreta. Como escreve Venn,

\begin{quote}
``At the present time the average duration of life in England may be, say, forty years; but a century ago it was decidedly less;
several centuries ago it was presumably very much less (...). Let us assume that the regularity is fixed and permanent. It is
making a hypothesis which may not be altogether consistent with fact, but which is forced upon us for the purpose of securing
precision of statement and definition.''\citep{Venn1866}
\end{quote}

Assim, para a visão frequentista a probabilidade que queremos medir é algo que existiria em um mundo idealizado no qual fosse
possível repetir o experimento infinitas vezes, sem haver variações alheias ao fenômeno estudado. Dentro dessa idealização,
a probabilidade medida se torna algo tangível e objetivo. No entanto, para qualquer problema real, essa objetividade
apenas se concretiza dentro da idealização de um dado modelo, para o qual temos que presumir uma certa regularidade. 
% PI: Aqui há dois elementos importantíssimos da noção freuqentista:
% modelo de variável aleatória e a suposição de que este modelo é o
% mesmo na escala de tempo e espaço de um estudo. Isso implica na
% concepção de parâmetro e estimativa frequentistas.

Em sua monografia Likelihood, de 1972, A.W.F. Edwards, escreve:

\begin{quote}
``Though the notion of a random choice is not devoid of philosophical difficulties, I have a fairly clear idea of what I mean
by `drawing a card at random'. That the population may exist only in the mind, an abstraction possibly infinite in extent,
raises no more (and no less) alarm than the infinite straight line of Euclid. I am only prepared to use probability to describe
a situation if there is an analogy between the situation and the concept of random choice from a defined population.''
\citep{Edwards72}
\end{quote}

Finalmente, a concepção frequentista, apesar de ter pontos de contato com o pensamento laplaciano, apresenta
uma ruptura com a metodologia de probabilidades inversas. Para um frequentista, uma probabilidade é algo objetivo
e mensurável, de forma que não faz sentido atribuir uma probabilidade a uma hipótese. 
Tampouco faz sentido tratar uma hipótese estatística como uma sentença escolhida aleatoriamente 
de uma população de sentenças, algumas das quais são verdadeiras.
Desta forma, a escolha da definição frequentista de probabilidade implica abandonar o método
das probabilidades inversas para comparar o mérito de diferentes hipóteses.
Essa percepção vai levar ao desenvolvimento da escola de testes de significância e à 
proposição do conceito de verossimilhança.

\section{Interpretação por propensões}

Uma terceira interpretação para a natureza da probabilidade é dada pelo cientista e lógico norte-americano C.S. Peirce,
cuja obra prolífica se estende sobre vários ramos da estatística moderna. 
O interesse de Peirce no estudo de probabilidades
deve ser traçado na sua visão indeterminista, segundo a qual eventos não 
se sucedem de maneira estritamente causal, mas são
influenciados por um elemento de chance. Peirce baseia essa visão 
na constatação de que ``the mind is not subject to `law'
in the same rigid sense that matter is (...). There always remains 
a certain ammount of arbitrary spontaneity in its action''
\citep{Peirce1892}. Os objetos do mundo material também 

A oposição de Peirce a às outras escolas filosóficas ocorre, então, em um nível mais fundamental do que dicotomia entre
subjetivistas e frequentistas: enquanto para os autores anteriores, notadamente Laplace, a realidade física 
é determinística, para Peirce a própria realidade é aleatória. Enquanto Laplace propõe uma visão de ciência que visa aproximar nosso
grau de conhecimento da onisciência - quando então a probabilidade deixaria de existir - Peirce abandona esse programa em favor
de uma ciência que deve lidar constantemente com a incerteza.

Se a experiência no mundo natural é intrinsecamente atormentada pela sorte, ela não pode ser base para um sistema de
conhecimento baseado estritamente na dedução. Peirce irá desenvolver
portanto um sistema de argumentação baseada em probabilidades, como o argumento de Dedução Provável Simples \citep{Fetzer93}:

\begin{enumerate}
	\item Uma proporção $p$ de As são Bs
	\item X é um A
	\item Portanto, X é um B com probabilidade $p$ 
\end{enumerate}

Peirce é certamente um objetivista, rejeitando qualquer utilidade para probabilidades pessoais; mas sua visão sobre a conexão entre o
mundo físico e a probabilidade é uma questão que o afastará da visão frequentista.
 
Peirce observa ainda um descompasso entre a concordância da frequência de ocorrência
de uma série de experimentos e sua probabilidade com a {\em definição} da probabilidade como sendo essa frequência. Para
essa visão, a probabilidade de um evento específico não pode ser definida. ``Qual a probabilidade de que um lance qualquer
de moeda dê cara?'' é uma pergunta que tem resposta no paradigma frequentista, enquanto ``qual a probabilidade de que 
{\em o próximo lance de moeda que eu fizer} dê cara?'' (e qualquer outra pergunta sobre probabilidades de caso único)
é uma pergunta que não pode ser respondida. Peirce, portanto, lança
mão de uma propriedade física intrínseca descrita como ``would-be'':

\begin{quote}
``I am, then, to define the meanings of the statement that the {\em probability} that if a dice be thrown from a dice
box it will turn up a number divisible by three, is one third. The statement means that the dice has a certain `would-be';
and to say that it has a certain `would-be' is to say that it has a property, quite analogous to any {\em habit} that a man
might have. Only the `would-be' of the dice is presumable as much simpler and more definite than the man's habit as the
dice's homogeneous composition and cubical shape is simpler than the nature of the man's nervous system and soul.''
(C. S. Peirce, CP 2.664, in \citep{Fetzer93})
\end{quote}

A interpretação de ``would-bes'' representa uma quebra fundamental com a interpretação frequentista, sem deixar de caracterizar
probabilidade como uma propriedade objetiva, e independente de um observador subjetivo. C. S. Peirce trata o ``would-be'' como
uma característica primitiva, que será indiretamente conectada com a experiência. Aceitar a existência de ``would-bes'', 
embora possa causar espanto ao cientista moderno, acostumado com a visão 
frequentista, não é diferente de aceitar outras propriedades
primitivas presentes nas teorias físicas, como massa, carga elétrica
ou {\em spin}.
% PI: Você afirma isso no sentido de propriedades fenomenológicas que foram
%atribuídas a objetos para tornar seu comportamento coerente com uma
%certa teoria? Note que carga e mais recentemente massa não são mais
%propriedades deste tipo, pois foram decompostas em processos ou
%componentes mais elementares. Há aí uma distinção essencial entrte
%ser fenomenológico por princípio ou por estratégia. Qual a posição de Peirce?

Peirce também é um pioneiro no desenho de experimentos, dando grande ênfase à criação de desenhos amostrais aleatorizados.
Nessa linha, ele vai argumentar contra o uso de ferramentas estatísticas desenvolvidas sob
hipótese de randomização em estudos que não a apresentem, como estudos observacionais \citep{Stigler78}.

Uma interpretação mais recente, de surpreendente similaridade com a dada por Peirce, é proposta independentemente por Karl Popper, sob o nome
de {\em propensão}, o que irá fazer com que filósofos contemporâneos passem a se referir à interpretação de Peirce pelo mesmo
nome \citep{Miller75}.
A visão de probabilidades como propensões, advogada por Peirce ou Popper é, 
no entanto, criticada por não prover nenhuma base operacional para o cálculo de probabilidades à partir
da definição de propensidades. É inconclusivo se esta visão pode ser considerada uma teoria de interpretação
de probabilidades, pois uma interpretação deve possuir uma determinada estrutura, como cita Peter Milne:

\begin{quote}
``Interpretations in the literal sense (...) have sufficient intrinsic mathematical structure that one can derive
the characteristics of probability from them.''\citep{Milne93}
\end{quote}

Por outro lado, autores mais recentes como Donald Gillies e J.H. Fetzer têm advogado o uso do termo ``propensão'' para
qualquer teoria de probabilidades objetiva e não baseada em frequências observadas.
Desta forma, embora a visão de propensões tenha recebido pouca atenção em meios científicos
ela tem tido novo interesse nos últimos anos em círculos filosóficos \citep{Gillies2000}. 
O interesse continuado em interpretações de probabilidade que permitam a probabilidade do caso único
está ligado ao estudo da física quântica, o que é natural por dois motivos.
O primeiro é que há evidências de que a natureza da física quântica é intrinsecamente probabilística \citep{Gudder88}, 
ao contrário de fenômenos
macroscópicos para os quais a descrição probabilística pode ser vista como uma aproximação dada a falta de informações completas.
O segundo é que o estudo de problemas quânticos leva naturalmente a estruturas matemáticas semelhantes às probabilidades, mas
com propriedades distintas, como as quasiprobabilidades, úteis no estudo de ótica quântica, as quais podem não satisfazer o
axioma de aditividade de Kolmogorov e ter regiões de probabilidade
negativa \citep{Mandel95}.

\section{A formalização da inferência}

A escola frequentista será de grande influência para a formulação da teoria de testes de hipóteses de R. A. Fisher, que se
lança no começo da década de 1920 sobre o problema de formalizar a inferência estatística, ou seja, o processo de
extrair conclusões a partir de dados obtidos em uma amostra. Baseando-se nos trabalhos de Karl Pearson e William Sealy Gosset
(conhecido por seu pseudônimo Student), Fisher realiza um passo fundamental dessa formalização
diferenciando os conceitos de população e amostra:

\begin{quote}
``It is customary to apply the same name, {\em mean}, {\em standard deviation}, {\em correlation coefficient}, etc., both 
to the true value which we should like to know, but can only estimate, and to the particular value at which we happen 
to arrive by our methods of estimation.''\citep{Fisher1922}
\end{quote}

Nesta perspectiva, precisamos diferenciar uma {\em população}, hipoteticamente infinita, na qual os objetos se dividam em duas
classes de acordo com uma característica de interesse, de uma {\em amostra} finita, que será nossa base para tirar conclusões.
Por exemplo, examinando o lançamento de um dado, a população de interesse se constitui numa sequência infinita de rolagens
de dado, na qual distinguimos uma característica, como o fato de uma rolagem resultar no número 5. O valor da probabilidade de que
uma rolagem resulte em 5 {\em na população} pode ser visto como um parâmetro da distribuição teórica de resultados para essa
população infinita, portanto trata-se de um número fixo, embora a princípio desconhecido; 
já a proporção de lançamentos realizados que resulta em 5 {\em em uma dada amostra} é um estimador desse parâmetro,
a partir da amostra, e seu valor está sujeito a variações conforme repetimos a amostragem diversas vezes.
Qualquer função calculada sobre uma dada amostra é conhecida então como uma {\em estatística} da amostra.
A população idealizada corresponde a um {\em modelo} da realidade, descrito por uma função matemática.
Assim, Fisher explicita o fato de que a inferência estatística, em sua visão, depende da construção de um modelo matemático
abstrato.

Em seu trabalho de 1922, Fisher divide a tarefa da inferência estatística em 3 classes de 
problemas: problemas de especificação, ou seja, a determinação da função
matemática que define a população; problemas de estimação, ou seja, o cálculo de estatísticas sobre a amostra que estimem 
corretamente os parâmetros populacionais; e problemas de distribuição, ou seja, estimações realizadas sobre a distribuição
das estatísticas calculadas sobre a amostra. Fisher discorre brevemente sobre o problema de especificação,
se concentrando nos problemas de estimação e distribuição. Sobre especificação, ele escreve:

\begin{quote}
``We may know by experience what forms are likely to be suitable, and the adequacy of our choice may be
tested {\em a posteriori} [by an objective criterion of goodness of fit].
For empirical as the specification of the hypothetical population may be, this empiricism is cleared of its dangers if
we can apply a rigorous and objective test of the adequacy with which the proposed population represents the whole of
the available facts.''\citep{Fisher1922}
\end{quote}

A economia de palavras sobre especificação não deve sugerir que este é um problema de menor importância: pelo contrário,
uma especificação incorreta vai inutilizar toda análise posterior. Fisher, no entanto, se debruça neste trabalho sobre
uma teoria matemática, e parece reconhecer que as escolhas que levam à {\em escolha} de um modelo requerem deliberações
que extrapolam suas fronteiras.

Presumindo uma escolha adequada de modelo, ou seja,	de função matemática que descreva as propriedades da população,
a tarefa da inferência estatística se concentra em estudar o pequeno número de parâmetros desse modelo que a descrevem
completamente. Esse estudo corresponde aos problemas de estimação e distribuição.

Fisher descreve ainda as seguintes propriedades desejáveis para um estimador. Embora as suas definições tenham sido
melhor formalizadas posteriormente, as idéias presentes ainda são de grande valia:
\begin{enumerate}
	\item Consistência: ``when applied to the whole population the derived statistic should be equal to the parameter''
	\item Eficiência: ``that statistic is to be chosen which has the least probable error''
	\item Suficiência: ``the statistic chosen should summarise the whole of the relevant information supplied by the sample''
\end{enumerate}

Para este estudo, no entanto, o ponto mais importante do trabalho de Fisher é a descrição de uma quantidade conhecida como
verossimilhança (likelihood), que será fundamental para contrastar as teorias de inferência. Embora já identificada por C.S.
Peirce, a exposição de Fisher sobre verossimilhança é um passo fundamental para sua aceitação entre estatísticos europeus.
Fisher define essa quantidade,
cujo uso mais imediato é identificar as hipóteses sobre determinado parâmetro que encontram maior suporte das evidências,
como:

\begin{quote}
``The likelihood that any parameter (or set of parameters) should have
any assigned value (or set of values) is proportional to the probability
that if this were so, the totality of observations should be that observed.''\citep{Fisher1922}
\end{quote}

Para melhor compreender essa grandeza, vamos usar um exemplo citado por Laplace \citep{Laplace1814}, que se ocupa da 
proporção de batismos (usado para representar o número de nascimentos) de meninos e meninas em Paris nos anos 
de 1745 a 1784. Laplace tem a hipótese, corroborada por dados coletados em diversas outras cidades, de que nascem mais meninos
do que meninas em uma proporção de $22$ para $21$. Coletando a
informação de que houve $393386$ nascimentos de meninos e $377555$
nascimentos de meninas no período, e presumindo o modelo binomial para os nascimentos, a verossimilhança da hipótese levantada
por Laplace é dada pela probabilidade atribuída ao valor $393386$ pelo modelo binomial com tamanho $393386 + 377555 = 770941$
e probabilidade de sucesso $22/(21+22) \sim 0.511$. Essa probabilidade é um número da ordem de $10^{-5}$. Ao calcular a 
verossimilhança da hipótese de que não há preponderância de nenhum gênero, ou seja, a probabilidade de sucesso é de $0.5$, 
encontramos o valor de $10^{-74}$, muitas ordens de grandeza inferior ao anterior. Portanto, os dados indicados por Laplace
fornecem uma evidência mais forte para a hipótese de que os nascimentos de meninos são mais prováveis na proporção de $22$
para $21$. % PI: em relação à outra hipótese. C: essa frase depende da interpretação do que é evidência, vide seção sobre Carnap

Laplace não formaliza esse argumento em seu livro, sem dúvida pela dificuldade de realizar as contas indicadas acima sem
a ajuda de computadores. Mas é importante notar que, enquanto Laplace poderia desenvolver esse raciocínio em termos das 
{\em probabilidades} das hipóteses conflitantes, Fisher nota que as verossimilhanças calculadas acima não se comportam como 
probabilidades, no sentido de não estarem sujeitas às leis da
probabilidade (veja na seção \ref{sec:axiomatica}).
%PI: Um aspecto importantíssimo aqui é que Fisher fez isso
%intencionalmente: ele procurava fazer frente aos Bayesianos e sua
%concepção de que nosso grau de confiança em uma hipótese pode ser
%expresso por uma probabilidade. Para isso, ele precisava de uma
%função que experessasse o suporte empírico de uma hipótese frente a
%dados que não fosse uma probabilidade. A solução é engenhosa: nossa
%confiança em uma hipótese é proporcional à probabilidade que ela
%atribui a dados observados, mas não é uma probabilidade (já que
%proporcionalidade não garante os axiomas).

\section{Abordagens para testes de hipóteses}
% Embora haja forte relação com a seção anterior, ela não está muito
% evidente. Em algum ponto entre as duas é preciso ligar o conceito de
% verossimilhança, o método de máxima verossimilhança com testes de
% significância e p-values.

A estrutura mental desenvolvida por Fisher durante a década de 1920 vai levar também a um procedimento conhecido como teste de 
significância, e que será alvo de uma intensa controvérsia entre ele e a dupla formada por Jerzy Neyman e Egon Pearson, 
durante a década de 1930. Em um terminologia que vai influenciar muito do pensamento em inferência estatística do século XX,
Fisher usa as expressões {\em modelo estatístico} como uma função que descreve a população de interesse, sendo definida 
por um conjunto de parâmetros; e {\em hipótese} como uma afirmação a respeito do valor desses parâmetros. O modelo contém, 
assim, todas as suposições sobre o problema a respeito das quais os dados não irão discriminar, enquanto a hipótese
contém a afirmação que será julgada pela aplicação do teste. Dada uma hipótese, é possível
calcular a distribuição que uma certa estatística teria, se um mesmo experimento fosse repetido infinitas vezes.

O procedimento de teste de significância, segundo Fisher, requer a escolha de um modelo apropriado e a especificação de uma 
hipótese (conhecida como hipótese nula) para a qual é possível
calcular a distribuição da estatística de interesse.% falta definir
                                % estatística de interesse, que é o
                                % construto central do teste de significância.
Após realizar um experimento, a estatística calculada sobre a amostra é comparada com a distribuição teórica sob a hipótese nula,
e é calculada a probabilidade do desvio entre a estatística calculada e a sua esperança ser {\em tão grande ou maior} do que o
encontrado. 

Em um exemplo simples, queremos saber se uma moeda é justa ou não lançando-a 20 vezes. É improvável que o número de caras seja
exatamente 10, mesmo para uma moeda justa. Como o resultado de cada lançamento é uma de duas respostas (cara ou coroa), 
e a probabilidade de cada lançamento é suposta constante e independente, o modelo escolhido para representar o problema
pode ser uma binomial\footnote{Determinar que o modelo binomial é adequado corresponde ao problema da especificação
segundo Fisher}.
A hipótese nula é a de que a probabilidade da binomial é de 0.5. Nesse caso, podemos calcular a 
distribuição esperada do número de caras, e veremos que em 82\% dos experimentos realizados, a diferença entre o valor esperado
de dez caras e o resultado obtido será de 1 ou mais; em 26\%, de 3 ou mais; e em 4\%, de 5 ou mais. Ao examinar o resultado 
de um experimento, podemos então {\em rejeitar} a hipótese nula, ou seja, entendemos que a moeda não é justa, ou {\em deixar
de rejeitá-la}, ou seja, não encontramos evidência suficiente de que esta hipótese está errada.

Neyman e Pearson fazem uma crítica ao método descrito por Fisher, visto por eles como incompleto, e estabelecem uma metodologia
para escolha entre diversas hipóteses concorrentes, muitas vezes sumarizadas em duas: a nula e a alternativa.
Um procedimento de teste de significância
no qual uma única hipótese sobre o valor de um parâmetro é privilegiada em detrimento de todo o espectro de outras respostas
possíveis é falho. Como o valor de parâmetros é uma variável contínua, Neyman e Pearson argumentam (numa linha muito semelhante
à usada por argumentos Bayesianos):

\begin{quote}
``If {\em x} is a continuous variable, then any value of {\em x} is a singularity of relative probability equal to zero. We are
inclined to think that as far as a particular hypothesis is concerned, no test based upon the theory of probability can by
itself provide any valuable evidence of the truth or falsehood of that hypothesis.'' \citep{Neyman1933}
\end{quote}

%Enquanto isso é verdade a respeito de probabilidades, o mesmo não se verifica com verossimilhanças. Esse é um dos motivos
%pelos quais a verossimilhança é sempre instrumental em comparar
%hipóteses alternativas.% esta observação não está meio perdida aqui?

Neyman e Pearson propõe um procedimento formal para delimitar quando devemos aceitar e quando devemos rejeitar a hipótese
nula em favor de uma hipótese alternativa, dividindo o resultado desse processo em quatro classes: podemos aceitar uma hipótese
verdadeira ou falsa, ou podemos rejeitar uma hipótese verdadeira ou falsa. O ato de rejeitar a hipótese nula verdadeira
é conhecido como erro de tipo I, e o ato de aceitar uma hipótese nula falsa é conhecido como erro de tipo II. O conceito do 
erro de tipo I encontra paralelo na teoria de Fisher, enquanto o erro de tipo II é ausente, e será contestado por Fisher em
diversas bases. Esse procedimento consiste na construção prévia de uma região crítica, tal que, se os dados observados se
encontrarem nesta região, a hipótese nula é rejeitada em favor da alternativa; caso contrário, a hipótese nula é aceita.

A construção dessa região crítica é feita de forma a tolerar um valor para a probabilidade de erro de tipo I, normalmente em
0.05. Um resultado central do trabalho de Neyman e Pearson foi a demonstração de que, entre todas as possíveis regiões críticas,
a região ótima, no sentido de minimizar a probabilidade de erro de tipo II, é aquela construída pela razão entre as 
verossimilhanças \citep{Neyman1933}. 

Convém notar que esse resultado é verdadeiro apenas para hipóteses
ditas simples, ou seja, hipóteses que atribuem um único valor de probabilidade para cada possível observação.
% PI: como este é conceito central na dissertação, creio
               % que é preciso defini-lo precisamente, e de uma
               % maneira ampla o suficiente para comportar os usos ao
               % longo de toda dissertação. C: done
Enquanto Neyman e Pearson obtém resultados
parciais para testes envolvendo hipóteses compostas, outros autores vão eliminar essas hipóteses das suas considerações.
Edwards, por exemplo, afirma que:

\begin{quote}
``The class of hypotheses we call `statistical' is not necessarily closed with respect to the logical operations of
alternation (`or') and negation (`not'). For a hypothesis resulting from either of these operations is likely to be composite,
and composite hypotheses do not have well-defined statistical consequences, because the probabilities of occurrence of the
component simple hypotheses are undefined. For example, if {\em p} is the parameter of a binomial model, about which inferences
are to be made from some particular binomial results, `$p = \frac{1}{2}$' is a statistical hypothesis, because its 
consequences are well-defined in probability terms, but its negation, `$p \neq \frac{1}{2}$', is not a statistical hypothesis,
its consequences beign ill-defined.''\citep{Edwards72}
\end{quote}

Fisher critica duramente o procedimento de Neyman-Pearson por uma série de motivos, entre os quais o engessamento do procedimento,
que levaria a uma aceitação ou rejeição de hipóteses acriticamente:

\begin{quote}
``It is a fallacy (...) to conclude from a test of significance that the null hypothesis is thereby established (...).
In an acceptance procedure, on the other hand, acceptance is irreversible, whether the evidence for it was strong or weak.
It is the result of applying mechanically rules laid down in advance; no {\em thought} is given to the particular case,
and the tester's state of mind, or his capacity for {\em learning}, is inoperant.
By contrast, the conclusions drawn by a scientific worker from a test of significance are {\em provisional}, and involve
an intelligent attempt to {\em understand} the experimental situation.''\citep{Fisher1955}
\end{quote}

É curioso que um autor que tenha atacado tão intensamente os conceitos de probabilidade subjetiva seja também tão crítico de 
uma abordagem que visa minimizar as decisões subjetivas a serem tomadas por meio de uma estruturação rígida do procedimento
de teste de hipóteses. 

\section{Probabilidade necessária}

Tanto de Morgan como Boole, contados entre os fundadores da interpretação subjetivista e frequentista da probabilidade,
se ocuparam do estudo da probabilidade como um subconjunto da lógica formal, que constituía um interesse mais amplo para
ambos. No mesmo espírito, o trabalho de Rudolph Carnap, à partir da década de 1940, vai ligar novamente o estudo da probabilidade
às suas raízes dentro da lógica. A notação usada originalmente por Carnap favoreceu sua comunicação com filósofos e logicistas,
mas dificultou seu acesso por estatísticos, de forma que vamos adaptar alguns termos e notações seguindo textos mais
modernos (\citep{Zabell09} e \citep{Fitelson07}), que combinam a notação de Carnap com a de W.E. Johnson, que forneceu
resultados semelhantes de forma independente 20 anos antes de Carnap. 

A questão da definição da palavra ``probabilidade'' não deve ser a de decidir quais acepções dessa palavra são
corretas ou incorretas, mas sim quais são satisfatórias, e se existe alguma definição mais satisfatória que as demais.
A pergunta ``o que é a probabilidade?''
vai receber diferentes respostas dependendo de para quem for formulada, assim como a pergunta ``baleias são peixes?''
recebia uma resposta afirmativa em tempos bíblicos e recebe uma negativa de cientistas modernos. É equivocado pensar que os
biólogos ``descobriram que baleias não são peixes'': o que mudou foi o próprio conceito de peixe. 

Carnap, primeiramente, reconhece que a palavra ``probabilidade'' é usada em
dois grupos diferentes de significados, e 
portanto separa os conceitos de {\em probabilidade$_1$} e {\em probabilidade$_2$}, o primeiro
se referindo a uma medida de confirmação, enquanto o segundo é uma medida de frequência. Enquanto {\em probabilidade$_1$} é
um conceito útil para a formulação de um sistema de inferência, {\em probabilidade$_2$} pode ver seu uso em leis físicas, por
exemplo. Assim, quando a lei da entropia é formulada em termos probabilísticos, é claro que a visão pessoal de cada sujeito
é irrelevante para o resultado de um experimento físico, como a mistura entre dois gases. 

Seguindo o trabalho de Wittgenstein, Carnap trabalha 
com a {\em probabilidade condicional} de uma proposição lógica $h$ dada uma proposição $e$, $c(h, e)$, 
e interpreta essa grandeza como a medida com a qual a evidência $e$ corrobora uma hipótese $h$ \citep{Zabell09}.
Essa visão será desenvolvida de forma a construir uma visão de probabilidade como uma necessidade lógica:

\begin{quote}
``The truth conditions for a probability statement are logical or semantic: they are built into the language in the same
way that the truth conditions for logical entailment are built into the language. Given the statement `It will rain tomorrow',
and given a body of evidence $e$, there is exactly one real number $r$, determined by logical conditions alone, such
that the probability of `It will rain tomorrow', relative to the body of evidence $e$, is $r$.'' \citep{Kyburg74}
\end{quote}

Carnap provê ainda uma distinção entre três formas de confirmação: classificatória (a evidência confirma ou não a hipótese),
quantitativa (com qual intensidade a evidência suporta a hipótese) e relacional (a evidência confirma uma hipótese mais do que
outra)\citep{Carnap62}. 

Johnson e Carnap desenvolvem um argumento segundo o qual observações passadas de um processo multinomial embasam a probabilidade
de observações futuras, baseados nos postulados da permutação e da combinação; 
provendo assim uma base para regras de sucessão como a 
desenvolvida por Laplace sem a necessidade de invocar uma ignorância {\em a priori} sobre o valor de um parâmetro. 
No entanto, a aplicação desses princípios para problemas gerais em inferência necessita de uma extensão desses resultados
para considerar não apenas observações futuras idênticas, mas observações {\em similares}. A definição dessa similaridade
e seu uso operacional para problemas de inferência levam a problemas matemáticos extremamente complexos, e uma teoria
geral de inferência baseada em probabilidades necessárias ainda não foi alcançada (veja \cite{Zabell09} para um histórico
mais completo).

\section{Probabilidade Bayesiana moderna}\label{sec:bayes}

Embora uma grande parcela da comunidade científica tenha adotado uma visão frequentista durante a primeira metade do século XX,
tomando algum dos lados do debate entre Fisher e Neyman-Pearson, muitos autores continuaram seguindo a interpretação 
subjetivista da probabilidade, mais ou menos semelhante à advogada por de Morgan. Entre eles, Frank P. Ramsey, Dennis Lindley,
Harold Jeffreys, Bruno de Finetti e Leonard J. Savage
podem ser apontados como os principais responsáveis por um interesse continuado na interpretação subjetiva, que, próximo à
década de 1960, passou a ser o principal embasamento para uma classe de métodos de inferência conhecidos como Bayesianos. 
Para Ramsey e Savage, a probabilidade surge como um sistema de preferência racional, enquanto para de Finetti, ela representa
uma chance justa em uma aposta; embora as visões sejam razoavelmente intercambiáveis, e difiram pouco 
no grau de empiricismo advogado,
ambas levam logicamente aos axiomas padrão da probabilidade\footnote{Para uma visão mais aprofundada das diferenças
filosóficas entre de Finetti e os demais subjetivistas, veja \citep{Galavotti89}.}

A inferência Bayesiana é devida ao rev. Thomas Bayes, que demonstrou o teorema que leva seu nome, e ao já mencionado marquês de
Laplace, que formulou a metodologia das probabilidades inversas. A contribuição relativa de Bayes e Laplace aos sucessos
e fracassos da teoria Bayesiana é um tópico controverso	entre os que estudam a história da probabilidade \citep{Zabell09}.
Fisher, por exemplo, considera o argumento com o qual Bayes estabelece uma {\em priori} uniforme como um comentário
a respeito do problema específico que Bayes estava estudando, enquanto a escola Bayesiana verá esse argumento
como aplicável a outras classes de problemas \citep{Aldrich08}. Assim, o resultado conhecido como Teorema de Bayes é aceito
por todas as escolas de interpretação, enquanto que o argumento de Laplace 
é, em essência, aceito pela escola subjetivista e completamente
rejeitado pela frequentista. % esta distinção não ficou clara. Passa
                             % por aí deixar mais clara se s distinção
                             % entre o teorema, que é uma dedução
                             % matemática da relação entre
                             % probabilidades condicionais, e o seu
                             % uso em diferentes contextos (um deles é
                             % o de inferência sob a concepção
                             % subjetivista de probabilidade, que é a
                             % única maneira de inputar probabilidades
                             % a hipóteses).

Talvez a formalização mais convincente da teoria subjetivista de probabilidades seja a dada por de Finetti no seu argumento sobre
{\em Dutch books}. Um {\em Dutch book}, ou ``aposta holandesa'' 
é um conjunto de apostas que garante um ganho para um apostador, qualquer que seja o
resultado observado após o jogo. Suponha que um evento vai ser observado (seja uma corrida de cavalos, uma luta, etc), e um
agente pode apostar uma quantia em dinheiro para cada possível resultado desse evento, baseado na sua avaliação subjetiva da
probabilidade de cada resultado, e recebe um prêmio correspondente ao resultado que de fato ocorrer. A relação entre as 
probabilidades e os prêmios é dita {\em incoerente} se uma ``aposta holandesa'' puder ser feita a favor ou contra o agente
(ou seja, garantindo lucro ou perda sempre), e {\em coerente} caso contrário. De Finetti mostra que toda avaliação 
coerente de probabilidades subjetivas implica nos axiomas da probabilidade \citep{deFinetti37}.

Já a teoria de inferência Bayesiana pode ser vista, resumidamente, como uma estrutura de atualização das probabilidades subjetivas
frente a novas evidências. O teorema de Bayes conecta a probabilidade {\em a priori} com a probabilidade {\em a posteriori}
através do uso da verossimilhança dos dados. No caso de duas hipóteses concorrentes $H1$ e $H2$, para explicar um evento observado
$E$, o teorema de Bayes diz que:

\begin{equation}
	\frac{p(H1|E)}{p(H2|E)}= \frac{p(E|H1)}{p(E|H2)} \,\frac{p(H1)}{p(H2)}
\end{equation}

Onde o termo $\frac{p(E|H1)}{p(E|H2)}$ é justamente a razão das verossimilhanças.
As grandes questões que precisam ser resolvidas são: como construir uma {\em priori}
a partir do conhecimento prévio (que será retomada na seção \ref{sec:classref}), e principalmente, como construir {\em prioris}
na ausência de informação sobre o problema. Fisher aponta que {\em presumir a indiferença} em relação a um parâmetro
$p$ depende da forma matemática do modelo empregado: tomando $\sin \theta = 2p-1$, uma {\em priori} ``plana'' em relação a $p$ 
privilegiará alguns valores de $\theta$ e vice-versa \citep{Fisher1922}.

A proposição da escola Bayesiana foi acompanhada de um debate intenso, nos quais os argumentos eram não raramente personalizados.
Ronald Fisher foi um árduo crítico
de Laplace, escrevendo que ``We can know nothing of the probability of hypotheses or hypothetical quantities'' \citep{Fisher1921},
e ``The theory of inverse probability is founded upon an error, and must be wholly rejected'' \citep{Fisher1925}, ao mesmo tempo
em que propunha uma metodologia de estimação fiducial, 
% Uma das tentativas de Fisher contra inferência foi de fato a
% fiducial, mas a outra foi a função de verossimilhança, como Edwards
% descreve em seu prefácio. Esta segunda pode ser considerada uma
% alternativa válida à ideia de probabilidade como suporte ao uma hipótese.
que permitia a inferência absoluta sobre hipóteses sem {\em prioris} 
especificadas para uma classe de ``problemas bem-formulados''. O método fiducial foi fundamentalmente abandonado nos anos 
seguintes à sua morte, sendo chamado por Savage de ``a bold attempt to make the Bayesian omelet without
breaking the Bayesian eggs'' \citep{Savage60}, e é considerado como refutado por 
I. J. Good \citep{Good92}. No entanto, os argumentos de Fisher
alcançaram várias vezes uma grande lucidez e perspicácia, e a formulação moderna da teoria Bayesiana 
deve muito à crítica constante feita por ele sobre os pontos mais sutis da sua lógica. \footnote{
O argumento fiducial continua sendo altamente controverso entre estatísticos, sendo defendido por exemplo
em \citep{Hacking65}.}

Com a axiomatização e formulação moderna da interpretação subjetivista, cuja história é intimamente ligada à da teoria
econômica de utilidade (veja por exemplo \citep{Friedman48} e \citep{Pfanzagl67}),
a escola Bayesiana de pensamento ganhou maior aceitação nos meios científicos. Savage escreve sobre essa formulação moderna:

\begin{quote}
``Personal probability can be regarded as part of a certain theory of coherent preference in the face of uncertainty. This
preference theory is normative; its goal is to help us make better decisions by exposing to us possible incoherences in our
attitudes toward real and hypothetical alternatives.'' \citep{Savage67}
\end{quote}

Essa exposição é reminiscente das palavras de de Morgan:

\begin{quote}
``I throw away objective probability altogether, and consider the word as meaning the state of the mind with respect to an 
assertion (...). `It is more probable than improbable' means in this chapter 'I believe that it will happen more than I 
believe that it will not happen. Or rather `I {\em ought} to believe, \&c.', for it may happen that the state of mind which {\em
is}, is not the state of mind which should be. 
D'Alembert believed that it was {\em two} to {\em one} that the first head which the throw of a halfpenny was to give would occur
before the third throw; a juster view of the mode of applying the theory would have taught him it was {\em three} to {\em one}.
But he {\em believed} it, and thought he could show reason for his belief: to him the probability {\em was} two to one. But 
I shall say, for all that, that the probability {\em is} three to one; meaning that in the universal opinion of those who
examine the subject, the state of mind to which a person {\em ought} to be able to bring himself is to look three times
as confidently upon the arrival as upon the non-arrival.''\citep{deMorgan1847}
\end{quote}

Savage, ainda, propõe a teoria subjetiva de probabilidade como uma resposta ao problema da indução, ou seja,
como podemos justificar que nossa experiência passada possa ser usada para prever eventos futuros:

\begin{quote}
``The theory of personal probability [prescribes] exactly how a set of beliefs should change in light of what is observed.
It can help you say, `My opinions today are the rational consequence of what they were yesterday and of what I have seen
since yesterday.' In principle, yesterday's opinions can be traced to the day before, but even given a coherent demigod
able to trace his present opinions back to those with which he was born and to what he has experienced since, the theory
of personal probability does not pretend to say with what system of opinions he ought to have been born. It leaves him, just
as Hume would say, without rational foundation for his beliefs of today (...). That all my beliefs are but my personal
opinions, no matter how well some of them may coincide with opinions of others, seems to me not a paradox but a truism (...).
If there is a rational basis for beliefs going beyond mere coherency, then there are some specific opinions that a rational
baby demigod must have. Put that way, the notion of any such basis seems to me quite counter intuitive.'' \citep{Savage67}
\end{quote}

\section{Críticas e secções do pensamento Bayesiano}

Uma crítica da escola frequentista pode ser resumida como: se existirem chances físicas associadas a um evento
e probabilidades subjetivas, não haveria motivo para supor que ambas serão iguais. Há diferentes respostas para esse problema 
entre os proponentes da teoria Bayesiana, que essencialmente caracterizam as diversas vertentes do Bayesianismo
moderno.

Por um lado, dadas algumas condições sobre a construção da
probabilidade subjetiva de um ator racional, pode ser demonstrado que as chances físicas de um evento são iguais 
às probabilidades subjetivas às quais ele deve chegar; embora
alguns autores discordem sobre quais são as condições razoáveis para tomar como axioma. Anscombe e Aumann, por exemplo,
escrevem sobre um problema compondo processos do tipo roleta (nas quais as chances físicas de cada resultado são conhecidas) 
com processos do tipo corridas de cavalo (nas quais a chance física é desconhecida) que:

\begin{quote}
``In this case the subjective probability of any outcome is equal to the [physical] chance associated with that outcome.
Since the two are equal, it does not matter much which word or symbol we use. The
chance refers to the phenomenon, the probability refers to your attitude
towards the phenomenon, and they are in perfect agreement''\citep{Anscombe63}
\end{quote}

Contraste-se com essa visão a posição advogada por de Finneti, para quem a probabilidade não precisa ser racionalmente
justificável:

\begin{quote}
``The subjective theory (...) does not contend that the opinions about probability are uniquely determined and
justifiable. Probability does not correspond to a self-proclaimed `rational' belief but to the effective personal
belief of anyone''\citep{deFinetti51}
\end{quote}

Neste sentido, a posição de Anscombe, que privilegia um certo valor racional para a probabilidade de um evento,
se afastam daquela defendida por de Finetti, e se aproxima da lógica probabilística de Rudolph Carnap e do Bayesianismo
objetivo advogado por E. T. Jaynes \citep{Jaynes68}, que se utiliza do princípio da máxima entropia para determinar
{\em prioris} plenamente objetivas.

Outra crítica diz respeito ao fato de que, enquanto subjetiva, a probabilidade percebida por um sujeito pode não corresponder
à probabilidade anunciada por ele. Esse problema é abordado pela teoria de {\em Scoring Rules}, desenvolvida por
de Finetti e Savage durante a década de 1970 \citep{Lindley82}, cujo
desenvolvimento mostra métodos para garantir coerência entre ambas. Dennis Lindley expande esses resultados para mostrar que:

\begin{quote}
``Let a person express his uncertainty about an event E, conditional upon an event F, by a number x and let him be given, 
as a result, a score which depends on x and the truth or falsity of E when F is true. It is shown that if the scores are 
additive for different events and if the person chooses admissible values only, then there exists a known transform of the 
values x to values which are probabilities. In particular, it follows that values x derived by significance tests, confidence 
intervals or by the rules of fuzzy logic are inadmissible. Only probability is a sensible description of uncertainty.''
\citep{Lindley82}
\end{quote}

A escola Bayesiana também é criticada ao assumir uma postura subjetivista, que não seria compatível com o desenvolvimento
da ciência, definida como a construção de conhecimento racional objetivo. Essa oposição, a bem da verdade, é mais reveladora
de uma visão sobre a natureza da ciência do que uma crítica propriamente ao paradigma Bayesiano. Frise-se que mesmo 
avaliações de probabilidade pessoais e subjetivas podem estar abertas ao estudo objetivo, uma posição que remonta ao trabalho
de C.S. Peirce \citep{Stigler78}.  Outro fator importante para responder essa crítica é a maior percepção moderna
de que a escola frequentista, ao ser obrigada a escolher um modelo mental de referência, é menos objetiva do que se propõe.
Savage escreve que ``the Bayesian approach is more objectivistic than the frequentist approach in that it imposes a greater
order on the subjective elements of the deciding person.'' \citep{Savage60}.

\section {O problema do ``Optional stopping''} 
% Na primeira leitura me perguntei: Se esta seção e a seguinte são 
% críticas ao frequentismo porque
% colocá-la aqui?
% Ao final acho que entendi sua linha de racioncínio, mas começar
% estas seções com uma frase que já indique ao leitor porque voltar a
% tratar de problemas do frequentismo pode ajudar a mantê-lo com você.

Uma crítica muito veemente aos proponentes da inferência frequentista é dada pelo problema da parada opcional, ou 
``optional stopping''. Suponha que um cientista realizou uma série de 12 ensaios de Bernoulli, ou seja, experimentos aleatórios
com dois possíveis resultados (sucesso e falha), independentes e de mesma probabilidade. Após obter 3 sucessos, o cientista
decide testar a hipótese nula de que a probabilidade de sucesso é de 0.5. Pelo paradigma frequentista, esse teste depende
da maneira como o experimento foi projetado. Se o cientista planejava fazer 12 experimentos, a probabilidade de observar 3
ou menos sucessos sob a hipótese nula é de $ \left( {0 \choose 12} + ... + {3 \choose 12}\right) (\frac{1}{2})^{12} = 7.3\%$. 
No entanto, se o cientista decidiu continuar realizando experimentos até
encontrar o terceiro sucesso, a probabilidade de chegar a doze tentativas é de 
$1 - \left( \frac{1}{2}^3 + {3 \choose 2} \frac{1}{2}^4 + ... + {10 \choose 2} \frac{1}{2}^{11}\right) = 3.3\%$. Ou seja,
dependendo da intenção original do cientista, os mesmos dados podem servir para refutar ou não a hipótese nula.

Para evitar esse tipo de  paradoxo, a sabedoria convencional dos estatísticos frequentistas afirma que um cientista {\em não pode}
examinar os próprios dados enquanto os coleta. Toda a análise estatística deve ser feita posteriormente à coleta. 
Desta forma, o cientista que realizou os 12 testes e encontrou um p-valor de 7.3\%, próximo do nível crítico de 5\%, é
proibido de continuar a coleta de dados. Richard Royall, citando essa prática, diz:

\begin{quote}
``Subsequent observations, no matter how consistent and convincing, can never justify a claim of statistical significance
at his target level (...). Finding that the early partial results represent evidence that is only fairly strong precludes
the possibility that the evidence in the final results might be quite strong. Does this make sense?'' \citep{Royall97}
\end{quote}

Enquanto alguns estatísticos vêem neste problema ``um uso ingênuo de p-valores'' \citep{Good92}, outros, como Richard Royall,
consideram esta situação uma indicação clara de que o paradigma frequentista leva a contradições inerentes, e portanto 
deve ser abandonado. A discordância entre as conclusões frequentistas e a inferência julgada aceitável por seus opositores 
pode ser traçada na incoerência entre
o procedimento de teste de hipóteses e o princípio da verossimilhança. Utilizado já por Neyman e Pearson\citep{Neyman1933},
em caráter razoavelmente intuitivo, o princípio é demonstrado por Allan Birnbaum em 1962, 
e afirma que toda a informação contida em uma amostra está	contida na função de verossimilhança \citep{Birnbaum62}. 
Desta forma, a inferência feita à partir da amostra deve ser dependente da função de verossimilhança, e não do desenho
experimental.
No exemplo discutido acima, a função de verossimilhança do experimento é a mesma, não importando a intenção do pesquisador:

\begin{equation}
	\mathcal{L}(p|x) \,\, \propto \,\, p^3(1-p)^9
\end{equation}

A razão de verossimilhanças, critério já utilizado por Fisher e Neyman-Pearson, entre os valores de $p=0.5$ e $p=3/12$ é de 
aproximadamente 5, oferecendo uma evidência moderada a favor do valor $p=3/12$. Dentro do paradigma Bayesiano, onde o princípio
da verossimilhança é aceito, essa razão de verossimilhanças deve ser usada para atualizar as probabilidades que cada hipótese
tem {\em a priori}. Como vamos expor na seção \ref{sec:likelihood}, proponentes da verossimilhança como base da inferência
vão questionar o uso explícito de {\em prioris}, se atendo apenas ao valor da razão de verossimilhanças.

\section{O problema da classe de referência}\label{sec:classref}

Em seu trabalho de 1922, Fisher escreve:

\begin{quote}
``The framing by means of a model is located at the beginning of the
statistical treatment of a problem of application. The postulate of randomness thus resolves itself into the 
question, `Of what population is this a random sample?' which must frequently be asked by every practical
statistician''\citep{Fisher1922}
\end{quote}

Fisher volta a essa questão em 1955, ao contrastar seu método de teste de hipóteses com o procedimento de Neyman-Pearson:

\begin{quote}
``The root of the difficulty of carrying over the idea from the field of acceptance procedures to that of tests of significance
is that, where acceptance procedures are appropriate, the source of supply has an objective reality, and the population
of lots, or one or more, which could be successively chosen for examination is uniquely defined; whereas if we possess a unique
sample in Student's sense on which significance tests are to be performed, there is always, as Venn (1876) in particular has
shown, a multiplicity of populations to each of which we can legitimately regard our sample as belonging; so that the phrase
`repeated sampling from the same population' does not enable us to determine which population is to be used to define the
probability level, for no one of them has objective reality, all being products of the statistician's imagination.''
\citep{Fisher1955}
\end{quote}

A mesma questão se manifesta no exemplo mais frequente de aplicação do teorema de Bayes: dado o resultado positivo em um
teste clínico, qual a probabilidade do paciente ter a doença? A resolução desse problema passa por identificar que:

\begin{equation}
P(D|+) = \frac{P(+|D)P(D)}{P(+|D)P(D)+P(+|\bar D) P(\bar D)}
\end{equation}

Onde $P(+)$ é a probabilidade do teste dar positivo 
e $P(D)$ é a probabilidade de um
indivíduo ter a doença. Este termo vai ser interpretado como a {\em priori} por um Bayesiano, ou mais geralmente
como a prevalência da doença na população. A aparente simplicidade desse exemplo esconde a {\em escolha} de uma população:
devemos considerar a prevalência na cidade onde o indivíduo mora, ou no país? Indivíduos de ambos os gêneros devem ser
considerados? A prevalência deve levar em consideração a faixa etária, a renda, o hábito esportivo, alimentar ou uso de
substâncias pelo indivíduo? Como cita Henry Kyburg:

\begin{quote}
``Probability theorists and statisticians have tended to take one of two tacks in relation to this problem:
either they deny the significance of probability assertions about individuals, claiming that they are meaningless;
or they open the door the whole way, and say that an individual may be regarded as a member of any number of classes, 
that each of these classes may properly give rise to a probability statement, and that, so far as the theory is concerned,
each of these probabilities is as good as another. (...)
\end{quote}

Some philosophers have argued that this is a merely pragmatic problem, rather than a theoretical one (...). But calling
a problem practical or pragmatic is no way to solve it.'' \citep{Kyburg74}

Essa questão, então, perpassa a formulação do problema de inferência para ambas as principais escolas. De certa forma,
podemos ver que os problemas de ambas as escolas se concentram não na interpretação do dado observado, mas sim
no {\em background} contra o qual ele vai ser comparado. Isso inspira a questão: será que é possível desenvolver
um programa de inferência focado na observação, sem precisar recorrer a classes externas de referência?

\section{A escolha da verossimilhança}\label{sec:likelihood}

A grandeza conhecida como verossimilhança, defendida por Fisher em 1922, viu grande uso nas últimas páginas. Por um lado,
ela é a base sobre a qual se assenta o teste de hipóteses de Neyman-Pearson. Por outro, ela é a ponte entre a {\em priori}
e a {\em posteriori} Bayesianas. No final dos anos 1960, o uso da verossimilhança como base da inferência ganha vida 
independente, e em grande parte devido ao livro de I. Hacking em 1965 sobre a lógica da inferência, 
e ao trabalho de A.W.F. Edwards em 1972, é possível considerar essa postura como
uma escola independente de pensamento, denominada às vezes Verossimilhantismo. Mais recentemente, essa escolha vai ser
veementemente apoiada por Richard Royall. Esta escola de pensamento se baseia na conjunção do princípio da 
verossimilhança, conforme demonstrado por Birnbaum, e na lei da verossimilhança, enunciada por Hacking:
% Não ficou clara a distinção entre a lei e o princípio, nem que a
% escola se distingue por aceitar ambos, enquanto todas as escolas
% aceitam a lei.

\begin{quote}
``If hypothesis $A$ implies that the probability that a random variable $X$ takes the value $x$ is $p_A(x)$, while
hypothesis $B$ implies that the probability is $p_B(x)$, then the observation $X=x$ is evidence supporting $A$ over $B$
if and only if $p_A(x) > p_B(x)$, and the likelihood ratio, $p_A(x)/p_B(x)$, measures the strength of that evidence.''
\citep{Hacking65}
\end{quote}

A escolha da verossimilhança pode ser vista como herdeira intelectual da abordagem de Neyman-Pearson, cujo ponto de partida
para atacar o teste de significância de Fisher foi reconhecer a
necessidade lógica da comparação entre diferentes hipóteses.
% Não é curioso que o ataque tenha usado uma função criada pelo
% próprio Fisher?
Enquanto o teste de significância privilegia uma hipótese nula, a abordagem de Neyman-Pearson, tal qual a de Edwards e Royall,
vai refutar essa abordagem como incompleta. Os dados recolhidos por um experimento podem indicar uma alta ou baixa
probabilidade para uma certa hipótese, % conceito bayesiano. Para
                                % escola frequentista e de
                                % verossimilhança hipóteses tem
                                % suporte, não probabilidades. Esta é
                                % a essência do argumento de Fisher
                                % para definir a verossimilhança,
                                % depois refinado por
                                % outros. Aproveitando: já
                                % mencionei a história em alguns
                                % outros comentários. A citacao
                                % classica é um trecho do início de
                                % Statistical Methods. Veja http://www.economics.soton.ac.uk/staff/aldrich/fisherguide/prob+lik.htm
mas isso não deve ser considerado evidência a favor ou contra essa hipótese sem
que hipóteses alternativas sejam igualmente escrutinizadas. É sobre o uso da razão entre verossimilhanças de diferentes hipóteses,
e não de qualquer estatística calculada sobre uma única hipótese, que Neyman, Pearson, Edwards e Royall vão assentar seu
programa de inferência. O detetive Sherlock Holmes, personagem de Conan Doyle, famoso por seu raciocínio arguto, é famoso
pela frase ``How often have I said to you that when you have eliminated the impossible, whatever remains, {\em however 
improbable}, must be the truth?''.

A divergência entre as abordagens se dá fundamentalmente com a {\em ação} que o cientista deve tomar após examinar a 
razão de verossimilhanças. Neyman-Pearson argumentam que esse valor deve nortear um processo de teste de hipóteses, que
leva ao {\em comportamento} de aceitar uma hipótese em detrimento das demais. Royall e Edwards realizam a separação
conceitual entre (1) o grau de certeza que temos sobre a hipótese, (2) a força da evidência que um conjunto de dados confere
a uma hipótese em detrimento de outras, e (3) o curso de ação tomado após verificar tais quantidades. O curso de ação tomado
deve levar em conta uma multiplicidade de outros fatores, representados por {\em priors} e funções de perda na teoria de 
decisões: Laplace já havia separado esses conceitos com o problema do 
número de juízes necessário para condenar um prisioneiro. A decisão, além de levar em conta as probabilidades de
condenar um inocente ou perdoar um culpado, deve levar em conta se a
pena será uma multa ou a morte.
% Uma diferença crucial aqui é que Royall atribui à inferência
% estatística apenas a medida da força de evidência (2) , e contrasta isso
% do papel de tomada de decisão (3) que Neymann-Pearson atribuem à
% inferência, e o papel de definir grau de certeza (1) que os Bayesianos
% incluem dentro da inferência. O que está em debate aqui não é mais a
% uma questão interna da estatística, como a concepção de
% probabilidade. Royal explicita melhor do que ninguém que é
% necessário discutir o papel da estatística na pesquisa
% científica. Um argumento fácil de vender é que a inferência deve
% replicar a lógica da pesquisa, veja meu comentário sobre isso
% abaixo. Aqui o que que é mais importante é além de distinguir (1),
% (2) e (3) indicar que, na concepção de Royall, as escolas são
% definidas por estas diferentes formalizações de inferência.


Um exemplo interessante 
sobre verossimilhanças é dado 
% Sugiro para ficar mais explícita a ligação com oc conceitos do
% parágrafo anterior (pulando o de AIC):
% Um exemplo que ilustra as diferentes concepções de
% inferência e suas consequências é dado ...
pelo seguinte experimento mental: 
tome um baralho ordinário e vire a primeira carta,
para encontrar um ás de espadas. Suponha agora duas hipóteses concorrentes: $H_N$ de que o baralho é normal e $H_A$
de que o baralho é um engodo, composto por 52 ases de espadas. A verossimilhança de $H_N$ é 
$\mathcal{L} (H_N|A \spadesuit) = \frac{1}{52}$, contra $\mathcal{L} (H_A | A \spadesuit) = 1 $. Enquanto o procedimento
de Neyman-Pearson diz ``escolha a hipótese que leva à maior verossimilhança'', Royall e Edwards vêem nessa situação
apenas uma {\em evidência} favorável a $H_A$ sobre $H_N$. Para transformar essa evidência em um curso de ação, é necessário
incorporar mais informação ao problema. 
Ainda, para transformar essa evidência em um grau de certeza,
é necessário incorporar o nosso conhecimento {\em a priori} sobre a situação: enquanto nossa intuição diz
que um baralho de engodo deve ser muito raro, e portanto o baralho examinado {\em provavelmente} é normal, um marciano,
que não tem a mesma {\em priori}, deve achar muito natural que todas as cartas tenham a mesma figura, afinal todas tem o mesmo
verso. 

Em particular, 
ao dissociar a {\em força de evidência} da {\em tomada de decisão}, Royall e Edwards estão solicitando ao cientista
que divulgue seus dados na forma de razões de verossimilhança, não transformadas em valores-p ou {\em posterioris}, para que
seus pares possam ter clareza de qual é a evidência apresentada. 
Ao recuperar o papel do cientista como tomador de decisões subjetivas, a abordagem da verossimilhança volta a se encontrar
com o pensamento de Fisher, que critica Neyman-Pearson por um programa que é adequado ao processamento industrial de lotes,
mas não à produção de conhecimento científico. Neste aspecto, Royall e Edwards ecoam as palavras de Fisher e de I. J. Good:

\begin{quote}
``We have the duty of formulating, of summarizing, and of communicating our conclusions, in intelligible form, in
recognition of the right of other free minds to utilize them in making their own decisions.'' \citep{Fisher1955}
\end{quote}

\begin{quote}
``If a Bayesian is a subjectivist, he will know that the initial probability density varies from person to person and
so he will see the value if graphing the likelihood function for communication.'' \citep{Good76}
\end{quote}

Porém, se a tomada de decisão deve ser feita com base na razão de verossimilhanças e na especificação de uma 
probabilidade {\em a priori}, no que o paradigma da verossimihança difere das escolas Bayesianas? Ou ainda: será que a 
inferência baseada em verossimilhanças é uma ``inferência Bayesiana sem {\em prioris}''? Para responder isso,
é importante retomar a formulação da lei da verossimilhança:

\begin{quote}
``If hypothesis $A$ implies that the probability that a random variable $X$ takes the value $x$ is $p_A(x)$, while
hypothesis $B$ implies that the probability is $p_B(x)$, then the observation $X=x$ is \textbf{evidence supporting $A$ over $B$}
if and only if $p_A(x) > p_B(x)$, and the likelihood ratio, $p_A(x)/p_B(x)$, measures the strenght of that evidence.''
\citep{Hacking65}
% Porque repetir a lei e não enunciar o princípio em nenhum ponto do texto?
\end{quote}

Embora seja possível considerar o paradigma Bayesiano como um paradigma verossimilhantista {\em sensu lato}, 
ele utiliza a razão das verossimilhanças como meio para construir quantias absolutas, a probabilidade 
{\em a posteriori} de $A$ e a probabilidade {\em a posteriori} de $B$. Um verossimilhantista {\em sensu stricto}, ao contrário,
verá na razão das verossimilhanças
o objeto final do seu estudo. O paradigma da verossimilhança, dessa forma, se opõe à transformação de uma quantidade de 
suporte relacional entre $A$ e $B$ em quantidades não-relacionais de
suporte para cada hipótese \citep{Fitelson07}. %perfeito!
Embora Royall e Edwards forneçam razões fortes para aceitar essa lei como premissa, ela não é uma necessidade lógica.
Note-se aqui que as definições de evidência ou suporte estatístico não possuem, nos textos de Royall e Edwards,
definições externas que lhes garantam
propriedades distintivas. % Seguro? Me soa estranho pois ambos começam com a
                          % definição de suporte. Talvez a razão
                          % esteja lá na maneira como Fisher tentou
                          % escapar separar probabilidade de
                          % credibilidade. Suspeito de um esqueleto no
                          % armário, gostaria de ler e discutir isso
                          % com vc.
A aceitação da lei da verossimilhança equivale logicamente à definição de evidência pela razão
da verossimilhança de hipóteses simples. % Entendo que o ponto prepare
                                % para a questão das hipóteses simples
                                % x compostas, mas aqui ele parece
                                % meio solto.

A interpretação verossimilhantista da evidência recebe diversas críticas. Uma, no entanto, é especialmente contundente:
Edwards propõe que definir a natureza da probabilidade é um problema irrelevante, dado que a evidência estatística é
formulada por razões de verossimilhança, e não por probabilidades. No entanto, a definição de verossimilhança {\em é} uma
probabilidade, de forma que sem uma interpretação para a probabilidade não podemos ter uma interpretação para a verossimilhança.
A teoria verossimilhantista corre, portanto, o risco de se assentar
sobre uma areia movediça filosófica.
% A verossimilhança é uma função
% proporcional à probabilidade, o que não é o mesmo. Ainda assim
% compro o ponto de que se o suporte é proporcional à probabilidade
% que o modelo atribui aos dados não se escapa de definir que
% probabilidade é esta. Mas suspeito que um verossimilhantista se
% justificaria que isso já foi feito na escolha do modelo. Podemos
% testar chamando o João para sua banca.

A escola de verossimilhança vai ganhar especial força devido ao desenvolvimento de um procedimento de escolha de modelos
geral, baseado no resultado de Akaike relacionando o número de parâmetros livres em um modelo e a log-verossimilhança
com a perda de informação de cada modelo \citep{Akaike74}. % Merece
                                % uma seção à parte, pela importãncia
                                % do conceito em si e pelo fenômeno de
                                % sociologia da ciência que foi a
                                % rápida aceitação da seleção de
                                % modelos baseada em AIC nas ciências
                                % naturais (Ver Johson & Omland Trends
                                % in Ecology and Evolution). Além
                                % disso, aqui está interposto entre os
                                % conceitos e o exemplo.
\section{A incerteza e a produção de conhecimento}

Concluímos essa seção com a ponderação de que a divisão rotineiramente feita entre as ditas ``estatística frequentista'' e 
``estatística Bayesiana'' esconde o fato de que há, na verdade, uma variedade de correntes filosóficas que, baseadas
em idéias distintas sobre a natureza das probabilidades, chegam a
conclusões distintas sobre assuntos variados da epistemologia 
à metodologia de inferência. A escola frequentista é fundamentalmente dividida entre a modelagem de Fisher e a rigidez de
Neyman-Pearson, enquanto a escola Bayesiana é dividida em uma miríade de escolas, desde
subjetivistas anti-realistas como de Finetti até objetivistas como Jaynes.
% PI: Pretende falar algo de Jaynes? C: Por enquanto não
Essa taxonomia, no entanto, não acomoda a visão de positivistas lógicos como Carnap e Karl Pearson, que atacarão tanto
a visão subjetiva de probabilidade quanto o modelo de população infinita dos frequentistas \citep{Lenhard06, Zabell09}.
Tampouco é suficiente para enquadrar o trabalho de Peirce sobre propensões, ou a visão agnóstica de Edwards, 
posteriormente expandida por Royall e Sober e complementada
Akaike, que propõem a razão de verossimilhança como única base lógica para a inferência.

É necessário que as divergências entre os diversos autores citados sejam interpretadas não na sua práxis, que representa 
o nível metodológico, como costuma acontecer em círculos mais inclinados à ciência do que à filosofia, 
e sim nos seus fundamentos, em cada suposição elementar feita sobre os
elementos constituintes da teoria probabilística e de inferência. Possivelmente, essas suposições devem ser mapeadas em
distintos elementos da filosofia da ciência.

Embora essa discussão seja muito vasta, acredito que dois pontos da filosofia - e suas conexões com o uso de estatística -
não podem ser evadidos por cientistas naturais: o problema da indução e o problema da demarcação.
O primeiro diz respeito à construção de conhecimento através da observação: se observamos que o sol sempre nasce no leste,
isso pode ser usado para estabelecer como fato que o sol nascerá sempre no leste? A conexão feita entre ``eu observei esse
fato no passado'' e ``eu prevejo que esse fato se repetirá no futuro'' não encontra justificativa lógica. Nossa própria convicção
de que a indução deve funcionar para estabelecer fatos é baseada em uma indução. O segundo problema pode ser formulado como
``o que distingue uma teoria científica?'' % e qual é o problema com isto?

Embora ambos os problemas possam ser considerados abertos ainda nos dias de hoje,
as respostas dadas a ambas as perguntas por Karl Popper desfrutam de grande aceitação na comunidade científica, em particular
nas ciências naturais. 
Enquanto autores anteriores consideravam o mérito de teorias científicas associado à sua {\em verificação}, Popper
contradiz essa visão com a idéia de que uma teoria científica deve ser {\em falseável}:

\begin{quote}
``It is easy to obtain confirmations, or verifications, for nearly every theory - if we look for confirmations.

Confirmations should count only if they are the result of {\em risky predictions}; that is to say, if, unenlightened by 
the theory in question, we should have expected an event which was incompatible with the theory - an 
event which would have refuted the theory.

Every {\em good} scientific theory is a prohibition: it forbids certain things to happen. The more a theory forbids, the 
better it is.

A theory which is not refutable by any conceivable event is non-scientific. Irrefutability is not a virtue of a 
theory (as people often think) but a vice.'' \citep{Popper63}
\end{quote}

O paradigma de testes de significância de Fisher talvez seja inspirado em uma versão dessa visão de falseabilidade: uma
teoria científica é aceitável enquanto a evidência contrária não é
suficiente para refutá-la; 
% Vejo diferenças fundamentais: Para Popper nunca se prova
% a validade de uma hipótese, portanto o conhecimento será sempre
% provisório. O que se prova é a invalidade de uma hipótese a qual,
% deixa de ter importância como explicação científica. O número de
% hipóteses candidatas é ilimitado e a ciência avança por tenativa e
% erro.
%Já no teste de
% significância o que se coloca à prova é uma hipótese nula, e a
% hipótese de interesse não é diretamente testada. Mais importante, a
% hipótese de interesse é aceita porque definida como a única
% alternativa à nula. O universo de hipóteses restringe-se a duas e a
% refutaçãod a nula implica em aceitação da alternativa, o que não é o
% mesmo que ter inúmeras candidatas e nem que submeter a hipótese de
% interesse a um teste direto. 
a escola da verossimilhança
parece estender essa noção ao apresentar um conceito de evidência mais geral do que o Fisheriano, que permite o ataque apenas
sobre uma hipótese nula privilegiada. O verossimilhantista, assim, enxerga que uma teoria científica não tem mérito absoluto:
ela só pode ser julgada adequada ou inadequada frente a explicações comparáveis vindas de outras teorias.
% Vejo paralelo com as ideias de Lakatos, que propôs que teorias são
% sempre conforntadas com outras, e que nenhuma revolução científica é
% motivada simplesmente para derrubar uma teoria vigente, mas sim para
% colocar uma concorrente no lugar.

Em contraste, Thomas Kuhn defende que o progresso da ciência não se dá por
simples acúmulo de evidência, e sim por um processo não-linear e episódico, no qual períodos de acumulação de evidência se
intercalam com períodos revolucionários \citep{Kuhn62}. 
Seu exemplo mais famoso é a recordação do modelo Copernicano de movimentação planetária:
Kuhn aponta que não apenas as observações astronômicas da época não eram capazes de refutar o modelo Ptolomaico de epiciclos,
como o modelo original proposto por Copérnico não trazia nenhuma melhora em poder preditivo. A preferência de alguns cientistas
a favor do modelo Copernicano se dava simplesmente por uma maior simplicidade deste. A premissa de que modelos mais simples
são preferíveis vai encontrar eco nas formulações de inferência baseadas na teoria da informação. Kuhn, no entanto, vai além
e propõe que diferentes paradigmas científicos são incomensuráveis, de forma que o ferramental e a linguagem desenvolvidos
em um não pode ser usado para refutar o outro. Essa visão, controversa entre filósofos da ciência, colocaria em cheque o uso da
inferência estatística como procedimento de arbitragem entre teorias.
% O que não é o mesmo que ser um procedimento de avaliação de
% suporte empírico no âmbito de cada corpo teórico. Novamente uma
% questãod e qual o papel da estatística na produção de conhecimento científico.

Já em relação ao problema da indução, Popper propõe que ele não tem uma solução aceitável porque a pergunta, em si, está
equivocada:

\begin{quote}
``The actual procedure of science is to operate with conjectures: to jump to conclusions — often after one single observation.
(...) Tests proceed partly by way of observation, and observation is thus very important; but its function is not that of 
producing theories. It plays its role in rejecting, eliminating, and criticizing theories'' \citep{Popper63}
\end{quote}

Vamos adotar neste trabalho uma visão essencialmente Popperiana, e propor que toda teoria estatística, para poder ser
usada na ciência, deve prover uma forma de testar teorias conflitantes (e comensuráveis)
e de rejeitar teorias incompatíveis com as observações experimentais. Como as teorias relevantes para problemas práticos costumam
realizar previsões sobrepostas, % o que é isso?
sem que testes conclusivos possam ser feitos, esse requisito tem dupla função: garantir
que toda teoria científica é testável pelos procedimentos estatísticos é também garantir que o procedimento estatístico escolhido
permite o teste de qualquer teoria científica relevante. Este requisito terá importância central no problema de testar
teorias cuja própria formulação é estatística:

\begin{quote}
``Those theories which are under a probabilistic form (thermodynamics, statistical [mechanics?], etc.) can never be refuted (...):
after all, even an event with probability 0 can well occur. According to Popper we then have to take a methodological decision
and consider a probabilistic theory {\em T} refuted if a certain event occurred which, as far as {\em T} is concerned, would 
otherwise be extremely unlikely.'' \citep{deFinetti2010}
\end{quote}

Reforçamos que a discussão metodológica na aplicação da estatística às ciências práticas deve estar subordinada a uma discussão
epistemológica e filosófica sobre a verdadeira base do pensamento estatístico. Nesse sentido, a proposição de
métodos baseados na verossimilhança no presente trabalho não trata de rejeitar o uso de testes de hipótese 
e procedimentos frequentistas em
bases pragmáticas, e sim na construção de ferramentas e na aparelhagem do pesquisador com métodos que sejam
adequados ao uso na ciência, norteada na discussão acima sobre indução e demarcação.
% Minha observação mais importante  para esta parte: 
% Cotejar epistemologia e inferência estatística é muito difícil e cheio de
% armadilhas. Uma das primeiras armadilhas é a da correspondência
% fácil e direta.  Um exemplo artificial seria: há teste em
% epistemologia e em estatística, logo os testes devem
% partilhar a mesma lógica.  Um caso real é o argumento Bayesiano que
% todo cientista tem conhecimento a priori e que o progresso
% científico é o aperfeiçoamento de conhecimento legado pelos nossos
% antecessores. Em primeiro lugar, já se impõe aí uma certa concepção
% de progresso científico. Fica a pergunta: esta concepção foi uma
% premissa da construção da inferência Bayesiana ou uma vez
% estabelecida esta escola buscou a prosteriori a concepção que melhor
% se adequava aos seus procedimentos?
% De uma forma ou de outra, há mais de uma possibilidade. No caso, a concepção Popperiana de avanço por aproximação,
% mas não com as interpretações de Khun ou Lakatos, para citar apenas
% os dois críticos de primeira hora de Popper, e os dois mais
% conhecidos por cinetistas naturais. O problema é que por outros
% critérios talvez as visões de Khun e Lakatos se adequem mais.
% Mas siga adianta e suponha que se aceite esta
% concepção de progresso. Isso não implica necessariamente em aceitar que todas as
% ações do cientista rumo ao progresso devam ser formalizadas em
% procedimentos estatísticos. Este é o ponto do Royal: a estatística
% não é todo o processo de pesquisa, e precisa delimitar seu papel
% apropriadamente para melhor cumprí-lo. Ainda neste exemplo do
% argumento Bayesiano, incorporar na estatística um procedimento de
% atualização de visão de mundo (ao invés de restringir-se a
% informar o suporte que os dados dão a uma visão em detrimento da
% outra, como defende Royal) pode dar uma falsa noção de objetividade
% ou normatização a um procedimento que sempre contem um certo grau de
% subjetividade.
% Por fim, as diferentes escolas de epistemologia analisam níveis
% diferentes da produção de conhecimento científico. Apelando a
% Lakatos, há procedimentos
% de manutenção de um núcleo duro de teoria que não é testada, e
% outros que são os testes de hipóteses derivadas. Cada uma dessas
% camadas tem lógicas e princípios distintos, e há ainda princípios
% gerais que os integram. Sabemos que a estatística liga dados a
% teoria. Mas em qual camada se situa, e de que maneira?
% Em resumo, a pergunta geral que começou meu comentário e que parece
% ser a sua é interessante, e como você acredito que devem existir
% alguns princípios lógicos comuns entre inferência estatística e
% epistemologia.  Mas é terreno muito traiçoeiro, precisamos da ajuda de filósofos da
% ciência para fazer avanços fundamentados.
 
