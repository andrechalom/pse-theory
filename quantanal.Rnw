\SweaveOpts{fig=T,echo=F}
\section{Quantitative output analysis}\label{QuantAnal}
\subsection{Uncertainty analysis}\label{UA}
The first question we would like to answer, in the context of quantitative 
analysis, is what is the probability distribution of the response variable $y$
given that we know the join probabilities of the input parameters $\bu{x}$ 
(see definitions in section \ref{PSE}), which is the subject of uncertainty
analysis \cite{Helton03}. 

This can be done by fitting a density curve to the output $y$ or an empiric
cumulative distribution function (ecdf). If there is any theoretical reason
to believe that the distribution of $y$ should follow one given distribution,
it is possible to fit this function to the actual output data and estimate
the distribution parameters. If the joint distribution of the input parameters
correspond to the actual probability of some natural system to exhibit some
given set of parameter values (as opposed to the case where we have no 
biologically 
relevant estimates for some parameters), the estimate represented by the
density and ecdf functions approaches the actual distribution that the
variable $y$ should present in nature. This functions may be used, for example,
to provide confidence intervals on the model responses.

However, this is only the case when the input variables are uncorrelated or
when enough correlation terms have been taken into account. Smith (\cite{Smith02})
provides an example where ignoring the correlation terms 
leads to inaccuracies on the estimation of confidence intervals.

The next reasonable step is to construct and interpret scatterplots relating
the result to each input parameter. These scatterplots may aid in the visual
identification of patterns, and altough they cannot be used to ``prove'' 
any relationship between the model response and input, they may direct the
research effort to the correct analyses. There are extensive reviews of the
use of scatterplots to identify the important factors and emerging patterns
in sensitivity analyses \cite{Kleijnen99}.

We will present here some quantitative analyses tools, aimed at identifying
increasingly complex patterns in the model responses. It should be stressed
that no single tool will capture all the relations between the input and
output. Instead, several tools should be applied to any particular model.

\subsection{Sensitivity analysis}
The question of ``what is the effect of some combination of parameters to
the model output'' may be answered by testing the relation between
the parameters and outputs. 
There are extensive reviews about detecting these relations after
generating samples with Latin Hypercubes \cite{Marino08, Kleijnen99}, 
so we will give just a brief overview. We will first note that
the methods used must take into account the variation of all the 
parameters. For example, instead of calculating the correlation between
the result and some parameter, partial correlation coefficients should
be used, which
discount the effect of all other parameters. 

These relations between the result and the parameters may be classified,
in order of
increasing complexity, as:

\begin{itemize}
		\item Linear relation, which can be tested with the Pearson partial correlation coefficient,
		\item Monotonic relation, which can be tested with the Spearman partial correlation coefficient,
		\item Trends in central location, for which the Kruskal-Wallis test may be applied,
		\item Trends in variability, for which the FAST method and Sobol' indices may be used.
\end{itemize}

Subsections \ref{linear} to \ref{FAST} will provide some
mathematical background for each method, and section \ref{Leslie} 
will present examples of use of those tests. We should stress here that
the application of one method is not enough to draw conclusions about 
the relations between the input and output variables, as these techniques
test different hypotheses, and have different statistical powers. Instead,
every model should be analysed by a combination of techniques, preferably
one for each category outlined here.

\subsubsection{Linear relation}\label{linear}
The most straightforward relationship between $y$ and $x_i$ is the linear, 
represented by $y \sim x_i$. This is case if, every time $x_i$ is increased, 
$y$ increases by aproximately the same ammount.
The Pearson correlation coefficient is the commonly used measure to test for
a linear correlation:

\begin{equation}
	\rho_{yx_i} = \frac{\sigma_{yx_i}}{\sigma_y\sigma_{x_i}}
	\label{PearsonRho}
\end{equation}

Where $\sigma_a$ is the variance of $a$ and $\sigma_{ab}$ is the covariance 
between $a$ and $b$. The correlation coefficient is a measure of the predicted
change in $y$ when $x_i$ is changed one unit, relative to its standard
deviations, and, as such, approaches $\pm 1$ when there is a strong
linear relation between the variables. The square of $\rho$, usually written as
$R^2$, measures the fraction of the variance in the output that can be accounted
for by a linear effect of $x_i$.
Is is usual to test the significance of this linear relation by a t-test 
\cite{Freedman07}.

Other than examining the individual relationships between the parameters and 
the output, we can investigate the joint effect of several $x_i$, as
$y \sim x_1 + x_2 + \cdots + x_m$. In this case, the multiple $R^2$ represent
the fraction of the variance on the output due to linear effects of all 
the $x_i$ considered.

However, a measure of $\rho$ close to zero does not mean that
no relationship exists between $y$ and $x_i$ - for instance, $x^2 + y = 1$, 
$x \in [-1,1]$ presents $\rho = 0$, so clearly other methods might be needed.

The Partial Correlation Coefficient (PCC) between $x_i$ and $y$ is the measure
of the linear effect of $x_i$ on $y$ after the linear effects of the remaining
parameters have been discounted. In order to calculate the PCC, first we fit
a linear model of $x_i$ as a function of the remaining parameters:

\begin{equation}
	\hat{x}_i \sim x_1 + x_2 + \cdots + x_{i-1} + x_{i+1} + \cdots + x_m
	\label{PCChatx}
\end{equation}

A corresponding model is done with $y$:

\begin{equation}
	\hat{y} \sim x_1 + x_2 + \cdots + x_{i-1} + x_{i+1} + \cdots + x_m
	\label{PCChaty}
\end{equation}

The PCC is calculated as the correlation between the residuals of these two
models:

\begin{equation}
	PCC(y, x_i) = \rho \left( (y - \hat y), (x_i - \hat{x}_i) \right)
	\label{PCC}
\end{equation}


\subsubsection{Monotonic relation}\label{PRCC}

Let us refer to each value of $y$ as $y_k$ and each value of $x_i$ as 
$x_{ik}$. The rank transformation of $y$, 
represented by $r(y_k)$ can
be found by sorting the values $y_k$, and assigning rank 1 to the smallest, 2 
to the second smallest, etc, and $N$ to the largest. The rank of $x_{ik}$,
$r(x_{ik})$, can be found in a similar way.

If there exists a strictly monotonic relation between $y$ and $x_i$, that is,
if every time $x_i$ increases, $y$ either always increase or always decreases
by any positive ammount, it should
be clear that the ranks of $y$ and $x_i$ present a linear relationship: 
$r(y) \sim r(x_i)$.

The correlation between $r(y)$ and $r(x_i)$ is called the Spearman 
correlation coefficient
$\eta_{yx_i}$. The same analyses presented on section \ref{linear} can also be
applied for the rank transformed data, including significance testing and 
multiple regression.

If the procedure described to calculate the PCC is followed on rank transformed
data, that is, if $y$ and $x_i$ are rank transformed and fitted as linear models
of the remaining parameters, the correlation between the residuals is called
PRCC, or Partial Rank Correlation Coefficient. This measure is a robust
indicator of monotonic interactions between $y$ and $x_i$, and is subject
to significance testing as described in \cite{Marino08}. This measure will 
perform better with increasing $N$.

\subsubsection{Trends in central location}\label{Kruskal}
Even if the relation between $y$ and $x_i$ is non monotonic, it may be 
important and well-defined. The case in which $y \sim x_i^2$, $x_i \in (-1,1)$
is a common example. This relation may be difficult to visualize, and sometimes
may not be expressed analytically. In these cases, the Kruskal-Wallis rank
sum test may be used to indicate the presence of such relations 
\cite{Kleijnen99}.

In order to perform the test, the distribution of $x_i$ must be divided 
into a number $N_{test}$ of disjoint intervals. The model response $y$ is then
grouped with respect to these intervals, and the Kruskal-Wallis test is used
to investigate if the $y$ values have aproximately the same distribution
in each of those intervals. A low p-value for this test indicates that the
mean and median of $y$ is likely to be different for each interval considered,
and thus that the parameter $x_i$ have a (possibly non monotonic) relationship
with $y$.

The number of intervals $N_{test}$ is not fixed as any ``magical number'', and
may have a large impact on the test results. It is then recommended that 
this test should be repeated with different values to obtain a more 
comprehensive picture of the interactions between $x_i$ and $y$ (fig. \ref{fig:Kruskal}).
\begin{figure}[htbp]
	\begin{center}
<<Kruskal, height=3.5>>=
par(mfrow=c(1,2), mar=c(3,2,2,1), pch=4)
x<-seq(-1,0.97,0.01)
y<-(x-1)*(x+1)+rnorm(x,0,0.25)
xcats <- rep(c(1,2), each=length(x)/2)
K <- kruskal.test(y,xcats)$p.value
plot(x,y, xlab='', ylab='',main=paste('N = 2, p =',format(K, digits=2)))
abline(v=0, lty=3)
ymean = tapply(y,xcats,mean)
segments(c(-1,0),ymean,c(0,1),ymean, lwd=3)
xcats <- rep(c(1,2,3), each=length(x)/3)
K <- kruskal.test(y,xcats)$p.value
plot(x,y, xlab='', ylab='',main=paste('N = 3, p =',format(K, digits=2)))
abline(v=-0.333,lty=3); abline(v=0.333,lty=3)
ymean = tapply(y,xcats,mean)
segments(c(-1,-0.333,0.333),ymean,c(-0.333,0.333,1),ymean, lwd=3)
@
	\end{center}
	\caption{Example of application of the Kruskal-Wallis test on the
	same data set, which present a strong quadratic component, by
	dividing the range in 2 intervals (right) or 3 intervals (left).
	The dashed lines are the divisions between the intervals, and
	the strong horizontal lines are the sample means for each 
	interval.}
	\label{fig:Kruskal}
\end{figure}

\subsubsection{Trends in variability}\label{FAST}
Other than the central tendency of the results, their dispersal may be 
dependent on the input parameters. A classical approach which may be used
to test whether the dispersal of the output is related to any input
parameter is to divide the distribution of $x_i$ must be divided 
into a number $N_{test}$ of disjoint intervals and group the model response
$y$ with respect to these intervals, as done on the Kruskal-Wallis test.
In this case, the ANOVA F statistic can be used to test for equality 
between the of $y$ conditional to each class \cite{Kleijnen99}.

A similar approach, which we will use, is to employ the eFAST indices
\cite{Saltelli99, Saltelli04},
which is a variance decomposition method based on the FAST and
Sobol' indices. While the Sobol' indices were described in 1969, in Russian,
FAST was developed by Cukier et al. in 1973, and both are identical in all
but one computation \cite{Archer97}. 

These methods estimate what fraction of the output variance can be explained
by variation in each parameter $x_i$, which is called the {\em first-order 
sensitivity of $x_i$} or {\em main effect of $x_i$}. The method estimates
as well the fraction which is explained by 
the higher-order interactions between $x_i$ and all other parameters.
The sum of all terms related to $x_i$ is called the {\em total-order
sensitivity of $x_i$}. 

The eFAST method estimates the main effect of each parameter by chosing 
a periodic function $f_i(x_i)$ for each parameter, where the 
frequency $\phi_i$ of
each function is distinct, and should, in theory, be incommensurable.
Each of this functions is sampled $N_s$ times, and a Fourier analysis
is applied to the model output. The Fourier coefficients at each frequency
$\phi_i$ is related to the main effect of the variable $x_i$. The total
order sensitivity of $x_i$ is then calculated as the fraction of the
variation which is not explained by the complimentary of $x_i$ (that is,
all parameters but this one).

There are two things that should be noted here about eFAST indices. The
first one is that the eFAST calculation does not involve the LHS sampling
scheme, and may require more model evaluations. Also, this method produces
small positive total-order sensitivity estimates even for parameters 
which do
not play any role on the model output, as many numeric approximations are
involved. 

\begin{shaded}
		\textbf{Box 2: Uncertainty and sensitivity}

		The following questions should be addressed in uncertainty
		and sensitivy analyses (between parentheses are the section numbers
		which detail each step):
		\begin{itemize}
				\item What is the range and distribution of model responses (\ref{UA})?
\item The ecdf plot of model responses suggests any theoretical distribution (\ref{UA})?
\item The scatterplots between each variable and the model response suggest
		any relationship (\ref{UA})?
\item Is there any linear or monotonic relation between the model 
		parameters (\ref{linear} and \ref{PRCC})?
\item Is there any trend in central location or dispersion of the response (\ref{Kruskal} and \ref{FAST})?
\item The output from the analyses is coherent between different sample sizes (\ref{SBMA})?
\end{itemize}
\end{shaded}
