\documentclass[twoside,12pt,a4paper]{article}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage[twoside,margin=1.2in]{geometry}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage[square,sort]{natbib}
\usepackage{framed, color}
\definecolor{shadecolor}{rgb}{0.9, 0.9, 0.9}
\setlength{\topmargin}{0cm}

% Create friendly environments for theorems, propositions, &c.
\newtheorem{axiom}{Axioma}[section]
\newtheorem{theorem}{Theorem}[section]
\newenvironment{proof}[1][Proof]{\begin{trivlist}
	\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{definition}[1][Definition]{\begin{trivlist}
	\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newcommand{\bu}[1]{\mbox{$\mathbf{#1}$}}
\usepackage{xspace}
\newcommand{\R}{\textnormal{\sffamily\bfseries R}\xspace}

\usepackage{Sweave}
\begin{document}
\SweaveOpts{fig=T,echo=F}
\setkeys{Gin}{width=0.8\textwidth}

\title{Uncertainty analysis and composite hypothesis under the likelihood paradigm}
\author{Andre Chalom \and Paulo Inacio Prado,\\ 
  Department of Ecology, Institute of Biosciences, University of SÃ£o Paulo \\
  \texttt{andrechalom@gmail.com}
  }
\date{07/28/2015}
\maketitle

\begin{abstract}
%%% ABSTRACT
  Abstract goes here
\end{abstract}

\newpage
\vfill
\noindent
``In an ideal world, all data would come from well-designed experiments
and would be sufficient to simultaneously estimate all parameters using rigorous
statistical procedures. The world is not ideal. One must often combine estimates
from different experiments, or supplement high-quality data (...) with uncertain
data, or even assumptions (...). Think carefully about whether your conclusions
may be artifacts of your assumptions or calculations, and document those assumptions
and calculations so that your reader can ask the same question, and then carry on.''
\begin{flushright}
(H. Caswell, Matrix Population Models)
\end{flushright}
\vfill
\indent
\newpage
\tableofcontents
\newpage

\section{Motivation}

The use of mathematical and computational models is now widespread in several fields of the biological 
sciences. However, several applications of these models lack one or more steps that should be taken in
order to correctly understand and interpret its results \citep{Bart95}. Though seldomly discussed together,
five steps can be seen as fundamental in this process: {\em verification, validation, calibration, 
uncertainty analysis} and {\em sensitivity analysis}. 

The two first steps are very tighly correlated, both in historical terms and in the common practice,
so it's common to refer to the set of validation and verification as ``V \& V''. We can think of verification
as trying to ensure that the computer code is correctly implementing the desired model, while validation
is trying to ensure that the theoretical model is capable of reproduce the actual phenomena of interest.
Another way to see it is to think that validation is determining that the model is solving the right 
equations, while verification is determining that the model is solving the equations right \citep{ASC2010}.

The third step, calibration, consists in determining the right values for the input parameters of a model
in order to enhance the correspondency between a model run and one observed situation. An appropriate
calibration of the model is then essential for the model to be used in a predictive fashion \citep{ASC2010}.

The last steps consist in determining how much the variation of the input parameters is translated into
the total variation of the results, which is called uncertainty analysis, and how much of the variation
of the results can be ascribed to the variation of each individual parameter, which is called sensitivity
analysis \citep{Helton03, Helton05}. These two have been extensively reviewed in another text 
(\citep{Chalom12}), so we will just highlight the fact that these analyses, together with model calibration,
strongly depend on previously and correctly executing the model verification and validation.

During the development and analysis of mathematical models in ecology, it is common to have completely 
separated procedures for calibration by parameter estimation (which we will call CPE) and 
sensitivity and uncertainty analyses (SUA) of a model. These steps are carried out in different sections of
the research papers, discussed in different chapters of books (ie: \citep{Caswell89}), and sometimes are
even performed by different people. In matrix models for population projections, the results discussion
is focused on a single combination of parameters, considered to be an optimum for the CPE, and the
SUA is presented as a separate step, being secondary to the main results (see, for example, 
\citep{SilvaMatos99}. We have no guarantees that even the statistical schools of though considered for 
both procedures will be coherent.

Moreover, the parameter set used in the model calibration may have few to no relation to parameters that
have biological relevance. In the case of matrix models, the former may be parameters related to a time 
series, while the later might be the projection matrix entries. This way, the researcher responsible for
the calibration may see the system as individual counts over time, while the researcher responsible for 
the uncertainty analyses will only be interested in the vital rates, which are real valued. When developed
in such disparate worlds, the SUA will be unable to point out directions for the planning of new experiments
or to single out which parameters should be targeted for more sampling effort.

This lack of integration between these fundamental steps for model analysis may have its roots in the
development of the CPE and SUA theories. While the parameter estimation has always been seen as belonging
to a statistical theory, the sensitivity of biological models was developed as an analytical tool, based on
the linear expansion of functions around a privileged point. Depending on the nature of the experiments
designed to measure vital rates, there may be a plethora of methods that might be used to determine the
set of values that best represent the status of knowledge that we have about a given natural system - and
these methods may stem from frequentist, bayesian or likelihood-based approaches to the statistical theory.
The analytical theory of SUA, on the other hand, is in great part due to the work of Hal 
Caswell \citep{Caswell89}, and proceeds by taking a model that is already optimally parametrized, and studying
the first order derivatives of the model answer in relation to each input parameter. This formulation 
disregards completely the quantity and quality of collected data, except for their average, and sensitivity
measures are taken exclusively over the variation of the model answer in response to infinitesimal changes.
Thus, models that are parametrized by data with large uncertainties will not present larger sensitivity 
indexes than models parametrized by data with great precision. Increasing the sampling effort to gather
data about the system will not affect the uncertainty values. We can conclude that the analytical formulation
of the SUA, ultimately, gives information about the structure of the model, but is silent about the more
embracing questions from the collection of field data to the formulation and execution of the model. This might
lead to a false sense of confidence being placed on studies that present a stable matrix structure, leading
to small sensitivity indexes, but use data with large uncertainties.

Other than this, questions about model validation are not usually addressed by literature in this field,
but they are also intimately connected to the relevant questions about uncertainty. We can
divide the uncertainty about a model in three main sources: structural uncertainty, parameter uncertainty
and stochastic uncertainty (see \citep{Marino08} for a review about the latter two). While most
of the SUA techniques are focused on the second component, the validation of a model may point out
how much confidence we can have about whether one model (or a set of models) is adequate at
representing the desired phenomena.

A stochastic formulation for a theory of global uncertainty and sensitivity analyses, as described in 
\citep{Chalom12}, and interpreted under the likelihood paradigm for statistics, is able to regard
in the same way all of the uncertainty sources described above, generating a complete picture of our
knowledge about a system. Although these formulations may converge for very simple systems, this is not
the general case. 

The main advantages of using a stochastic formulation are:

\begin{enumerate}
  \item The analytical procedures are local, thus responding only to what happens after infinitesimal
    perturbations, and depends on the model functions being ``well-behaved'' in the chosen neighborhood.
    The stochastical procedures are global and have no such limitation.
  \item Any stochastic approach allows for the information contained in the sample variability to be used
    to represent the uncertainty about the parameters.
  \item By accepting the Likelihood Principle, we know that all relevant information on the samples is
    contained in the associated likelihood function. This way, we can use the likelihood function alone
    to proceed in the analysis, while the analytical procedures at times will disregard information, and
    at times will lead to the false sensation of having more detailed information than the sample allows.
\end{enumerate}

\section{Historical background}\label{sec:likelihood}

The statistical use of likelihoods is very widespread, starting from Fisher's definition on 1922, and
spanning several schools of thought. 
On the one hand, the framework for Neyman-Pearson's hypothesis testing is built upon it; on the other hand,
it is a bridge between the {\em a priori} and {\em a posteriori} probabilities in Bayesian analysis. 
Starting on the decade of 1960, the use of likelihoods as a basis for statistical inference started to be
more widely advocated as an alternative to both frequentism and Bayesianism. The 1965 book by Ian Hacking 
about the logic of inference and the 1972 book by A.W.F. Edwards on likelihoods can be seen as the starting
points for this position to become an independent school of thought, sometimes called Likelihoodism.
More recently, this position will be strongly supported by the works of Richard Royall and Elliott Sober.

The likelihoodist school of thought is based on the conjunction of the likelihood principle, as demonstrated
by Allan Birnbaum, and the law of likelihood, as enunciated by Ian Hacking.

Birnbaum derived the likelihood principle in 1962, from two generally accepted principles\footnote{
This demonstration is controversial to this date. See \citep{Mayo10} for a recent critique and 
\citep{Gandenberger12} for a recent defense}: the sufficiency principle and the conditionality principle.
Informally, we can think about the two first affirming that ``data that does not aggregate new information
is irrelevant to the inference'', and ``experiments that could have been realized but were not are irrelevant
to the inference''. From this principles, Birnbaum deduces the likelihood principle, which can informally
be stated as ``experimental results which are possible but were not observed are irrelevant to the 
inference''. Or, in Birnbaum's more formal terms:

\begin{quote}
``The likelihood principle: If $E$ and $E\prime$ are any two experiments with the same parameter space,
 represented respectively by density functions $f(x, \theta)$ and $g(y, \theta)$; and if $x$ and $y$ are
 any respective outcomes determining the same likelihood function; then $Ev(E, x) = Ev(E\prime, y)$.
 That is, the evidential meaning of any outcome $x$ of any experiment $E$ is characterized fully by giving
 the likelihood function $cf(x, \theta)$ (which need be described only up to an arbitrary positive constant
 factor), without other reference to the structure of $E$.''
\citep{Birnbaum62}
\end{quote}

The law of likelihood was formulated by Hacking as:

\begin{quote}
``If hypothesis $A$ implies that the probability that a random variable $X$ takes the value $x$ 
is $p_A(x)$, while hypothesis $B$ implies that the probability is $p_B(x)$, then the observation $X=x$
is \textbf{evidence supporting $A$ over $B$} if and only if $p_A(x) > p_B(x)$, and the likelihood ratio, 
$p_A(x)/p_B(x)$, measures the strength of that evidence.''
\citep{Hacking65}\footnote{Emphasis added}
\end{quote}

The divergence between the likelihood school and frequentist school is due to the later rejecting the
likelihood principle (although it accepts the principles of sufficiency and conditionality). On the other 
hand, the Bayesian school generally disagrees with the likelihoodists by the interpretation of the law of
likelihood.

The likelihood school of thought can be seen as an intellectual heir to the Neyman-Pearson hypothesis
testing framework, as its breaking point from Fisherian significance testing happens because of the
logical necessity of comparing competing hypothesis. Fisher's tests give a special privileged position
to the so-called {\em null hypothesis}, which Neyman and Pearson will see as incomplete\footnote{
For a more complete and extremely didactic insight into their view, see the original paper from 
\citep{Neyman1933}}. The data
gathered by one experiment may indicate a small or a large support for one given hypothesis, but this alone
should not be considered as evidence in favor of this hypothesis before the other alternatives have
been also scrutinized. In other words, the probability or improbability of a single hypothesis should not
be used to conclude about its veracity. As the fictional detective Sherlock Holmes, by Sir Arthur Conan
Doyle famously said, ``How often have I said to you that when you have eliminated the impossible, 
whatever remains, {\em however improbable}, must be the truth?''. 

However, while Neyman and Pearson will use the likelihood ratio from several hypothesis to guide the 
{\em behavior} of the scientist in accepting or rejecting hypothesis, Royall and Edwards will separate
the concepts of (1) the degree of certainty that we have about one hypothesis; (2) the strenght of evidence
that some data confers to one hypothesis over the others and (3) the course of action that should be taken
after examining such evidences. While Neyman and Pearson conflate the strenght of evidence (2) with the
course of action (3), and Bayesian analysis conflate the degree of certainty (1) with the strenght of 
evidence (2), the proposers of Likelihoodism propose that the role of statistical inference is to provide
{\em only} the strenght of evidence \citep{Royall97}. 
The degree of certainty and course of action should take into account
a multiplicity of other factors, and should not be part of the inference. One of the greatest precursors 
of the statistical thought, Marquis Pierre-Simon de Laplace, had already foreshadowed this position with
the problem of the number of judges necessary to convict or acquit a prisioner: this decision, to Laplace,
needs to take into account not only the probability of convicting an innocent of acquitting a guilty 
prisioner, but also whether the punishment will be a fine or the death sentence\citep{Laplace1814}.

Moreover, by dissociating the strenght of evidence from the decision making, Royall and Edwards are asking
the scientist to reveal the untransformed likelihood ratios derived from its data, instead of transforming
it into p-values or {\em a posterioris}, so that her peers are able to assess clearly what is the presented
evidence. By retaking the scientist role as a subjective decision maker, this proposition converges to
Fisher's, who criticized Neyman-Pearsons programme as being useful for industrial processing, but useless
to the scientific community. The likelihoodist proposal thus echos the words of Fisher, and also the words
of the Bayesian probabilist I. J. Good:

\begin{quote}
``We have the duty of formulating, of summarizing, and of communicating our conclusions, in intelligible form, in
recognition of the right of other free minds to utilize them in making their own decisions.'' 
\citep{Fisher1955}
\end{quote}

\begin{quote}
``If a Bayesian is a subjectivist, he will know that the initial probability density varies from person to person and
so he will see the value of graphing the likelihood function for communication.'' \citep{Good76}
\end{quote}

However, there is a very important difference between the Bayesian and the Likelihoodist paradigms:
the {\em a posteriori} probabilities, constructed by the multiplication of the likelihood by an {\em a priori}
probability under the Bayesian school, is a absolute quantity. It is thus possible to refer to the {\em
a posteriori} probability of event $A$, or the {\em a posteriori} probability of an event $B$. In contrast,
the Likelihoodist approach will see in the likelihood ratio the end point for the statistical analysis. 
This is why the law of likelihood is expressed over {\bf evidence supporting $A$ over $B$}: because the
likelihoodist approach does not allow for the transformation of the relational support of $A$ over $B$ to
be translated into non-relational quantities of support for each hypothesis\footnote{See the text by
\citep{Fitelson07} for more details on this matter, including Carnap's classification of confirmation}.

\section{The challenge for uncertainty estimation}
From this section, we will consider problems where $\bu{x}$ represent a vector of data obtained independently
from one or more experiments from a vector-valued random variable $\bu{X}$, such that $P(\bu{X}\!=\!\bu{x}) = f(\bu{x};\theta)$. The parameter vector $\theta$ is unknown and may assume values in $\Theta$. We will
refer to the likelihood of $\theta$ given the observed data $\bu{x}$ as 
$\mathcal{L} (\theta | \bu{x}) = f(\bu{x}; \theta)$. The likelihood ratio over the same 
data set can be abbreviated as
$L(\theta_1, \theta_2) = \frac{\mathcal{L}(\theta_1|\bu{x})} {\mathcal{L}(\theta_2|\bu{x})}$. 

In order to perform an uncertainty analyses for a mathematical model under the likelihood paradigm, the 
general question that we would like to answer can be written as: ``how much support does the collected data
provide for the concurrent hypothesis over the model results?''. For example, in a population growth model,
the question may become ``how much support the collected data provide for the hypothesis that
the population is growing at the rate of 1 individual per year, {\em versus} the hypothesis that it is stable
?''\footnote{As we will see, the question can become more general, such as ``how much support the collected
data provide for the hypothesis that the population is growing or stable {\em versus} the hypothesis that is
declining?''}

This question, formulated over the parameters of a probability distribution, should be 
answered by means of the likelihood function. It can be demonstrated that one-to-one functions over the
parameters presenrve the likelihood function, such that if $\mathcal{L}(\theta|\bu{x})$ is the likelihood
of $\theta$ and $\phi = f(\theta)$ is given by a one-to-one function $f(\cdot)$, the 
likelihood of $\phi$ is given by 
$\mathcal{L}(\phi|\bu{x}) = \mathcal{L} \left(f(\theta)|\bu{x}\right)$ \citep{Edwards72}.

However, the application of a mathematical model is analogous to the application of a generic function, 
and the reasoning above does not hold for arbitrary many-to-one functions. Thus, defining a methodology
for uncertainty analyses of arbitrary models is closely related to the problem of composite statistical
hypotheses. However, the law of likelihood is expressed in terms of simple statistical hypothesis, and there
is no way of combining these values to form the ``likelihood of a composite hypothesis''. As Edwards writes:

\begin{quote}
``No special meaning attaches to any part of the area under a likelihood curve, or to the sum of the likelihoods of two or more
hypotheses (...).
Although the likelihood function, and hence the curve, has the mathematical form of a [known] distribution, it does not
represent a statistical distribution in any sense.'' \citep{Edwards72}
\end{quote}

How can we work with composite hypothesis then? The traditional answer from the proposers of Likelihoodism is
that we don't. Edwards puts off composite hypothesis as ``uninteresting to science'', while Royall sees
this restriction to simple hypothesis as an advantage, not a limitation, to the likelihood approach.
However, this is necessary if we want to derive an uncertainty estimation for model results.
Here, we will present three paths that may be pursued for this end:

\begin{enumerate}
	\item Formulate a new law of likelihood, which can be applied for composite hypothesis;\label{i:alt}
	\item Present a methodology based on a logical extension of the law of likelihood;\label{i:ext}
	\item Address the issue using a model similar to the one applied for {\em nuisance parameters}. \label{i:prof}
\end{enumerate}

Proposal \ref{i:alt} is based on the axiomatic formulation of a generalized law of likelihood (GLL) that
is compatible with the law of likelihood (LL) for simple hypothesis for simple hypothesis, but able to extend
its domain for composite hypothesis. And propostal \ref{i:ext} is based on the use of a weaker definition
of statistical evidence, based on a weak law of likelihood (WLL). We will review some existing results related
to these proposals in the following sections.

Proposal \ref{i:prof} is based on methods such as profiled likelihood, which are {\em ad hoc} methods
widely used to reduce the study of multiparametric statistical models to visualizing one parameter at a time.
A typical use for these methods is exemplified by the problem of fitting a normal distribution over some data.
While, strictly, the comparable hypothesis must be given by pairs $(\mu = \mu_0, \sigma = \sigma_0)$,
representing one value for the mean and on for the variance of the distribution, it is possible to
compare the approximate support for different values for the mean, while leaving the variance free to assume
any viable value - thus treating the composite hypothesis $(\mu = \mu_0, \sigma \geq 0)$. We will get back
to this proposal in section \ref{sec:plue}.

\section{GLL: A general law of likelihood}\label{sec:GLL}

Talvez a generalizaÃ§Ã£o mais Ã³bvia para a lei da verossimilhanÃ§a para hipÃ³teses compostas seja tomar o mÃ¡ximo (ou supremo,
no casos em que o mÃ¡ximo nÃ£o existe) da verossimilhanÃ§a de suas hipÃ³teses simples. Esse Ã© o caminho perseguido por
\cite{Zhang09, Zhang13} e \cite{Bickel10}, por exemplo. Zhang considera duas hipÃ³teses, $H_1 : \theta \in \Theta_1 \subset \Theta$
versus $H_2 : \theta \in \Theta_2 \subset \Theta$, e postula os seguintes axiomas (levemente modificados para coerÃªncia
com a notaÃ§Ã£o):

\begin{axiom}
	Se $\inf \mathcal{L}(\Theta_1 | \bu{x}) > \sup \mathcal{L} (\Theta_2 | \bu{x})$, entÃ£o a observaÃ§Ã£o $\bu{x}$ 
	Ã© uma evidÃªncia a favor de $\Theta_1$.\label{ax:inf}
\end{axiom}

\begin{axiom}
	Se $\bu{x}$ Ã© evidÃªncia a favor de $H_1^*$ em relaÃ§Ã£o a $H_2$ e $H_1^*$ implica em $H_1$, entÃ£o $\bu{x}$ Ã© evidÃªncia de 
	$H_1$ sobre $H_2$.\label{ax:coh}
\end{axiom}

O primeiro axioma estabelece que se a imagem de $\mathcal{L} (\Theta_1|\bu{x})$ e $\mathcal{L} (\Theta_2|\bu{x})$ sÃ£o intervalos
disjuntos, portanto, se toda hipÃ³tese simples que implica em $H_1$ Ã© suportada {\em versus} toda hipÃ³tese simples que implica
em $H_2$, entÃ£o $H_1$ Ã© suportada {\em versus} $H_2$. NÃ£o parece haver motivo para rejeitar esse axioma, alÃ©m de uma
indisposiÃ§Ã£o prÃ©via a tratar hipÃ³teses compostas. JÃ¡ o segundo axioma Ã© uma forma de estabelecer coerÃªncia lÃ³gica.
Ã importante clarificar que esse requisito de coerÃªncia {\em nÃ£o Ã©} equivalente a supor que a estrutura lÃ³gica das hipÃ³teses
deve ser usada como base para justificar um grau de crenÃ§a sobre $H_1^*$ ou $H_1$: se $H_1^*$ implica em $H_1$ e a recÃ­proca
nÃ£o Ã© verdadeira, o axioma \ref{ax:coh} {\em nÃ£o} justifica que $H_1^*$ seja melhor suportada que $H_1$ por qualquer evidÃªncia.
Destes axiomas, deriva-se uma lei geral da verossimilhanÃ§a:

\begin{theorem}
	\textbf{(GLL)} Se $\sup \mathcal{L} (\Theta_1 | \bu{x} ) > \sup \mathcal{L} (\Theta_2 | \bu{x})$, entÃ£o existe evidÃªncia
	a favor de $H_1$ sobre $H_2$.
\end{theorem}

\begin{proof}
	Seja $\sup \mathcal{L} (\Theta_1 | \bu{x} ) > \sup \mathcal{L} (\Theta_2 | \bu{x})$. EntÃ£o existe $\theta_1 \in \Theta_1$
	tal que $\mathcal{L} (\theta_1 | \bu{x}) > \sup \mathcal{L} (\Theta_2 | \bu{x})$. Do axioma \ref{ax:inf}, a hipÃ³tese
	$H_1^* : \theta = \theta_1$ Ã© suportada em relaÃ§Ã£o a $H_2$. Mas $H_1^*$ implica $H_1$, e a conclusÃ£o segue do axioma
	\ref{ax:coh}.
\end{proof}

Embora nÃ£o decorra dos axiomas, o uso de uma razÃ£o de verossimilhanÃ§a generalizada $\sup \mathcal{L}(\Theta_1 | \bu{x}) / 
\sup \mathcal{L} (\Theta_2 | \bu{x}) $ parece natural para quantificar a forÃ§a da evidÃªncia. Ã trivial que GLL Ã© 
compatÃ­vel com LL no caso de hipÃ³teses simples, e com alguns casos particulares expostos por \citep{Royall97}. Essa formulaÃ§Ã£o
da GLL pode ser usada para apresentar questÃµes de anÃ¡lise de incerteza e para formalizar o uso de verossimilhanÃ§as perfilhadas
em alguns casos de modelos com {\em nuisance parameters}. 

No entanto, a implicaÃ§Ã£o mais surpreendente da GLL Ã© que ela permite uma medida de confirmaÃ§Ã£o {\em absoluta} de hipÃ³teses.
Uma hipÃ³tese  $H_1: \theta \in \Theta_1$ possui evidÃªncia favorÃ¡vel se $\sup \mathcal{L}(\Theta_1|\bu{x}) > 
\sup \mathcal{L} (\Theta_1^c|\bu{x}$, onde $^c$ representa o conjunto complementar, ou $L(\Theta_1, \Theta_1^c) > 1$.
Embora esse suporte seja nulo no caso
de hipÃ³teses simples sobre parÃ¢metros contÃ­nuos, esse nÃ£o Ã© caso quando $\Theta$ Ã© finito.

Considere um exemplo, dado por \cite{Royall97}: 

Existem trÃªs urnas com bolas brancas em diferentes proporÃ§Ãµes: um quarto ($\theta_1$), metade ($\theta_2$) e 
trÃªs quartos ($\theta_3$); identificamos como $H_i$ as trÃªs urnas possÃ­veis.
Se nenhuma bola branca for observada apÃ³s 5 sorteios com reposiÃ§Ã£o, essa observaÃ§Ã£o, $\bu{x}$, implica que
$\mathcal{L} (\theta_1 | \bu{x} ) \propto (\frac{3}{4})^5$,
$\mathcal{L} (\theta_2 | \bu{x} ) \propto (\frac{1}{2})^5$ e
$\mathcal{L} (\theta_3 | \bu{x} ) \propto (\frac{1}{4})^5$.

A LL nos permite afirmar que $L(\theta_1, \theta_2) = 7.6$ Ã© evidÃªncia mediana a favor de $H_1$ sobre $H_2$, e 
$L(\theta_2, \theta_3) = 32$ Ã© evidÃªncia forte de $H_2$ sobre $H_3$. No entanto, a LL nÃ£o permite inferÃªncias sobre a hipÃ³tese
composta $H_c : \theta=\theta_1 \lor \theta=\theta_3$ em comparaÃ§Ã£o com $H_2$; equivalentemente, LL nÃ£o permite
inferÃªncias sobre a evidÃªncia absoluta $H_2$ sobre $\sim\!\! H_2$.

Por outro lado, a GLL permite a afirmaÃ§Ã£o de que $L(\theta_1 \lor \theta_3, \theta_2) = 7.6$ representa suporte para $H_c$ em
relaÃ§Ã£o a $H_2$, e ainda que
o suporte absoluto para as trÃªs hipÃ³teses Ã© de 
$L(\theta_1, \sim\!\! \theta_1) = 7.6$,
$L(\theta_2, \sim\!\! \theta_2) = 0.13$,
$L(\theta_3, \sim\!\! \theta_3) = 0.004$, confirmando que apenas a hipÃ³tese 1 tem forÃ§a de evidÃªncia superior a 1.

Uma desvantagem em aceitar a GLL Ã© que o suporte a hipÃ³teses compostas nÃ£o se comporta da mesma forma que a razÃ£o
de verossimilhanÃ§as para hipÃ³teses simples. Suponha que o pesquisador, no mesmo problema das urnas, retira agora duas
bolas brancas. Essa nova observaÃ§Ã£o leva aos resultados 
$L(\theta_1, \sim\!\! \theta_1) = 0.11$,
$L(\theta_2, \sim\!\! \theta_2) = 0.44$,
$L(\theta_3, \sim\!\! \theta_3) = 2.25$, oferecendo suporte apenas para $H_3$. No entanto, nÃ£o hÃ¡ como combinar essa
evidÃªncia com a apresentada acima: as quantidades derivadas do supremo da verossimilhanÃ§a nÃ£o sÃ£o multiplicativas para conjuntos
de dados distintos, ao contrÃ¡rio da verossimilhanÃ§a. 
A forÃ§a de evidÃªncia absoluta para $\theta_1$ tomando o conjunto completo 
de dados Ã© de aproximadamente $1.8 \neq 7.6 * 0.11$.
Essa constataÃ§Ã£o, no entanto, nÃ£o invalida o teorema da GLL, embora
lance dÃºvidas quanto Ã  aplicabilidade da razÃ£o de verossimilhanÃ§a generalizada como forÃ§a de evidÃªncia para hipÃ³teses compostas.

Em um desenvolvimento independente, o estatÃ­stico indiano Debabrata Basu propÃ´s uma generalizaÃ§Ã£o
alternativa para a lei da verossimilhanÃ§a em 1975:
\begin{quote}
``{\em The strong law of likelihood}: For any two subsets $A$ and $B$ of $\Theta$, the data supports the 
hypothesis $\omega \in A$ better than the hypothesis $\omega in B$ if
\begin{equation}
\sum_{\omega \in A} \mathcal{L}(\omega) > \sum_{\omega \in B} \mathcal{L}(\omega)
\end{equation}
Let us recall the [assertion] that all our sets (the sample space, the parameter space, etc.) are finite.''
\end{quote}\citep{Basu75}

Novamente, o grande problema ao aceitar esta generalizaÃ§Ã£o Ã© encontrar uma medida de forÃ§a de evidÃªncia 
compatÃ­vel.

\section{WLL: A weak law of likelihood}

Confrontados com os insucessos de trabalhar com a verossimilhanÃ§a de hipÃ³teses compostas e os problemas derivados
de uma medida absoluta de confirmaÃ§Ã£o, os principais defensores da lei da verossimilhanÃ§a viram por bem abandonar a 
consideraÃ§Ã£o de hipÃ³teses compostas. O outro curso de aÃ§Ã£o que podemos tomar Ã© abandonar a lei da verossimilhanÃ§a,
e Ã© o caminho que vamos seguir nesta seÃ§Ã£o.

Um exemplo devido a \cite{Fitelson07} mostra uma questÃ£o importante sobre a lei da verossimilhanÃ§a:
tome um baralho de 52 cartas bem embaralhado, e considere as hipÃ³teses (simples) 
$H_1$: a primeira carta Ã© o Ã¡s de espadas {\em versus}
$H_2$: a primeira carta Ã© preta. A observaÃ§Ã£o de uma carta de espadas leva Ã s seguintes verossimilhanÃ§as: 
$\mathcal{L} ( H_1 | \spadesuit ) =  1 k> \mathcal{L} ( H_2 | \spadesuit ) =  \frac{1}{2} k$, sendo $k$ uma constante de 
proporcionalidade, e a lei da verossimilhanÃ§a nos leva a declarar o suporte para $H_1$ sobre $H_2$. Mas a evidÃªncia apresentada
sobre $H_1$ Ã© inconclusiva, mas garante $H_2$. Fitelson traÃ§a essa discordÃ¢ncia na relaÃ§Ã£o entre os chamados ``{\em catch-alls}'',
$P(E | \sim\!\!H_1)$ e $P(E|\sim\!\!H_2)$\footnote{Em casos de cartas de baralhos, os {\em catch-alls} estÃ£o bem definidos;
objetivistas podem ter problemas em aceitar a formulaÃ§Ã£o de {\em catch-alls} em problemas mais gerais, como "essa observaÃ§Ã£o
provÃª suporte Ã  teoria da evoluÃ§Ã£o"}. Uma formulaÃ§Ã£o mais geral sobre a lei da verossimilhanÃ§a, apelidada por J. Joyce de
lei fraca da verossimilhanÃ§a (WLL) Ã© dada por:

\begin{description}
\item[WLL]
``Evidence $E$ favors hypothesis $H_1$ over hypothesis $H_2$ {\em if} $P(E|H_1) > P(E|H_2)$ {\em and}
$P(E|\sim\!\!H_1) \leq P(E|\sim\!\!H_2)$.''
\citep{Fitelson07}
\end{description}

Ã evidente que LL $\implies$ WLL. No entanto, as principais correntes de Bayesianismo moderno {\em tambÃ©m} aceitam WLL: dada uma
medida de confirmaÃ§Ã£o $c(H,E)$, a expressÃ£o Bayesiana equivalente a LL Ã©:

\begin{description}
\item[$\dagger_c$]
``Evidence $E$ favors hypothesis $H_1$ over hypothesis $H_2$ according to measure $c$ {\em iff} $c(H_1,E) > c(H_2,E)$.''
\citep{Fitelson07}
\end{description}

TrÃªs possÃ­veis formas para a medida de confirmaÃ§Ã£o sÃ£o:
\begin{description}
\item[DiferenÃ§a] $d(H,E) = P(H|E)-P(H)$
\item[RazÃ£o] $r(H,E) = \frac{P(H|E)}{P(H)}$
\item[RazÃ£o de verossimilhanÃ§as] $l(H,E) = \frac{P(E|H)}{P(E|\sim\!\!H)}$
\end{description}

Ã importante relembrar que a confirmaÃ§Ã£o Bayesiana dada por $c(H,E)$ 
tem um carÃ¡ter nÃ£o-relacional, Ã  partir do qual se {\em deriva} uma 
medida de confirmaÃ§Ã£o relacional; verossimilhantistas enxergam na lei da verossimilhanÃ§a uma relaÃ§Ã£o primitiva.
E enquanto a construÃ§Ã£o da medida de confirmaÃ§Ã£o nÃ£o-relacional depende da especificaÃ§Ã£o de {\em prioris}, a WLL nÃ£o faz uso 
direto deles, mas apenas da especificaÃ§Ã£o de {\em catch-alls}. Uma teoria que utilizasse a WLL sem invocar {\em prioris}
poderia embasar uma escola de inferÃªncia intermediÃ¡ria, sem o uso de {\em prioris} ao qual objetivistas objetam, 
nem a restriÃ§Ã£o
arbitrÃ¡ria ao tratamento de hipÃ³teses compostas.

ConcluÃ­mos esta seÃ§Ã£o ponderando que
o paradigma da verossimilhanÃ§a parece estar encravado dentro do pensamento Bayesiano: se generalizamos a Lei da VerossimilhanÃ§a,
nos encontramos em um paradigma Bayesiano; se a enfraquecemos, encontramos todas as correntes do Bayesianismo moderno.
Se o problema da inferÃªncia estatÃ­stica Ã© respondido pela Lei da VerossimilhanÃ§a, essa resposta deve passar por explicar
essa singularidade.

\section{PLUE: a proposal for likelihood profiling}\label{sec:plue}

Nesta seÃ§Ã£o, vamos descrever uma metodologia tentativa para a realizaÃ§Ã£o de anÃ¡lises de incerteza
baseada no perfilhamento da verossimilhanÃ§a dos parÃ¢metros. Argumentamos que essa metodologia Ã©
intuitivamente atraente dentro de um paradigma de verossimilhanÃ§a.
Vamos nos referir ao nosso
procedimento como PLUE - Profiled Likelihood Uncertainty Estimation.

\subsection{Intuition}

Suponha que temos em mÃ£os um modelo de crescimento populacional estruturado como aqueles discutidos
por \citep{Chalom12}, e um conjunto de dados a partir dos quais podemos calcular
as taxas de sobrevivÃªncia, crescimento e fertilidade para a espÃ©cie. 

De posse desses dados, gostarÃ­amos de fazer as seguintes perguntas:

{\em ``Qual o suporte que os dados dÃ£o para a afirmaÃ§Ã£o de que a populaÃ§Ã£o estÃ¡ estÃ¡vel {\em versus} 
em declÃ­nio? Qual o suporte que os dados dÃ£o para a afirmaÃ§Ã£o de que a populaÃ§Ã£o vai se extinguir em menos 
de 10 anos {\em versus} em mais de dez anos?''}

Essas perguntas nÃ£o podem ser respondidas de um ponto de vista verossimilhantista, por corresponderem
a hipÃ³teses compostas sobre os parÃ¢metros. No entanto, podemos tomar o ponto de mÃ¡xima verossimilhanÃ§a
como um ponto privilegiado, e restringir nossa pergunta Ã  forma:

{\em ``Qual o suporte que os dados dÃ£o para o ponto de mÃ¡xima verossimilhanÃ§a {\em versus} qualquer ponto
cuja taxa de crescimento populacional seja negativa?''}

Esquematicamente, podemos ver na figura \ref{fig:esquema} que estamos comparando a verossimilhanÃ§a
do ponto de mÃ¡ximo global ($x_{max}$) com o mÃ¡ximo obtido em uma regiÃ£o distinta ($x_{lim}$). Esta pergunta
compara uma hipÃ³tese simples, a que corresponde Ã  mÃ¡xima verossimilhanÃ§a, com uma hipÃ³tese composta.
Esse tipo de comparaÃ§Ã£o evita os problemas e peculiaridades que surgem em propostas de leis gerais de 
verossimilhanÃ§a devido Ã  sobreposiÃ§Ã£o entre a verossimilhanÃ§a de hipÃ³teses diferentes. Em termos dos axiomas
de Zhang\footnote{vide seÃ§Ã£o \ref{sec:GLL}}, estamos aceitando uma forma fraca do primeiro, mas nÃ£o o segundo.

Esse raciocÃ­nio pode ser expandido para perguntarmos:

{\em ``Quais sÃ£o os pontos do espaÃ§o de parÃ¢metros para os quais o suporte Ã© maior do que uma certa distÃ¢ncia
$\delta$ em relaÃ§Ã£o ao ponto de mÃ¡xima verossimilhanÃ§a?''}

Ao fazer essa pergunta para uma sÃ©rie de valores de $\delta$, estamos efetivamente perfilhando a 
verossimilhanÃ§a de cada ponto do espaÃ§o de parÃ¢metros. Neste ponto, precisamos apontar que procedimentos
de perfilhamento vÃªm sendo utilizados na inferÃªncia por verossimilhanÃ§a hÃ¡ dÃ©cadas para reduzir a 
dimensionalidade do espaÃ§o de parÃ¢metros - seja na forma de uma simples eliminaÃ§Ã£o de ``{\em nuisance
parameters}'', seja de forma elaborada atravÃ©s de anÃ¡lises de componentes principais. 
Considere, por exemplo, o caso tÃ­pico em que desejamos ajustar uma distribuiÃ§Ã£o normal a uma sÃ©rie de dados,
e ao invÃ©s de compararmos pares da forma $(\mu = \mu_0, \sigma = \sigma_0)$ representando um valor fixo para 
a mÃ©dia e desvio padrÃ£o dessa normal, desejamos fazer afirmaÃ§Ãµes apenas sobre a mÃ©dia $\mu$ desta normal.
JÃ¡ apontamos que este raciocÃ­nio corresponde a encarar hipÃ³teses compostas da forma 
$(\mu = \mu_0, \sigma \geq 0)$. Agora, vamos notar que isto Ã© equivalente a considerar a funÃ§Ã£o 
$g(\mu, \sigma) = \mu$ - ou seja, estamos projetando o espaÃ§o bidimensional formado por $\mu$ e $\sigma$
em um espaÃ§o unidimensional. Nossa proposta pode ser considerada uma generalizaÃ§Ã£o deste
procedimento: ao invÃ©s de considerar apenas funÃ§Ãµes que removam um dos parÃ¢metros, estamos considerando
qualquer funÃ§Ã£o nÃ£o-inversÃ­vel.

HÃ¡ que se considerar, no entanto, que tÃ©cnicas de remoÃ§Ã£o de ``{\em nuisance parameters}'' dentro do paradigma
de verossimilhanÃ§a no caso geral se assentam sobre questÃµes que nÃ£o estÃ£o totalmente resolvidas - e que
se situam na fronteira entre o pensamento verossimilhantista e a inferÃªncia fiducial \citep{Kalbfleisch70,
Edwards72}. Se estes mÃ©todos nÃ£o podem ser, em geral, encarados como forma de inferÃªncia tÃ£o rigorosa quanto
a inferÃªncia pela funÃ§Ã£o de verossimilhanÃ§a, Ã© bem aceito que esta Ã© uma anÃ¡lise exploratÃ³ria vÃ¡lida. Nossa
generalizaÃ§Ã£o se encontra no mesmo estado: embora ela nÃ£o possa ser vista como uma forma rigorosa de 
inferÃªncia, ela pode ser usada de forma exploratÃ³ria sem incorrer em problemas mais profundos que a perfilhagem
tradicional.

Lembramos que, se do ponto de vista da anÃ¡lise de verossimilhanÃ§a, nossa proposta nÃ£o apresenta grande 
inovaÃ§Ã£o, na literatura de anÃ¡lise de sensibilidade ela Ã© Ãºnica em propor uma metodologia que utilize toda
a informaÃ§Ã£o contida nas amostras coletadas. 

\begin{figure}[htb]
<<>>=
L <- function(x) {
	x = x + 3
	((x)*(x-2)*(x-6)*(x-9)+100)/20
}
x = seq(-3,5, length.out=200)
plot(x, L(x), type="l", xaxs = "i", yaxs = "i", ylim=c(0, 10))
# regioes shaded
x = seq(-3,0, length.out=200)
polygon(c(-3,x,0), c(0,L(x),0), col="gray20")
x = seq(0,5, length.out=200)
polygon(c(0,x,5), c(0,L(x),0), col="gray60")
# pontos de "interesse"
lines(c(0,0), c(7.7,8.3), lty=2)
text(0, 8.5, expression(x[lim]))
lines(c(1.08,1.08), c(9,9.5), lty=2)
text(1.08, 9.7, expression(x[max]))
arrows(1.08, 7.7, 1.08, 9, angle=90, code=3, length=0.15)
text(1.4,(7.7+9)/2,(expression(Delta[L])), cex=1.3)
@
	\caption{RepresentaÃ§Ã£o esquemÃ¡tica de uma funÃ§Ã£o de verossimilhanÃ§a, mostrando dois pontos de interesse:
	o mÃ¡ximo global $x_{max}$ e o mÃ¡ximo de uma regiÃ£o distinta $x_{lim}$. Veja detalhes no texto.} 
	\label{fig:esquema}
\end{figure}

\subsection{Method}
\begin{description}
	\item[DefiniÃ§Ãµes]
		Considere um modelo de interesse biolÃ³gico qualquer - vamos chamÃ¡-lo de modelo biolÃ³gico, para
		diferenciar do modelo estatÃ­stico que serÃ¡ apresentado abaixo. Vamos considerar o caso simples
		de um resultado escalar $y$ de um modelo com um vetor de entradas $\bu{\theta}$: $y = F(\bu{\theta})$.
		Vamos representar por $\chi$ o conjunto de dados coletados. Se temos $n$ parÃ¢metros e $m$ 
		observaÃ§Ãµes, $\chi$ Ã© uma tabela de $n$ colunas por $m$ linhas.

		Formule diferentes modelos para explicar seus dados (ex, fertilidade constante ou agrupada, taxa de 
		crescimento constante ou decrescente com a classe de tamanho, modelo com 4 ou com 5 classes de 
		tamanho, etc) - este vai ser designado o modelo estatÃ­stico.
		Escreva a funÃ§Ã£o de verossimilhanÃ§a $\mathcal{L}(\bu{\theta}|\chi)$ para cada modelo. 
		Encontre o conjunto de parÃ¢metros que melhor ajusta seus dados para cada modelo e 
		determine o valor de AIC para cada modelo estatÃ­stico. 
	\item[Amostragem]
		De posse do modelo estatÃ­stico de melhor AIC\footnote{Ã possÃ­vel que uma abordagem de inferÃªncia
		multi-modelo possa ser utilizada em casos de empate de AIC
		\citep{Burnham02}}, utilize um mÃ©todo de Monte Carlo para gerar um grande nÃºmero de amostras com
		densidade proporcional a $\mathcal{L}(\bu{\theta}|\chi)$. Vamos chamar esta amostra discreta de 
		$\bu{A}$. Ã cada amostra $A_{i \cdot}$ da funÃ§Ã£o $\mathcal{L}(\bu{\theta}|\chi)$, associamos 
		$L_i = \mathcal{L}(A_{i \cdot} | \chi)$, o valor de verossimilhanÃ§a desta amostra, e 
		$Y_i = F(A_{i \cdot})$, o resultado do modelo biolÃ³gico quando executado sobre esta amostra.
		Normalize $L_i$ de forma que o mÃ­nimo de verossimilhanÃ§a seja no $0$.
	\item[AgregaÃ§Ã£o]
		A partir dos valores resultantes do modelo $Y_i$ e seus valores de verossimilhanÃ§a $L_i$ associados,
		construa o perfil superior para a verossimilhanÃ§a de $y$ da seguinte forma: 
		para cada incremento $z$, encontre o maior valor $\bar y$ em $Y_i$ tal que $L_i \leq z$. 
		Anote este valor como $P_{sup}(z) = \bar y$ e repita para um valor maior de $z$.

		Proceda de forma anÃ¡loga para construir o perfil inferior de verossimilhanÃ§a. Os dois perfis, em 
		conjunto, podem ser utilizados para investigar as regiÃµes de plausibilidade para $y$.
\end{description}

\section[Case study]{Case study: a minimal model for population growth}

Um modelo de populaÃ§Ã£o estruturada que pode ser considerado mÃ­nimo Ã© o modelo
com juvenis nÃ£o-reprodutivos e adultos reprodutivos, modelo discutido por \cite{Caswell08} 
como um exemplo simples da aplicaÃ§Ã£o da anÃ¡lise de sensibilidade analÃ­tica. 
O modelo Ã© associado Ã  seguinte matriz:

\begin{equation}
 A = \left[
 \begin{array}{ll}
	 \sigma_1 (1-\gamma) &   f \\
     \sigma_1 \gamma & \sigma_2
 \end{array}
 \right]
\end{equation}

Aqui, $\sigma_1$ Ã© a probabilidade de sobrevivÃªncia de juvenis, $\sigma_2$ Ã© a probabilidade de sobrevivÃªncia de adultos, 
$\gamma$ Ã© a probabilidade de maturaÃ§Ã£o, e $f$ Ã© a fertilidade dos adultos. Vamos representar por $\lambda$ o maior
autovalor dessa matriz.

Vamos primeiramente presumir que a sobrevivÃªncia independe do estÃ¡gio ($\sigma_1=\sigma_2=\sigma$). TambÃ©m vamos fazer
a suposiÃ§Ã£o de que Ã© possÃ­vel marcar inequivocamente quais dos juvenis nasceram no Ãºltimo ciclo, e quais adultos
passaram pelo processo de maturaÃ§Ã£o no Ãºltimo ciclo, para chegar ao modelo:

\begin{equation}
 A = \left[
 \begin{array}{ll}
	 \sigma (1-\gamma) &   f \\
     \sigma \gamma & \sigma
 \end{array}
 \right]
\end{equation}

Para animais de tamanho grande, com um filhote por estaÃ§Ã£o reprodutiva, $f$ pode ser aproximado pela proporÃ§Ã£o de
adultos que gera prole, $\sigma$ Ã© dado pela proporÃ§Ã£o de indivÃ­duos que sobrevivem por um ciclo e $\gamma$ pela
proporÃ§Ã£o de juvenis que se tornam adultos por ciclo, de forma que os trÃªs parÃ¢metros podem ser modelados por
distribuiÃ§Ãµes binomiais, com probabilidades $\theta_i$ desconhecidas e nÃºmero de tentativas dados, respectivamente, por
$n_1$, o nÃºmero original de juvenis, $n_2$, o nÃºmero original de adultos, e $n_t$, o tamanho total da populaÃ§Ã£o:

\begin{align}
	\gamma & \sim \operatorname{binom}(\theta_1, n_1) \\
	f      & \sim \operatorname{binom}(\theta_2, n_2) \\
	\sigma & \sim \operatorname{binom}(\theta_3, n_t = n_1+n_2)
\end{align}

Vamos utilizar ainda a suposiÃ§Ã£o de que os parÃ¢metros sÃ£o independentes neste exemplo, para chegar Ã s 
funÃ§Ãµes de verossimilhanÃ§a retratadas na fig. \ref{fig:LikFunc} (detalhes sobre a construÃ§Ã£o dessa funÃ§Ã£o
podem ser vistos na seÃ§Ã£o \ref{apmat}).

<<load, fig=F>>=
# First, we set the random seed to ensure reproducibility:
set.seed(42)
# Loads the pse package:
library(pse)
# Helper functions for the model:
tr <- function (A) return(A[1,1]+A[2,2])
A.to.lambda <- function(A) 1/2*(tr(A) + sqrt((tr(A)^2 - 4*det(A))))
getlambda = function (sigma, f, gamma) {
	A.to.lambda (matrix(c(sigma*(1-gamma), f, 
                        sigma*gamma, sigma), 
                      ncol=2, byrow=TRUE) )
}
getlambda = Vectorize(getlambda)
model <- function(x) getlambda(x[,1], x[,2], x[,3])

factors = c("sigma", "f", "gamma")
N = 50000
# Initial population: juveniles, adults, total
n <- c(10, 15); n.t <- sum(n)
# Observed quantities: matured, born, survival
obs <- c(3, 2, 23)
# Best guess for the parameters
sigma <- obs[3]/n.t
f <- obs[2]/n[2]
gamma <- obs[1]/n[1]
start = c(sigma, f, gamma)
lambda <- getlambda(sigma, f, gamma)
# probability distribution. It's the POSITIVE LL.
# NOTE: LL function uses GLOBAL obs and n!!!
LL <- function (x) 
{
	t <- dbinom(obs[3], n.t, as.numeric(x[1]), log=TRUE) +
				  dbinom(obs[2], n[2], as.numeric(x[2]), log=TRUE) +
				  dbinom(obs[1], n[1], as.numeric(x[3]), log=TRUE)
	if (is.nan(t)) return (-Inf);
	return(t);
}
plue <- PLUE(model, factors, N, LL, start, method="mcmc", opts=list(blen=10), nboot=50)
minlik <- unique(plue$res[plue$nLL == min(plue$nLL)])
@

Vamos examinar um exemplo numÃ©rico com a populaÃ§Ã£o inicial contendo \Sexpr{n[1]} juvenis e \Sexpr{n[2]} adultos. O tamanho
populacional pequeno Ã© importante para acentuar as diferenÃ§as entre as abordagens. ApÃ³s um ciclo, observamos \Sexpr{obs[1]}
adultos recÃ©m maduros, \Sexpr{obs[2]} nascidos e \Sexpr{obs[3]} sobreviventes. Ã fÃ¡cil ver na figura \ref{fig:LikFunc}
que a melhor estimativa para
cada parÃ¢metro Ã© dada por $\sigma = $ \Sexpr{round(sigma,2)}, $f = $ \Sexpr{round(f,2)} e $\gamma = $ \Sexpr{round(gamma,2)}.
Neste caso, o valor de $\lambda$ Ã© \Sexpr{round(lambda,2)}.

\begin{figure}
<<LikFunc>>=
curve(dbinom(obs[3], n.t, x))
curve(dbinom(obs[2], n[2], x), col = 2, add=TRUE)
curve(dbinom(obs[1], n[1], x), col = 3, add=TRUE)
@
	\caption{FunÃ§Ã£o de verossimilhanÃ§a para cada parÃ¢metro do modelo. Preto = $\sigma$, vermelho = $f$ e verde = $\gamma$.}
	\label{fig:LikFunc}
\end{figure}

As funÃ§Ãµes de verossimilhanÃ§a de cada parÃ¢metro
foram utilizadas para gerar \Sexpr{get.N(plue)} amostras pelo mÃ©todo de Metropolis, a partir das quais 
geramos uma distribuiÃ§Ã£o empÃ­rica para $\lambda$, de forma proporcional Ã  verossimilhanÃ§a dos parÃ¢metros.
Esta distribuiÃ§Ã£o de valores de $\lambda$, conjuntamente com os valores de verossimilhanÃ§a associados a cada
input, foi usada para gerar um perfil de verossimilhanÃ§a para o resultado do modelo. O mÃ­nimo de verossimilhanÃ§a para
$\lambda$ Ã© atingido em $\lambda = $ \Sexpr{round(minlik,2)}.

As figuras \ref{fig:lambdascatter} e \ref{fig:lambdaprcc} mostram
resultados preliminares da aplicaÃ§Ã£o de tÃ©cnicas de anÃ¡lise de sensibilidade sobre as amostras geradas, anÃ¡logas Ã s
discutidas no capÃ­tulo \ref{cap:pse}. Ã importante ressaltar que estas anÃ¡lises foram realizadas sobre uma
vizinhanÃ§a nÃ£o infinitesimal (como seria o caso analÃ­tico) nem arbitrÃ¡ria (como seriam as anÃ¡lises descritas
no cap. \ref{cap:pse}), centrada no ponto de mÃ¡xima verossimilhanÃ§a.

\begin{figure}
<<Case1>>=
plot(plue)
@
	\caption{AnÃ¡lise de verossimilhanÃ§a perfilhada sobre os resultados do modelo mÃ­nimo de populaÃ§Ã£o estruturada.  }
	\label{fig:lambda}
\end{figure}
\begin{figure}
<<>>=
# Analise de sensibilidade sobre lambda
plotscatter(plue)
@
	\caption{GrÃ¡fico de dispersÃ£o dos valores de parÃ¢metros (no eixo x) e resultados do modelo de crescimento estruturado
	mÃ­nimo, gerados a partir de uma abordagem de verossimilhanÃ§a.}
	\label{fig:lambdascatter}
\end{figure}
\begin{figure}
<<>>=
plotprcc(plue)
@
	\caption{AnÃ¡lise de Partial Rank Correlation Coefficient entre as entradas do modelo e o resultado em um modelo
	estruturado mÃ­nimo de crescimento populacional}
	\label{fig:lambdaprcc}
\end{figure}

<<largerSample, fig=FALSE>>=
Mult=3
n <- Mult*n; n.t = Mult*n.t; obs = Mult*obs;
largePlue <- PLUE(model, factors, N, LL, start, method="mcmc", opts=list(blen=10), nboot=50)

# pop inicial: juv, ad, total
n <- c(10, 15); n.t <- sum(n)
# x obs: maturados, nascidos, sobrev.totais
obs <- c(3, 2, 23)
@

A anÃ¡lise realizada indica que o valor de $\lambda$ estimado Ã© pouco confiÃ¡vel, tendo um perfil muito aberto. 
Ã importante notar que esses resultados sÃ£o para uma Ãºnica amostra, e um tamanho amostral decididamente pequeno. 
Considerando uma amostra \Sexpr{Mult} vezes maior, na qual todas as proporÃ§Ãµes se mantenham as mesmas (ou seja,
o nÃºmero de indivÃ­duos maturados, nascidos e sobreviventes Ã© multiplicado por \Sexpr{Mult}), a anÃ¡lise resulta em
um perfil muito mais fechado (veja figura \ref{fig:lambda2}).

\begin{figure}
<<>>=
plot(largePlue)
@
	\caption{AnÃ¡lise de verossimilhanÃ§a perfilhada sobre os resultados do modelo mÃ­nimo de populaÃ§Ã£o estruturada,
	mas com tamanho amostral maior. Em comparaÃ§Ã£o com a figura \ref{fig:lambda}, o perfil Ã© muito mais fechado. }
	\label{fig:lambda2}
\end{figure}

\subsection{Mathematical details}\label{apmat}
Nesta seÃ§Ã£o, vamos desenvolver alguns detalhes matemÃ¡ticos sobre o exemplo acima.

Para um dado nÃºmero observado $x_A$ de juvenis que passaram pelo processo de maturaÃ§Ã£o, se tornando adultos, apÃ³s um ciclo,
a funÃ§Ã£o de log-verossimilhanÃ§a para $\gamma$ Ã© dada por

\begin{equation}
\mathcal{L} \left( \theta_1 | x_A \right) 
= \log \left( {n_1 \choose x_A} \theta_1^{x_A} (1-\theta_1) ^{n_1-x_A} \right)
\end{equation}

Da mesma forma, a funÃ§Ã£o de log-verossimilhanÃ§a para $f$ Ã© dada em funÃ§Ã£o do nÃºmero de juvenis nascidos no Ãºltimo ciclo, $x_J$:
\begin{equation}
\mathcal{L} \left( \theta_2 | x_J \right) 
= \log \left( {n_2 \choose x_J} \theta_2^{x_J} (1-\theta_2) ^{n_2-x_J} \right)
\end{equation}

Por fim, a funÃ§Ã£o de log-verossimilhanÃ§a referente a $\sigma$ Ã© dada em funÃ§Ã£o do nÃºmero de indivÃ­duos sobreviventes, $x_S$,
calculado como o nÃºmero de indivÃ­duos observado menos $x_J$:
\begin{equation}
\mathcal{L} \left( \theta_3 | x_S \right) 
= \log \left( {n_t \choose x_S} \theta_3^{x_S} (1-\theta_3) ^{n_t-x_S} \right)
\end{equation}

Com o pressuposto forte de que as trÃªs variÃ¡veis sÃ£o independentes, e escrevendo $n_t = n_3$ e 
$\{x_A, x_J, x_S\} = \{x_1, x_2, x_3\}$
para facilitar a notaÃ§Ã£o\footnote{Importante frisar: $x_1$ nÃ£o corresponde aqui ao nÃºmero de juvenis observados apÃ³s um ciclo, etc.}
a funÃ§Ã£o de verossimilhanÃ§a para o vetor de parÃ¢metros $\boldsymbol\theta$ Ã©:

\begin{equation}
\mathcal{L} \left( \boldsymbol{\theta} | \mathbf{x} \right) 
= \sum_i \log \left( {n_i \choose x_i} \theta_i^{x_i} (1-\theta_i) ^{n_i-x_i} \right) \label{eqn:loglik}
\end{equation}

O resultado do modelo Ã© $\lambda$, o maior autovalor de A obedecendo 
\begin{equation}
	\det \left[ 
	\begin{array}{ll} \lambda - \sigma(1-\gamma) & -f \\
		-\sigma \gamma &         \lambda - d 
	\end{array}
	\right]
	= \lambda^2 - \lambda \operatorname{tr}(A) + \operatorname{det}(A) = 0
\end{equation}
\begin{align}
	\lambda = \frac{1}{2} \left(\operatorname{tr}(A) + \sqrt{\operatorname{tr}^2(A) - 4 \operatorname{det}(A)} \right)
\end{align}

\subsection{\R code for performing the analyses}

\SweaveOpts{fig=F,echo=T}
Code for loading the package and creating the model objects:
<<lload, echo=TRUE>>=
<<load>>
@

This document presents a brief introduction to a proposed methodology for the likelihood profiling of
of the results from a computational model. This methodology is nicknamed PLUE, for 
Profiled Likehood Uncertainty Estimation, and is implemented in the {\em pse} package by the PLUE function.
A paper describing the theoretical background for this proposal is under preparation for publication.
The present document presumes you are familiar with general concepts from parameter space exploration. If
you are not, please refer to our work in \citep{Chalom12}.
The PLUE methodology is useful if you are interested in analysing a computational model and if you have
already gathered some data from which you can estimate likelihood distributions for your input parameters.
If you are interested in conducting an exploratory analysis and you don't have any data collected, you 
should use the tools described in the ``pse\_tutorial'' vignette in this package.
You should have installed \R 
\footnote{This tutorial was written and tested with \R version 
3.0.1, but it should work with newer versions}
along with an interface and 
text editor of your liking, and the package ``pse''
(available on http://cran.r-project.org/web/packages/pse).

The general question we are attempting to answer here is: {\em how much support does the data give to 
alternative hypothesis concerning the result of a (non-invertible) model?} It should be noted that while this
question may not be always answered under a likehoodist approach to statistical inference, it does have
an answer when we restrict one of the alternative hypothesis to being the maximum likehood estimator for the
parameters. This answer is given by profiling the likelihood of the model parameters, and while this procedure
leads to a function that is not a true likelihood function (thus not possessing many desirable properties),
it is generally accepted as a valid exploratory analysis.

\subsubsection{Biological and statistical models}
First, we should define our interest model. We will refer to this model as the biological model\footnote{
Because the models of interest in my research are biological. It can also be a physical model, 
geochemical model, etc.} to distinguish this from the statistical model we will be using to 
estimate likelihoods. This model 
must be formulated as an \R function that
receives a {\em data.frame}, in which every column represent a different
parameter, and every line represents a different combination of values
for those parameters. The function must return an array with the same
number of elements as there were lines in the original data frame,
and each entry in the array should correspond to the result of running
the model with the corresponding parameter combination. For example, it can be this:

<<model>>=
oneRun <- function (r, K, Xo) {
    X <- Xo
    for (i in 0:20) {
       X <- X+r*X*(1-X/K)
    }   
    return (X) 
}
modelVec <- Vectorize(oneRun) 
model <- function(x) modelVec(x[,1], x[,2], x[,3])
@

Following the definition of the model, we should define the likelihood function for our parameters.
To do this, we can formulate and test several statistical models. In order to fit competing models to the 
data and select the best of them, we recommend using the \R package \textbf{bbmle}.
Then, the best model should be written as a function receiving a numeric vector representing one realization
of the parameter vector and returning the {\em positive} log-likelihood of that vector.

For example, the best model may be that the parameters $r$, $K$ and $Xo$ are all independent from each other,
coming from two exponentials and from the {\em size} parameter of one binomial distribution, 
respectively, fitting the data in the {\em observations} data.frame below:
<<>>=
r <- c(1.4, 1.2, 1.8)
K <- c(70, 85, 98)
Xo <- c(50, 60, 45)
obs = data.frame(r=r, K=K, Xo=Xo)
@

The likelihood function, in this case, should be:
<<>>=
LL <- function (x) 
{
	t <- sum(dexp(1/obs$r, as.numeric(x[1]), log=TRUE)) +
		  sum(dexp(1/obs$K, as.numeric(x[2]), log=TRUE)) +
		  sum(dbinom(obs$Xo, as.integer(x[3]), p=0.5, log=TRUE))
	if (is.nan(t)) return (-Inf);
	return(t);
}
LLs <- function(x) suppressWarnings(LL(x))
@
Please note that this function uses the global variable {\em obs}, and that it return minus infinity instead
of not-a-number in cases where the likelihood is not properly defined. This can happen, for instance, if
any of the values of {\em x} is negative.
Also, notice that this function converts the third element of {\em x} to integer, as the {\em dbinom} function does not accept a fractional value for the {\em size} parameter.

\subsubsection{Profiling: sampling and aggregating the results}
After carefully constructing the model of interest and the likelihood function, as described in the 
previous section, performing the PLUE analysis is simply a matter of calling the {\em PLUE} function.
This function performs three steps. First, it performs a Monte Carlo sampling of the likelihood function
in order to generate a large sample from the likelihood distribution. Then, the biological model is applied
to this sample, and finally the model results are combined by means of profiling the likelihood function
associated with each data point.

The {\em pse} package implements a simple Metropolis sampling function that can be used
by setting {\em method=`internal'} in the {\em PLUE} function call. For more elaborate sampling schemes, 
and more control over the process, we recommend using {\em method=`mcmc'}, which uses the {\em mcmc} \R 
package.

<<>>=
library(pse)
factors = c("r", "K", "X0")
set.seed(42)
N = 10000
# The starting point for the Monte Carlo sampling
start = c(mean(obs$r), mean(obs$K), 2*max(obs$Xo))
plue <- PLUE(model, factors, N, LLs, start)
@

\textbf{Important note:} the example above uses a $N$ of 10.000, which is very low. For practical applications, always use larger samples and the `mcmc' method.

In order to see the profiled likelihood of the model result, simply run:
<<plot,fig=T>>=
plot(plue)
@

The profile seen in this figure shows that the
model result is very unreliable. It can be considered very plausible, from the data collected, that the result of the model lie anywhere in the $20 - 120$ interval.

Additional plots may be seen by using the 
{\em plotscatter} and {\em plotprcc} functions:
<<scatter,fig=T>>=
plotscatter(plue, add.lm=F)
@
<<prcc,fig=T>>=
plotprcc(plue)
@

The interpretation of these graphs is analogous
to the graphs generated by the Latin Hypercube
Sampling, and described in the 
``pse\_tutorial'' vignette. However, it is 
important to notice that, instead of the 
arbitrary region of the parameter space that
is sampled in the LHS scheme, the plots 
presented in this vignette are representing
a discretization of the likelihood surfaces
of the parameters, thus incorporating all the
information about the data collected.

\section*{Acknowledgements}
This work was supported by a CAPES scholarship.

\newpage
\bibliographystyle{apalike}
\bibliography{chalom}
\end{document}
