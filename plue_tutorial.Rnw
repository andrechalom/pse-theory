\SweaveOpts{fig=F,echo=T}
<<seed, echo=F>>=
set.seed(42)
@

This document presents a brief introduction to a proposed methodology for the likelihood profiling of
of the results from a computational model. This methodology is nicknamed PLUE, for 
Profiled Likehood Uncertainty Estimation, and is implemented in the {\em pse} package by the PLUE function.
A paper describing the theoretical background for this proposal is under preparation for publication.
The present document presumes you are familiar with general concepts from parameter space exploration. If
you are not, please refer to our work in \citep{Chalom12}.
The PLUE methodology is useful if you are interested in analysing a computational model and if you have
already gathered some data from which you can estimate likelihood distributions for your input parameters.
If you are interested in conducting an exploratory analysis and you don't have any data collected, you 
should use the tools described in the ``pse\_tutorial'' vignette in this package.
You should have installed \R 
\footnote{This tutorial was written and tested with \R version 
3.0.1, but it should work with newer versions}
along with an interface and 
text editor of your liking, and the package ``pse''
(available on http://cran.r-project.org/web/packages/pse).

The general question we are attempting to answer here is: {\em how much support does the data give to 
alternative hypothesis concerning the result of a (non-invertible) model?} It should be noted that while this
question may not be always answered under a likehoodist approach to statistical inference, it does have
an answer when we restrict one of the alternative hypothesis to being the maximum likehood estimator for the
parameters. This answer is given by profiling the likelihood of the model parameters, and while this procedure
leads to a function that is not a true likelihood function (thus not possessing many desirable properties),
it is generally accepted as a valid exploratory analysis.

\section{Biological and statistical models}
First, we should define our interest model. We will refer to this model as the biological model\footnote{
Because the models of interest in my research are biological. It can also be a physical model, 
geochemical model, etc.} to distinguish this from the statistical model we will be using to 
estimate likelihoods. This model 
must be formulated as an \R function that
receives a {\em data.frame}, in which every column represent a different
parameter, and every line represents a different combination of values
for those parameters. The function must return an array with the same
number of elements as there were lines in the original data frame,
and each entry in the array should correspond to the result of running
the model with the corresponding parameter combination. For example, it can be this:

<<model>>=
oneRun <- function (r, K, Xo) {
    X <- Xo
    for (i in 0:20) {
       X <- X+r*X*(1-X/K)
    }   
    return (X) 
}
modelVec <- Vectorize(oneRun) 
model <- function(x) modelVec(x[,1], x[,2], x[,3])
@

Following the definition of the model, we should define the likelihood function for our parameters.
To do this, we can formulate and test several statistical models. In order to fit competing models to the 
data and select the best of them, we recommend using the \R package \textbf{bbmle}.
Then, the best model should be written as a function receiving a numeric vector representing one realization
of the parameter vector and returning the {\em positive} log-likelihood of that vector.

For example, the best model may be that the parameters $r$, $K$ and $Xo$ are all independent from each other,
coming from two exponentials and from the {\em size} parameter of one binomial distribution, 
respectively, fitting the data in the {\em observations} data.frame below:
<<>>=
r <- c(1.4, 1.2, 1.8)
K <- c(70, 85, 98)
Xo <- c(50, 60, 45)
obs = data.frame(r=r, K=K, Xo=Xo)
@

The likelihood function, in this case, should be:
<<>>=
LL <- function (x) 
{
	t <- sum(dexp(1/obs$r, as.numeric(x[1]), log=TRUE)) +
		  sum(dexp(1/obs$K, as.numeric(x[2]), log=TRUE)) +
		  sum(dbinom(obs$Xo, as.integer(x[3]), p=0.5, log=TRUE))
	if (is.nan(t)) return (-Inf);
	return(t);
}
@
Please note that this function uses the global variable {\em obs}, and that it return minus infinity instead
of not-a-number in cases where the likelihood is not properly defined. This can happen, for instance, if
any of the values of {\em x} is negative.
Also, notice that this function converts the third element of {\em x} to integer, as the {\em dbinom} function does not accept a fractional value for the {\em size} parameter.

\section{Profiling: sampling and aggregating the results}
After carefully constructing the model of interest and the likelihood function, as described in the 
previous section, performing the PLUE analysis is simply a matter of calling the {\em PLUE} function.
This function performs three steps. First, it performs a Monte Carlo sampling of the likelihood function
in order to generate a large sample from the likelihood distribution. Then, the biological model is applied
to this sample, and finally the model results are combined by means of profiling the likelihood function
associated with each data point.

The {\em pse} package implements a simple Metropolis sampling function that can be used
by setting {\em method=`internal'} in the {\em PLUE} function call. For more elaborate sampling schemes, 
and more control over the process, we recommend using {\em method=`mcmc'}, which uses the {\em mcmc} \R 
package.

<<>>=
library(pse)
factors = c("r", "K", "X0")
set.seed(42)
N = 10000
# The starting point for the Monte Carlo sampling
start = c(mean(obs$r), mean(obs$K), 2*max(obs$Xo))
plue <- PLUE(model, factors, N, LL, start)
@

\textbf{Important note:} the example above uses a $N$ of 10.000, which is very low. For practical applications, always use larger samples and the `mcmc' method.

In order to see the profiled likelihood of the model result, simply run:
<<plot,fig=T>>=
plot(plue)
@

The profile seen in this figure shows that the
model result is very unreliable. It can be considered very plausible, from the data collected, that the result of the model lie anywhere in the $20 - 120$ interval.

Additional plots may be seen by using the 
{\em plotscatter} and {\em plotprcc} functions:
<<scatter,fig=T>>=
plotscatter(plue, add.lm=F)
@
<<prcc,fig=T>>=
plotprcc(plue)
@

The interpretation of these graphs is analogous
to the graphs generated by the Latin Hypercube
Sampling, and described in the 
``pse\_tutorial'' vignette. However, it is 
important to notice that, instead of the 
arbitrary region of the parameter space that
is sampled in the LHS scheme, the plots 
presented in this vignette are representing
a discretization of the likelihood surfaces
of the parameters, thus incorporating all the
information about the data collected.
